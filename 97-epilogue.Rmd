# Epilogue

*Author: Matthias AÃŸenmacher*

Since this project was realized in a limited time frame and accounted for about one third
of the ECTS points which should be achieved during one semester, it is obvious that this
booklet cannot provide exhaustive coverage of the vast research field of _Natural Language Processing_.  

Furthermore this area of research is moving very rapidly at the moment, which means that
certain architectures, improvements or ideas had net yet even been published when we sat down
and came up with the chapter topics in February 2020. Thus, this epilogue tries to put the content
of this booklet into context and relate it to what is currently happening. Thereby we will focus on 
mainly three aspects:

- New influential (or even state-of-the-art) architectures
- Improvements and work on the Attention-mechanism and Transformers
- Work on proper evaluation and interpretability

## New influentioal architectures

In [**Chapter 7: "Transfer Learning for NLP I"**](./transfer-learning-for-nlp-i.html) and [**Chapter 9: "Transfer Learning for NLP I"**](./transfer-learning-for-nlp-ii.html) some of the most important models for _sequential transfer learning_ have been presented. We chose to narrow ourselves down to this type of models, since we considered them to be most important to begin with, in order to unterstand the overall concept. Nevertheless, other influential architectures shall also be addressed:

- An architecture with a relatively interesting pre-training objective the interested reader might want to have a look at, is [**ELECTRA**](https://openreview.net/pdf?id=r1xMH1BtvB) (@Clark2020ELECTRA).
- [**Google's T5**](https://arxiv.org/pdf/1910.10683.pdf) (@raffel2019exploring) (already briefly mentioned in [Chapter 9](./transfer-learning-for-nlp-ii.html)) does not fit into the category of sequential tansfer learning but rather belongs to _multi-task learning_ models (cf. Fig. 7.1) in [Chapter 7](./transfer-learning-for-nlp-i.html))) since it is trained on multiple tasks at once. This is possible due to transformation of the entire input _and_ output to strings, which essentially converts every tasks to as seq-to-seq task.
- In May 2020 the [**OpenAI GPT-3**](https://arxiv.org/pdf/2005.14165.pdf) (@brown2020language) shook the NLP community and triggered _a lot_ of subsequent research. This model puts, as already mentioned in [Chapter 9](./transfer-learning-for-nlp-ii.html), is by far bigger then every previous model and put a special focus on _few-shot learning_-

## Improvements of the Self-Attention mechanism

Recently, there has been a lot effort put in improving the Self-Attention, mostly by reducing its computational cost
and this enabling models to process longer sequences. One interesting article has already been discussed at the end of [Chapter 8](./attention-and-self-attention-for-nlp.html), while another interesting piece of work has been published recently by @wang2020linformer: The so-called [Linformer](https://arxiv.org/pdf/2006.04768.pdf)

## Evaluation and Interpretability

While "traditional" Benchmark (collections) have been discussed in [Chapter 11](./resources-and-benchmarks-for-nlp.html), there is a lot of ongoing research about proper evaluation and interpretability of NLP models. Here are just two examples of impressive work, which was published very recently:

- @ribeiro-etal-2020-beyond won the best paper award at ACL 2020 for their article [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)
- Google launched a [Language Interpretability Tool](https://github.com/pair-code/lit) together with an accompanying [research article](https://arxiv.org/pdf/2008.05122.pdf) (@tenney2020language) 

We hope that this little outlook can adequately round off this nice piece of academic work created by extremely motivated students and we hope that you enjoyed reading.