<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Foundations/Applications of Modern NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Foundations/Applications of Modern NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Foundations/Applications of Modern NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-09-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-deep-learning-for-nlp.html"/>
<link rel="next" href="recurrent-neural-networks-and-their-applications-in-nlp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-and-development"><i class="fa fa-check"></i><b>1.1</b> History and development</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#statistical-background"><i class="fa fa-check"></i><b>1.2</b> Statistical Background</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.3</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning-and-system-design-choices"><i class="fa fa-check"></i><b>3.3</b> Hyperparameter Tuning and System Design Choices</a></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.4</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#outlook-and-resources"><i class="fa fa-check"></i><b>3.5</b> Outlook and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extensions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extensions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.2</b> Encoder-Decoder Architecture</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>6</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>6.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>6.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>6.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>7</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="7.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#outline"><i class="fa fa-check"></i><b>7.1</b> Outline</a></li>
<li class="chapter" data-level="7.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>7.2</b> Sequential inductive transfer learning</a><ul>
<li class="chapter" data-level="7.2.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#types-of-transfer-learning"><i class="fa fa-check"></i><b>7.2.1</b> Types of transfer learning</a></li>
<li class="chapter" data-level="7.2.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#feature-extraction-vs.fine-tuning"><i class="fa fa-check"></i><b>7.2.2</b> Feature Extraction vs. Fine-tuning</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#models"><i class="fa fa-check"></i><b>7.3</b> Models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>7.3.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="7.3.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit"><i class="fa fa-check"></i><b>7.3.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="7.3.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>7.3.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>8</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="8.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>8.1</b> Attention</a><ul>
<li class="chapter" data-level="8.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>8.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>8.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="8.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#computational-difference-between-luong--and-bahdanau-attention"><i class="fa fa-check"></i><b>8.1.3</b> Computational Difference between Luong- and Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.4" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>8.1.4</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>8.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="8.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#the-transformer"><i class="fa fa-check"></i><b>8.2.1</b> The Transformer</a></li>
<li class="chapter" data-level="8.2.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers-as-rnns"><i class="fa fa-check"></i><b>8.2.2</b> Transformers as RNNs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>9</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="9.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>9.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>9.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="9.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>9.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="9.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>9.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="9.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>9.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="9.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-prediction"><i class="fa fa-check"></i><b>9.1.5</b> Next-sentence Prediction</a></li>
<li class="chapter" data-level="9.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>9.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>9.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>9.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>9.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="9.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>9.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="9.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>9.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="9.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>9.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="9.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>9.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="9.2.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>9.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>9.3</b> XLNet</a><ul>
<li class="chapter" data-level="9.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>9.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="9.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>9.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="9.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>9.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="9.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>9.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="9.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>9.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="9.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>9.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="9.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>9.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>9.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>10</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="11" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="11.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>11.1</b> Metrics</a></li>
<li class="chapter" data-level="11.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>11.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="11.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>11.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="11.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>11.2.2</b> CoQA</a></li>
<li class="chapter" data-level="11.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>11.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="11.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>11.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="11.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>11.2.5</b> SNLI</a></li>
<li class="chapter" data-level="11.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#overview"><i class="fa fa-check"></i><b>11.2.6</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-trained-models"><i class="fa fa-check"></i><b>11.3</b> Pre-Trained Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#bert"><i class="fa fa-check"></i><b>11.3.1</b> BERT</a></li>
<li class="chapter" data-level="11.3.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#openai-gpt-3"><i class="fa fa-check"></i><b>11.3.2</b> OpenAI GPT-3</a></li>
<li class="chapter" data-level="11.3.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#google-5t"><i class="fa fa-check"></i><b>11.3.3</b> Google 5T</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>11.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-cases-for-nlp.html"><a href="use-cases-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Use-Cases for NLP</a></li>
<li class="chapter" data-level="13" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>13</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="13.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>13.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="13.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>13.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="13.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>13.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="13.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>13.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="13.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>13.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="13.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>13.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="13.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>13.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>13.4</b> Dialog Systems</a><ul>
<li class="chapter" data-level="13.4.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>13.4.1</b> Types</a></li>
<li class="chapter" data-level="13.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>13.4.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#image-captioning-system"><i class="fa fa-check"></i><b>13.5</b> Image Captioning System</a><ul>
<li class="chapter" data-level="13.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#experiments"><i class="fa fa-check"></i><b>13.5.1</b> Experiments</a></li>
<li class="chapter" data-level="13.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#implementation"><i class="fa fa-check"></i><b>13.5.2</b> Implementation</a></li>
<li class="chapter" data-level="13.5.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#results"><i class="fa fa-check"></i><b>13.5.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>13.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>14</b> Epilogue</a><ul>
<li class="chapter" data-level="14.1" data-path="epilogue.html"><a href="epilogue.html#new-influentioal-architectures"><i class="fa fa-check"></i><b>14.1</b> New influentioal architectures</a></li>
<li class="chapter" data-level="14.2" data-path="epilogue.html"><a href="epilogue.html#improvements-of-the-selfattention-mechanism"><i class="fa fa-check"></i><b>14.2</b> Improvements of the SelfAttention mechanism</a></li>
<li class="chapter" data-level="14.3" data-path="epilogue.html"><a href="epilogue.html#evaluation-and-interpretability"><i class="fa fa-check"></i><b>14.3</b> Evaluation and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="foundationsapplications-of-modern-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Foundations/Applications of Modern NLP</h1>
<p><em>Authors: Viktoria Szabo</em></p>
<p><em>Supervisor: Christian Heumann</em></p>
<p>Word embeddings can be seen as the beginning of modern natural language processing. They are widely used in every kind of NLP task. One of the advantages is that one can download and use pretrained word embeddings. With this, it is possible to save a lot of time for training the final model. But if the task is not a standard one it is usually better to train own embeddings to get a better model performance for the specific task. In the following the evolution from sparse representations of words to dense word embeddings will be outlined in the first part. After that the calculation methods for word embeddings within a neural network language model and with word2vec and GloVe will be described. The third part shows how to improve the model performance regardless of the chosen model class based on hyperparameter tuning and system design choices and explains some model expansion to tackle problems of the aforementioned methods. The evaluation of word embeddings on different tasks and datasets is another topic which will be covered in the fourth part of this chapter. Finally some resources to download pretrained word embeddings will be presented.</p>
<div id="the-evolution-of-word-embeddings" class="section level2">
<h2><span class="header-section-number">3.1</span> The Evolution of Word Embeddings</h2>
<p>Since computers work with numeric representations, converting the text and sentences to be analyzed into numbers is unavoidable. One-Hot Encoding and Bag-of-Words (BOW) are two simple approaches to how this could be accomplished. These methods are usually used as input for calculating more elaborate word representations called word embeddings.<br />
The <strong>One-Hot Encoding</strong> labels each word in the vocabulary with an index. Let <span class="math inline">\(n\)</span> be size of the vocabulary, then each word is represented by a vector with dimension <span class="math inline">\(n\)</span>. Every vector entry is zero except for the one corresponding to its index, which is set to <span class="math inline">\(1\)</span>. A sentence is represented as a matrix of shape (<span class="math inline">\(n\times n\)</span>) where <span class="math inline">\(n\)</span> is the number of unique words in the sentence or a document. In figure <a href="foundationsapplications-of-modern-nlp.html#fig:onehot-bow-01-01">3.1</a> an example for a one-hot encoded word is shown on the left side.<br />
A more elaborate approach compared to the first one is called <strong>Bag-of-Words (BOW)</strong> and belongs to the count-based approaches. This approach counts the occurrences and co-occurrences of all distinct words in a document or a text chunk. Each text chunk is then represented by a row in a matrix, where the columns are the words. That means that, compared to the One-Hot Encoding, this approach already incorporates some context information in sentences and text chunks. An example for this kind of representation can be seen on the right side in figure <a href="foundationsapplications-of-modern-nlp.html#fig:onehot-bow-01-01">3.1</a>.</p>
<div class="figure"><span id="fig:onehot-bow-01-01"></span>
<img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_one-hot.png" alt="One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure." width="50%" /><img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_bow.png" alt="One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure." width="50%" />
<p class="caption">
FIGURE 3.1: One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure.
</p>
</div>
<p>These approaches definitely have some <strong>positive points</strong> about them. They are very simple to construct, robust to changes, and it was observed that simple models trained on large amounts of data outperform complex systems trained on less data. Bag-of-Words is especially useful if the number of distinct words is small and the sequence of the words doesn’t play a key role, like in sentiment analysis. Without calculating word embeddings on top of them, these approaches should only be used if there is a small number of distinct words in the document, the words are not meaningfully correlated and there is a lot of data to learn from.<br />
Nevertheless, the <strong>problems</strong> that arise from these approaches usually outweigh the positive points. The most obvious one is that these approaches lead to very sparse input vectors, that means large vectors with relatively few non-zero values. Many machine learning models won’t work well with high dimensional and sparse features (<span class="citation">Goldberg (<a href="#ref-goldberg2016primer">2016</a>)</span>). Neural networks in particular struggle with this type of data. And with growing vocabulary the feature size vectors also increases by the same length. So, the dimensionality of these approaches is the same as the number of different words in your text. That means estimating more parameters and therefore using exponentially more data is required to build a reasonably generalizable model. This is known as the curse of dimensionality. But these problems can be solved with dimensionality reduction methods such as Principal Component Analysis or feature selection models where less informative context words, such as <em>the</em> and <em>a</em> are dropped.<br />
The major drawback of these methods is that there is no notion of similarity between words. That means words like <em>cat</em> and <em>tiger</em> are represented as similar as <em>cat</em> and <em>car</em>. If the words <em>cat</em> and <em>tiger</em> would be represented as similar words one could use the information won from the more frequent word “cat” for sentences in which the less frequent word <em>tiger</em> appears. If the word embedding for <em>tiger</em> is similar to that of <em>cat</em> the network model can take a similar path instead of having to learn how to handle it completely anew.<br />
</p>
<p>To overcome these problems <strong>word embeddings</strong> were introduced. Word embeddings use continuous vectors to represent each word in a vocabulary. These vectors have <span class="math inline">\(n\)</span> dimensions, usually between 100 and 500, which represent different aspects of the word. With this approach, semantic similarity can be maintained in the representation and generalization may be achieved. Through these vectors, the words are mapped to a continuous vector space, called a semantic space, where semantically similar words occur close to each other, while more dissimilar words are far from each other. Figure <a href="foundationsapplications-of-modern-nlp.html#fig:word-embedding1">3.2</a> shows a simple example to convey the idea behind this approach. In this fictional example the words are represented by a two-dimensional vector, which represents the cuteness and scariness of the creatures.</p>
<div class="figure"><span id="fig:word-embedding1"></span>
<img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_word_embeddings_1.png" alt="Example for word embeddings with two dimensions. Source: Own figure"  />
<p class="caption">
FIGURE 3.2: Example for word embeddings with two dimensions. Source: Own figure
</p>
</div>
<p>If the goal is to represent higher dimensional word vectors one could use dimension reduction methods such as principal component analysis (PCA) to break down the number of dimensions into two or three and then plot the words. There is an example of this for selected country names and their capitals in figure <a href="foundationsapplications-of-modern-nlp.html#fig:word-embedding2">3.3</a>.The country names all have negative values on the x-axis and the capitals all have positive values on the x-axis. Furthermore, the countries have y-axis values similar to their corresponding capitals.</p>
<div class="figure"><span id="fig:word-embedding2"></span>
<img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_word_embeddings_2.png" alt="Two-dimensional PCA projection of 1000-dimensional word vectors of countries and their capital cities. Source: @Mikolov.2013c"  />
<p class="caption">
FIGURE 3.3: Two-dimensional PCA projection of 1000-dimensional word vectors of countries and their capital cities. Source: <span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span>
</p>
</div>
<p>With such word vectors even algebraic computations become possible as shown in <span class="citation">Tomáš Mikolov, Yih, and Zweig (<a href="#ref-mikolov2013linguistic">2013</a>)</span>. For example, <span class="math inline">\(vector(King)-vector(Man) + vector(Woman)\)</span> results in a vector that is closest to the vector representation of the word <em>Queen</em>. Another possibility to use word embeddings vectors is translation between languages. <span class="citation">Tomas Mikolov, Le, and Sutskever (<a href="#ref-mikolov2013exploiting">2013</a>)</span> showed that they can find word translations by comparing vectors generated from different languages. By searching for a translation one can use the word vector from the source language and search for the closest vector in the target language vector space, this word can then be used as a translation. The reason this works is that if a word vector from one language is similar to the word vector of the other language, this word is used in a similar context. This method can be used to infer missing dictionary entries. An example for this method depicted in figure <a href="foundationsapplications-of-modern-nlp.html#fig:word-embedding3">3.4</a>. In figure <a href="foundationsapplications-of-modern-nlp.html#fig:word-embedding3">3.4</a> the vectors for numbers and animals are depicted on the left side and the same words are depicted on the right side. It can be seen that the vectors for the correct translation align in similar geometric spaces. Again, two-dimensional representation was achieved by using dimension reduction methods.</p>
<div class="figure"><span id="fig:word-embedding3"></span>
<img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_language.png" alt="Distributed word vector representations of numbers and animals in English (left) and Spanish (right). Source: @mikolov2013exploiting"  />
<p class="caption">
FIGURE 3.4: Distributed word vector representations of numbers and animals in English (left) and Spanish (right). Source: <span class="citation">Tomas Mikolov, Le, and Sutskever (<a href="#ref-mikolov2013exploiting">2013</a>)</span>
</p>
</div>
</div>
<div id="methods-to-obtain-word-embeddings" class="section level2">
<h2><span class="header-section-number">3.2</span> Methods to Obtain Word Embeddings</h2>
<p>The basic idea behind learning word embeddings is the so called <em>distributional hypothesis</em> (<span class="citation">Harris (<a href="#ref-Harris.1954">1954</a>)</span>). It states that words that occur in the same contexts tend to have similar meanings. For instance, the words <em>car</em> and <em>truck</em> tend to have similar semantics as they appear in similar contexts, e.g., with words such as <em>road</em>, <em>traffic</em>, <em>transportation</em>, <em>engine</em>, and <em>wheel</em>. Hence machine learning and deep learning algorithms can find representations by themselves by evaluating the context in which a word occurs. Words that are used in similar contexts will be given similar representations. This is usually done as an unsupervised or self-supervised procedure, which is a big advantage. That means word embeddings can be thought of as unsupervised feature extractors for words. However, the methods to find such similarities in the context of words vary. Finding word representations started out with more traditional count-based techniques, which collected word statistics like occurrence and co-occurrence frequencies as seen above with BOW. But these representations often require some sort of dimensionality reduction. Later, when neural networks were introduced into NLP, the so-called predictive techniques, mainly popularized after 2013 with the introduction of word2vec, supplanted the traditional count-based word representations.These models learn what is called <em>dense representations</em> of words, since they directly learn low-dimensional word representations, without needing the additional dimensionality reduction step. In the following an introduction to the best-known predictive approaches to model word embeddings will be given. First, neural network language models, where word embeddings are learnt as a part of the final language model will be discussed. The description of the two popular algorithms word2vec and GloVe, which learn word embeddings in a pre-step before the actual statistical language model, follow afterwards.</p>
<div id="feedforward-neural-network-language-model-nnlm" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Feedforward Neural Network Language Model (NNLM)</h3>
<p><span class="citation">Bengio et al. (<a href="#ref-Bengio.2003">2003</a>)</span> were the first to propose learning word embeddings within a statistical <strong>neural network language model (NNLM)</strong>. The goal of the NNLM model of <span class="citation">Bengio et al. (<a href="#ref-Bengio.2003">2003</a>)</span> is to predict the next word based on a sequence of preceding words. Using a simple feedforward neural network, the model first learns the word embeddings and in a second step the probability function for word sequences. This way, one obtains not only the model itself, but also the learned word representations, which can be used as input for other, potentially unrelated, tasks.<br />
The proposed neural network architecture has an input layer with one-hot encoded word inputs, a linear projection layer for the word embeddings, and a hidden layer with a hyperbolic tangent function, where most of the computation is done, followed by a softmax classifier output layer. The output of the model is a vector of the probability distribution over all words given a specific context. That means a vector with probability scores for each word of the vocabulary. The <em>i</em>-th element of the output vector is the probability estimation <span class="math inline">\(P(w_t = i|context)\)</span>. The softmax classifier is used to guarantee positive probabilities summing to one. It computes the following function:
<span class="math display">\[\widehat{P}(w_t|w_{t-1},...,w_{t-n+1}) = \frac{ e^{Y_{w_t}} }{ \sum_{i} {e^{y_i}} }\]</span>
The <span class="math inline">\(y_i\)</span> are the unnormalized log-probabilities for each output word <span class="math inline">\(i\)</span>, which were computed in the previous layer. The model architecture proposed in <span class="citation">Bengio et al. (<a href="#ref-Bengio.2003">2003</a>)</span> is depicted in figure <a href="foundationsapplications-of-modern-nlp.html#fig:bengio-nnlm">3.5</a>.<br />
When training a neural network, one has to define a loss function <span class="math inline">\(L(\widehat{y}, y)\)</span> stating the loss of predicting <span class="math inline">\(\widehat{y}\)</span> when the true output is <span class="math inline">\(y\)</span>. In the NNLM literature, a cross-entropy loss is very common (see <span class="citation">Goldberg (<a href="#ref-goldberg2016primer">2016</a>)</span>). The <span class="math inline">\(\widehat{y}\)</span> is the network output vector, which was transformed by the softmax classifier and represents the conditional distribution. The <span class="math inline">\(y\)</span> is usually either a one-hot vector for the correct output word or a vector representing the true multinomial probability distribution over the vocabulary given the specific context. Then the parameter <span class="math inline">\(\phi\)</span> of the neural network (for example the weights for the embedding vectors) is iteratively changed in order to minimize the loss <span class="math inline">\(L\)</span> over the training examples.<br />
This is usually done with the <strong>stochastic gradient descent (SGD)</strong> optimizer where the gradient is obtained via <strong>backpropagation</strong>. The gradient descent optimizer tries to find the direction of the strongest descent via partial derivatives and updates the parameter <span class="math inline">\(\phi\)</span> accordingly. The learning rate <span class="math inline">\(\varepsilon\)</span> defines the size of the step in this direction (see <span class="citation">Goldberg (<a href="#ref-goldberg2016primer">2016</a>)</span>).<br />
In <span class="citation">Bengio et al. (<a href="#ref-Bengio.2003">2003</a>)</span> a gradient ascent optimizer is used, which performs the following iterative update after presenting the t-th word of the training corpus:
<span class="math display">\[\theta \leftarrow  \theta + \varepsilon\frac{\partial log\widehat{P}(w_t|w_{t-1},...,w_{t-n+1})}{\partial \theta }\]</span>
The method by which parameter adjustments are made during training so they can be optimized is called backpropagation. Backpropagation essentially consists of six steps:</p>
<ol style="list-style-type: decimal">
<li>Initialization of the parameter of the network</li>
<li>Calculation of <span class="math inline">\(y(x_i)\)</span> for the inputs <span class="math inline">\(x_i\)</span></li>
<li>Determining the cost of the inputs <span class="math inline">\(x_i\)</span></li>
<li>Calculation of the partial derivatives of the loss for each parameter</li>
<li>Update the parameter in the network using the partial derivatives calculated in step 4</li>
<li>Return to step 2 and continue the procedure until the partial derivatives of the loss approach zero</li>
</ol>
<div class="figure"><span id="fig:bengio-nnlm"></span>
<img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_bengio_nnlm.png" alt="Architecture for NNLM proposed by @Bengio.2003 .  Source: @Bengio.2003"  />
<p class="caption">
FIGURE 3.5: Architecture for NNLM proposed by <span class="citation">Bengio et al. (<a href="#ref-Bengio.2003">2003</a>)</span> . Source: <span class="citation">Bengio et al. (<a href="#ref-Bengio.2003">2003</a>)</span>
</p>
</div>
</div>
<div id="word2vec" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Word2Vec</h3>
<p>In 2013 <span class="citation">Tomas Mikolov, Chen, et al. (<a href="#ref-mikolov2013efficient">2013</a>)</span> proposed the two word2vec algorithms which led to a wave in NLP that popularized word embeddings. In contrast to the NNLM model above, the word2vec algorithms are not used for a statistical language modeling goal, but rather to learn the word embeddings themselves. The two word2vec algorithms named Continuous Bag-of-Words (CBOW) and Continuous Skip-Gram use shallow neural networks with an input layer, a projection layer, and an output layer. This means compared to the previously explained feedforward NNLM, the non-linear hidden layer is removed.<br />
The general idea behind CBOW is to predict the focus word based on a window of context words. The order of context words does not influence the prediction, thus the name Bag-of-Words. In contrast, Skip-Gram tries to predict the context words given a source word. This is done while adjusting the initial weights during training so that a loss function is reduced.<br />
In the <strong>CBOW</strong> architecture the <span class="math inline">\(N\)</span> input (context) words are each one-hot encoded vectors of size <span class="math inline">\(V\)</span>, where <span class="math inline">\(V\)</span> is the size of the vocabulary. Compared to the NNLM model CBOW uses both previous and following words as context instead of only the previous words. The projection layer is a standard fully connected (dense) layer which has the dimensionality <span class="math inline">\(1 \times D\)</span>, where <span class="math inline">\(D\)</span> is the size of the dimensions for the word embeddings. The projection layer is shared for all words. That means all words get projected into the same position in a linear manner, where the vectors are averaged. The output layer outputs probabilities for the target words from the vocabulary and has a dimensionality of <span class="math inline">\(V\)</span>. That means the output is a probability distribution over all words of the vocabulary as in the NNLM model, where the prediction is the word with the highest probability. But instead of using a standard softmax classifier as in the NNLM model the authors propose to use a log-linear hierarchical softmax classifier for the calculation of the probabilities. The model architecture is shown in figure <a href="foundationsapplications-of-modern-nlp.html#fig:word2vec">3.6</a>.
The <strong>continuous Skip-gram</strong> architecture also uses a log-linear hierarchical softmax classifier with a continuous projection layer, but the input is only one source word, and the output layer consists of as many probability vectors over all words as the chosen number of context words. Also, since the more distant words are usually less related to the source word, the skip-gram model weighs nearby context words more heavily than more distant context words by sampling less from those words in the training examples. The model architecture for skip-gram can be found on the right side of figure <a href="foundationsapplications-of-modern-nlp.html#fig:word2vec">3.6</a>.</p>
<div class="figure"><span id="fig:word2vec"></span>
<img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_word2vec.png" alt="Learning word embeddings with the model architecture of CBOW and Skip-Gram.  Source: @mikolov2013efficient"  />
<p class="caption">
FIGURE 3.6: Learning word embeddings with the model architecture of CBOW and Skip-Gram. Source: <span class="citation">Tomas Mikolov, Chen, et al. (<a href="#ref-mikolov2013efficient">2013</a>)</span>
</p>
</div>
<p>As said before the word2vec models use hierarchical softmax, where the vocabulary is represented as a Huffman binary tree, instead of the standard softmax classifier explained in the section before. With hierarchical softmax the size of the output vector can be reduced from the vocabulary size <span class="math inline">\(V\)</span> to the logarithm to base 2 of <span class="math inline">\(V\)</span>, which is a dramatic change in computational complexity and number of operations needed for the algorithm. Further explanations for this method can be found in <span class="citation">Morin and Bengio (<a href="#ref-morin2005hierarchical">2005</a>)</span>. For both models <span class="citation">Tomas Mikolov, Chen, et al. (<a href="#ref-mikolov2013efficient">2013</a>)</span> use gradient descent optimization and backpropagation as described in the previous chapter.<br />
<span class="citation">Tomas Mikolov, Chen, et al. (<a href="#ref-mikolov2013efficient">2013</a>)</span> show that their word2vec algorithms outperform a lot of other standard NNLM models. CBOW is faster while skip-gram does a better job for infrequent words. Skip-gram works well with small amounts of training data and has good representations for words that are considered rare, whereas CBOW trains several times faster and has slightly better accuracy for frequent words.</p>
</div>
<div id="glove" class="section level3">
<h3><span class="header-section-number">3.2.3</span> GloVe</h3>
<p>GloVe stands for <strong>Glo</strong>bal <strong>Ve</strong>ctor word representation, which emphasizes the global character of this model. Unlike the previously described algorithms like word2vec, GloVe not only relies on local context information but also incorporates global co-occurrence statistics. Instead of extracting the embeddings from a neural network that is designed to perform a task like predicting neighboring words (CBOW) or predicting the focus word (Skip-Gram), the embeddings are optimized directly, so that the dot product of two word vectors is equal to the log of the number of times the two words will occur near each other. The model builds on the possibility to derive semantic relationships between words from the co-occurrence matrix and that the ratio of co-occurrence probabilities of two words with a third word is more indicative of semantic association than a direct co-occurrence probability (see <span class="citation">Pennington et al. (<a href="#ref-Pennington.2014">2014</a>)</span>).<br />
Let <span class="math inline">\(P_{ij} = P(j|i) = X_{ij}/X_i\)</span> be the probability that word <span class="math inline">\(j\)</span> appears in the context of word <span class="math inline">\(i\)</span>. Figure <a href="foundationsapplications-of-modern-nlp.html#fig:glove">3.7</a> shows an example with the words <em>ice</em> and <em>steam</em>. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with other words <span class="math inline">\(k\)</span>.<br />
For words like <em>solid</em> which are related to <em>ice</em> but not <em>steam</em> the ratio is large (&gt;1), while for words like <em>gas</em>, which is related to <em>steam</em> but not to <em>ice</em>, the ratio is small (&lt;1). For words like <em>water</em> or <em>fashion</em>, which are either related to both of the words or to none the ratios are close to 1. It is evident that the comparison of co-occurrence probabilities with a third word (<span class="math inline">\(P_{ik}/P_{jk}\)</span>) is more indicative of the semantic meanings of words <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> and is better at identifying words (<em>solid</em> and <em>gas</em>) that distinguish <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> and words that do not (<em>water</em> and <em>fashion</em>) than the raw probabilities.</p>
<div class="figure"><span id="fig:glove"></span>
<img src="figures/01-01-foundations-applications-of-modern-NLP/01-01_glove_ratios.png" alt="Co-occurrence probabilities for target words ice and steam with selected context words.  Source: @Pennington.2014"  />
<p class="caption">
FIGURE 3.7: Co-occurrence probabilities for target words ice and steam with selected context words. Source: <span class="citation">Pennington et al. (<a href="#ref-Pennington.2014">2014</a>)</span>
</p>
</div>
<p><span class="citation">Pennington et al. (<a href="#ref-Pennington.2014">2014</a>)</span> tried to incorporate the ratio <span class="math inline">\(P_{ik}/P_{jk}\)</span> into computing word embeddings. They propose an optimization problem which aims at fulfilling the following objective:
<span class="math display">\[w_i^Tw_k + b_i + b_k = log(X_{ik})\]</span>
Where <span class="math inline">\(b_i\)</span> and <span class="math inline">\(b_k\)</span> are bias terms for word <span class="math inline">\(w_i\)</span> and probe word <span class="math inline">\(w_k\)</span> and <span class="math inline">\(X_{ik}\)</span> is the number of times <span class="math inline">\(w_i\)</span> co-occurs with <span class="math inline">\(w_k\)</span>. Fulfilling this objective minimizes the difference between the dot product of <span class="math inline">\(w_i\)</span> and <span class="math inline">\(w_k\)</span> and the logarithm of their number of co-occurrences. In other words, the optimization results in the construction of vectors <span class="math inline">\(w_i\)</span> and <span class="math inline">\(w_k\)</span> whose dot product gives a good estimate of their transformed co-occurrence counts. To solve this optimization problem, they reformulate the equation as a least squares problem and introduce a weighting function, since rare co-occurrences add both noise to the model and less information than more frequent co-occurrences.</p>
</div>
</div>
<div id="hyperparameter-tuning-and-system-design-choices" class="section level2">
<h2><span class="header-section-number">3.3</span> Hyperparameter Tuning and System Design Choices</h2>
<p>Once adapted across methods, hyperparameter tuning significantly improves performance in every task. <span class="citation">Levy, Goldberg, and Dagan (<a href="#ref-levy2015improving">2015</a>)</span> showed that in a lot of cases, changing the setting of a single hyperparameter could yield a greater increase in performance than switching to a better algorithm or training on a larger corpus. They conducted a series of experiments where they assessed the contributions of diverse hyperparameters. They also show that when all methods are allowed to tune a similar set of hyperparameters, their performance is largely comparable. However they also found that choosing the wrong hyperparameter settings can actually degrade performance of a model. That is why tuning the hyperparameter fitting the specific context is very important. Depending on the model one chooses there are a lot of hyperparameters available for tuning. These are parameters like the number of epochs, batch-size, learning rate, embedding size, window size, corpus size et cetera. In the following the focus will lie on hyperparameters which are frequently discussed. Furthermore, some system design and setup choices will be described which will tackle some of the problems posed by the algorithms mentioned above.</p>
<p><strong>Word embedding size</strong><br />
The question of how many embedding dimensions should be used is mostly answered empirically. It depends on the task, computing capacity and vocabulary. The trade-off is between accuracy and computational concerns. More dimensions could potentially increase the accuracy of the representations; since the vectors can capture more aspects of the word. But more dimensions also mean higher computing time and effort. In practice word embedding vectors with dimensions around 50 to 300 are usually used as a rule of thumb (see <span class="citation">Goldberg (<a href="#ref-goldberg2016primer">2016</a>)</span>).<br />
In contrast, <span class="citation">Patel and Bhattacharyya (<a href="#ref-patel2017towards">2017</a>)</span> found that the dimension size should be chosen based on some text corpus statistics. One can calculate a lower bound from the number of pairwise equidistant words of the corpus vocabulary. Choosing a dimension size below this bound results in a loss of quality of learned word embeddings. This result was tested empirically for the skip-gram algorithm (see <span class="citation">Patel and Bhattacharyya (<a href="#ref-patel2017towards">2017</a>)</span>).<br />
<span class="citation">Pennington et al. (<a href="#ref-Pennington.2014">2014</a>)</span> compare performance of the GloVe model for embedding sizes from 1 to 600 for different evaluation tasks (semantic, syntactic and overall). They found that after around 200 dimensions the performance increase begins to stagnate.</p>
<p><strong>Context Window</strong><br />
In traditional approaches the context window around the focus word is constant-sized and unweighted. For example, if the symmetrical context size is 5 then the 5 words before the focus word and 5 words after the focus words are the context window. It is also possible to use an asymmetric context window, which for example only uses words that appear before the focus word. A window size of 5 is commonly used to capture broad topic/domain information like what other words are used in related discussions (i.e. <em>dog</em>, <em>bark</em> and <em>leash</em> will be grouped together, as well as <em>walked</em>, <em>run</em> and <em>walking</em>), whereas smaller windows contain more specific information about the focus word and produce more functional and syntactic similarities (i.e. <em>Poodle</em>, <em>Pitbull</em>, <em>Rottweiler</em>, or <em>walking</em>, <em>running</em>, <em>approaching</em>) (see <span class="citation">Goldberg and Levy (<a href="#ref-goldberg2014word2vec">2014</a>)</span>, <span class="citation">Goldberg (<a href="#ref-goldberg2016primer">2016</a>)</span>).<br />
Since words which appear closer to the focus word are usually more indicative of its meaning it is possible to give the context words weights according to their distance from the focus word. Both word2vec and GloVe use such weighting schemes. GloVe’s implementation weights contexts using the harmonic function, e.g. a context word three tokens away will have 1/3 as a weight. Word2vec’s uses weightings of the distance from the focus word divided by the window size. For example, a size-5 window will weigh its contexts by <span class="math inline">\(\frac {5}{5}\)</span>, <span class="math inline">\(\frac {4}{5}\)</span>, <span class="math inline">\(\frac {3}{5}\)</span>, <span class="math inline">\(\frac {2}{5}\)</span>, <span class="math inline">\(\frac {1}{5}\)</span> (Levy et al. 2015).<br />
A performance comparison for GloVe using different window sizes for different evaluation tasks (semantic, syntactic and overall) can be found in <span class="citation">Pennington et al. (<a href="#ref-Pennington.2014">2014</a>)</span>. Which shows that, depending on the task, larger performance gains can be expected from a larger window size. But for all three tasks the performance gains decrease after a window size of 4.</p>
<p><strong>Document Context</strong><br />
Instead of using a few words as the context window one could consider all the other words that appear with the focus word in the same sentence, paragraph, or document. One can either consider this as using very large window sizes or, as in the doc2vec algorithm from <span class="citation">Le and Mikolov (<a href="#ref-le2014distributed">2014</a>)</span>, add another embedding vector for a whole paragraph to the other context word vectors. These approaches will result in word vectors that capture topical similarity (words from the same topic, i.e. words that one would expect to appear in the same document, are likely to receive similar vectors). (<span class="citation">Goldberg (<a href="#ref-goldberg2016primer">2016</a>)</span>; <span class="citation">Le and Mikolov (<a href="#ref-le2014distributed">2014</a>)</span>)</p>
<p><strong>Subsampling of Frequent Words</strong><br />
<span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> proposed for their word2vec algorithms to use a subsample of the most frequent words. Very frequent words are often so-called stop-words, like <em>the</em> or <em>a</em>, which do not provide much information. Using fewer of these frequent words leads to a significant speedup and improves accuracy of the representations of less frequent words (see <span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span>). The method randomly removes words <span class="math inline">\(w\)</span> with a probability <span class="math inline">\(p\)</span> that occur more often than a certain threshold <span class="math inline">\(t\)</span>. In <span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> the probability <span class="math inline">\(p\)</span> is defined as follows:
<span class="math display">\[P(w_i) = 1-\sqrt{\frac{t}{f(w_i)}}\]</span>
where <span class="math inline">\(f\)</span> marks the word’s frequency in the text corpus. In <span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> the threshold <span class="math inline">\(t\)</span> is set to <span class="math inline">\(10^{-5}\)</span>, but generally this parameter is open for tuning. In <span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> this subsampling is done before processing the text corpus. This leads to an artificial enlargement of the context window size. One could perform the subsampling step without affecting the context window size, but <span class="citation">Levy, Goldberg, and Dagan (<a href="#ref-levy2015improving">2015</a>)</span> found that it does not affect performance too much.</p>
<p><strong>Negative Sampling</strong><br />
In their first paper <span class="citation">Tomas Mikolov, Chen, et al. (<a href="#ref-mikolov2013efficient">2013</a>)</span> proposed using hierarchical softmax instead of the standard softmax function to speed up the calculation in the neural network. But later they published a new method called negative sampling, which is even more efficient in the calculation of word embeddings. The negative sampling approach is based on the skip-gram algorithm, but it optimizes a different objective. It maximizes a function of the product of word and context pairs <span class="math inline">\((w, c)\)</span> that occur in the training data, and minimizes it for negative examples of word and context pairs <span class="math inline">\((w, c_n)\)</span> that do not occur in the training corpus. The negative examples are created by drawing <span class="math inline">\(k\)</span> negative examples for each observed <span class="math inline">\((w, c)\)</span> pair. <span class="math inline">\(k\)</span> can be tuned as a hyperparameter. <span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> found that <span class="math inline">\(k\)</span> in the range 5-20 is useful for small training datasets; while for large datasets <span class="math inline">\(k\)</span> can be as small as 2-5 (see <span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span>; <span class="citation">Goldberg and Levy (<a href="#ref-goldberg2014word2vec">2014</a>)</span>).</p>
<p><strong>Subword Information</strong><br />
An individual word can convey a lot of information besides the general meaning of the word. Ignoring the internal structure of words can lead to a large loss of information, especially in morphologically rich languages like Finnish or Turkish. Furthermore, coping with completely unseen words not included in the training data, so-called <em>out-of-vocabulary</em> (OOV) words, is not possible if every word gets assigned a distinct new vector representation like in the previously presented models. One way to deal with these problems is to train character-based models instead of word-based models. But as <span class="citation">Goldberg (<a href="#ref-goldberg2016primer">2016</a>)</span> states: “working on the character level is very challenging, as the relationship between form (characters) and function (syntax, semantics) in language is quite loose. Restricting oneself to stay on the character level may be an unnecessarily hard constraint.” A more promising approach to solve the problem of OOV words is to use subword information. In this context <strong>fastText</strong> was introduced in two papers 2016 and 2017 (see <span class="citation">Joulin et al. (<a href="#ref-joulin2016bag">2016</a>)</span> and <span class="citation">Bojanowski et al. (<a href="#ref-bojanowski2017enriching">2017</a>)</span>). fastText builds upon the previously described continuous skip-gram model. But instead of learning vectors for words directly as done by skip-gram, fastText represents each word as an n-gram of characters. So, for example, take the word, <em>planning</em> with n=3, the fastText representation of this word is &lt;pl, pla, lan, ann, nni, nin, ing, ng&gt;, where the angular brackets indicate the beginning and end of the word. This helps capture the meaning of shorter words inside longer words and allows the embeddings to understand suffixes and prefixes. In addition to these n-grams fastText learns embeddings to the whole word. <span class="citation">Bojanowski et al. (<a href="#ref-bojanowski2017enriching">2017</a>)</span> extracted all the n-grams for n greater or equal to 3 and smaller or equal to 6. They also state that “the optimal choice of length ranges depends on the considered task and language and should be tuned appropriately”. In the end one word will be represented by the sum of the vector representations of its n-grams and the word itself. In the case of an unseen word (OOV), the corresponding embedding is induced by averaging the vector representations of its constituent character n-grams. Hence fastText performs well when having data with a large number of rare words.</p>
<p><strong>Phrase representation</strong><br />
The models described in the previous section all focus on individual words as input. But there are many words which will only be meaningful in combination with other words, or which change meaning completely when paired up with another word. Therefore, there are phrases with meanings that are more than a simple composition of the meanings of their individual words. Often these are names like “New York” for a city or “Toronto Raptors” for a basketball team. Since the meaning is changed completely when evaluating the combination of these words one embedding must be learned for the whole phrase instead of using the embeddings of the individual words.<br />
<span class="citation">Tomas Mikolov, Sutskever, et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> proposed an approach to deal with such phrases. First, they use the following score to find them in the text corpus:
<span class="math display">\[score(w_i,w_j) = \frac{count(w_iw_j)-\delta }{count(w_i)\times count(w_j)}\]</span>
With this score they try to find words that appear frequently together, and infrequently in other contexts. The score compares the appearance of two words together to their appearance alone in the text corpus. The <span class="math inline">\(\delta\)</span> is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed. When the score of a word pair is above a chosen threshold value it will be used as a phrase. They repeat this calculation 2 to 4 times with decreasing threshold value, allowing longer phrases that consist of several words to be formed.</p>
</div>
<div id="evaluation-methods" class="section level2">
<h2><span class="header-section-number">3.4</span> Evaluation Methods</h2>
<p>The use of different algorithms, hyperparameter or system choices need to be evaluated and compared in their performance to make a reasonable choice. Therefore, there are several tasks and datasets which are used to evaluate word embeddings. In this chapter the two most common tasks and five corresponding datasets for each task will be presented. The dataset collection was done by <span class="citation">Bakarov (<a href="#ref-bakarov2018survey">2018</a>)</span>, also refer to this paper for a more thorough examination of the evaluation tasks.</p>
<p><strong>Word Similarity Task</strong><br />
The word similarity task tries to evaluate the distances between embedding vectors, which should represent their similarity, by comparing them to similarity scores given by humans. The more similar the embedding distance to the human score the better the embedding. The word vectors are evaluated by ranking the pairs according to their cosine similarities, and measuring the correlation (Spearman’s ρ) with the human ratings. These are five datasets used to evaluate word similarity ranked by their size:</p>
<ol style="list-style-type: decimal">
<li>SimVerb-3500, 3 500 pairs of verbs assessed by semantic similarity with a scale from 0 to 4 (<span class="citation">Gerz et al. (<a href="#ref-gerz2016simverb">2016</a>)</span>).</li>
<li>MEN, 3 000 pairs assessed by semantic relatedness with a discrete scale from 0 to 50 (<span class="citation">Bruni, Tran, and Baroni (<a href="#ref-bruni2014multimodal">2014</a>)</span>).</li>
<li>RW (acronym for Rare Word), 2 034 pairs of words with low occurrences (rare words) assessed by semantic similarity with a scale from 0 to 10 (<span class="citation">Luong, Socher, and Manning (<a href="#ref-luong2013better">2013</a>)</span>).</li>
<li>SimLex-999, 999 pairs assessed with a strong respect to semantic similarity with a scale from 0 to 10 (<span class="citation">Hill, Reichart, and Korhonen (<a href="#ref-hill2015simlex">2015</a>)</span>).</li>
</ol>
<p><strong>Word analogy task</strong><br />
In the word analogy task word relations are predicted in the form <em>“a is to a∗ as b is to b∗”</em>, where b∗ is hidden, and must be guessed from the entire vocabulary. A popular example of an analogy is that <em>king</em> relates to <em>queen</em> as <em>man</em> relates to <em>woman</em>. These are five datasets used to evaluate word analogy ranked by their size:</p>
<ol style="list-style-type: decimal">
<li>WordRep, 118 292 623 analogy questions (4-word tuples) divided into 26 semantic classes (<span class="citation">Gao, Bian, and Liu (<a href="#ref-gao2014wordrep">2014</a>)</span>).</li>
<li>BATS (acronym for Bigger Analogy Test Set), 99 200 questions divided into 4 classes (inflectional morphology, derivational morphology, lexicographic semantics and encyclopedic semantics) and 10 smaller subclasses (<span class="citation">Gladkova and Drozd (<a href="#ref-gladkova2016intrinsic">2016</a>)</span>).</li>
<li>Google Analogy (also called Semantic-Syntactic Word Relationship Dataset), 19 544 questions divided into 2 classes (morphological relations and semantic relations) and 10 smaller subclasses (8 869 semantic questions and 10 675 morphological questions) (<span class="citation">Tomas Mikolov, Chen, et al. (<a href="#ref-mikolov2013efficient">2013</a>)</span>).</li>
<li>SemEval-2012, 10 014 questions divided into 10 semantic classes and 79 subclasses prepared for the SemEval-2017 Task 2 (Measuring Degrees of Relational Similarity) (<span class="citation">Jurgens et al. (<a href="#ref-jurgens2012semeval">2012</a>)</span>).</li>
<li>MSR (acronym for Microsoft Research Syntactic Analogies), 8 000 questions divided into 16 morphological classes (<span class="citation">Tomáš Mikolov, Yih, and Zweig (<a href="#ref-mikolov2013linguistic">2013</a>)</span>).</li>
</ol>
</div>
<div id="outlook-and-resources" class="section level2">
<h2><span class="header-section-number">3.5</span> Outlook and Resources</h2>
<p>The use of word embeddings furthered much development in the field of natural language processing. Still, there are problems word embeddings are often not suited to resolve. When calculating word embeddings, the word order is not taken into account. For some NLP tasks like sentiment analysis, this does not pose a problem. But for other tasks like translation, word order can not be ignored. Recurrent neural networks, which will be presented in the following chapter, are one of the tools to face this difficulty.<br />
Furthermore, there are a lot of words with two or more different meanings. <em>Mouse</em> for example can be understood as an animal or as an operator for a computer. Humans naturally take the context into account, in which the word was used, to infer the meaning. The above described word embeddings are not able to do this. But ELMO, which will be discussed in chapter <a href="./transfer-learning-for-nlp-i.html">Chapter 7</a>, uses contextualized embeddings to solve this problem.<br />
Still there are two additional problems, which will not be addressed in this book. First of the word embeddings are mostly learned from text corpora from the internet, therefore they learn a lot of stereotypes that reflect everyday human culture. Another problem is that there are some domains and languages for which only little training data exists on the internet. The algorithms described above all use large amounts of training data to learn exact word embeddings. Solutions to these problems are still work in progress.<br />
Last but not least some resources for downloading pre-calculated word embeddings will be presented. As stated before, if the task is rather common and the words used are from a general vocabulary, one could use pre-calculated word embeddings for the training of the language model. The first two links lead to websites, where word embeddings learned with GloVe and fastText can be downloaded. These were trained on different training data sources like <em>Wikipedia</em>, <em>Twitter</em> or <em>Common Crawl</em> text. GloVe embeddings can only be downloaded for english words, whereas fastText also offers word embeddings for 157 different languages. The last link leads to a website, which is maintained by the Language Technology Group at the University of Oslo and offers word embeddings for many different languages and models.</p>
<p><strong>Glove</strong>:<br />
<a href="https://nlp.stanford.edu/projects/glove/" class="uri">https://nlp.stanford.edu/projects/glove/</a></p>
<p><strong>fastText</strong>:<br />
<a href="https://fasttext.cc/docs/en/english-vectors.html" class="uri">https://fasttext.cc/docs/en/english-vectors.html</a></p>
<p><strong>Different Models and different languages</strong>:<br />
<a href="http://vectors.nlpl.eu/repository/" class="uri">http://vectors.nlpl.eu/repository/</a></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bakarov2018survey">
<p>Bakarov, Amir. 2018. “A Survey of Word Embeddings Evaluation Methods.” <em>arXiv Preprint arXiv:1801.09536</em>.</p>
</div>
<div id="ref-Bengio.2003">
<p>Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. “A Neural Probabilistic Language Model.” <em>Journal of Machine Learning Research</em>, no. 3: 1137–55.</p>
</div>
<div id="ref-bojanowski2017enriching">
<p>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. “Enriching Word Vectors with Subword Information.” <em>Transactions of the Association for Computational Linguistics</em> 5. MIT Press: 135–46.</p>
</div>
<div id="ref-bruni2014multimodal">
<p>Bruni, Elia, Nam-Khanh Tran, and Marco Baroni. 2014. “Multimodal Distributional Semantics.” <em>Journal of Artificial Intelligence Research</em> 49: 1–47.</p>
</div>
<div id="ref-gao2014wordrep">
<p>Gao, Bin, Jiang Bian, and Tie-Yan Liu. 2014. “Wordrep: A Benchmark for Research on Learning Word Representations.” <em>arXiv Preprint arXiv:1407.1640</em>.</p>
</div>
<div id="ref-gerz2016simverb">
<p>Gerz, Daniela, Ivan Vulić, Felix Hill, Roi Reichart, and Anna Korhonen. 2016. “Simverb-3500: A Large-Scale Evaluation Set of Verb Similarity.” <em>arXiv Preprint arXiv:1608.00869</em>.</p>
</div>
<div id="ref-gladkova2016intrinsic">
<p>Gladkova, Anna, and Aleksandr Drozd. 2016. “Intrinsic Evaluations of Word Embeddings: What Can We Do Better?” In <em>Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for Nlp</em>, 36–42.</p>
</div>
<div id="ref-goldberg2016primer">
<p>Goldberg, Yoav. 2016. “A Primer on Neural Network Models for Natural Language Processing.” <em>Journal of Artificial Intelligence Research</em> 57: 345–420.</p>
</div>
<div id="ref-goldberg2014word2vec">
<p>Goldberg, Yoav, and Omer Levy. 2014. “Word2vec Explained: Deriving Mikolov et Al.’s Negative-Sampling Word-Embedding Method.” <em>arXiv Preprint arXiv:1402.3722</em>.</p>
</div>
<div id="ref-Harris.1954">
<p>Harris, Zellig S. 1954. “Distributional Structure.” <em>WORD</em> 10 (2-3): 146–62.</p>
</div>
<div id="ref-hill2015simlex">
<p>Hill, Felix, Roi Reichart, and Anna Korhonen. 2015. “Simlex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation.” <em>Computational Linguistics</em> 41 (4). MIT Press: 665–95.</p>
</div>
<div id="ref-joulin2016bag">
<p>Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. “Bag of Tricks for Efficient Text Classification.” <em>arXiv Preprint arXiv:1607.01759</em>.</p>
</div>
<div id="ref-jurgens2012semeval">
<p>Jurgens, David, Saif Mohammad, Peter Turney, and Keith Holyoak. 2012. “Semeval-2012 Task 2: Measuring Degrees of Relational Similarity.” In <em>* SEM 2012: The First Joint Conference on Lexical and Computational Semantics–Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (Semeval 2012)</em>, 356–64.</p>
</div>
<div id="ref-le2014distributed">
<p>Le, Quoc, and Tomas Mikolov. 2014. “Distributed Representations of Sentences and Documents.” In <em>International Conference on Machine Learning</em>, 1188–96.</p>
</div>
<div id="ref-levy2015improving">
<p>Levy, Omer, Yoav Goldberg, and Ido Dagan. 2015. “Improving Distributional Similarity with Lessons Learned from Word Embeddings.” <em>Transactions of the Association for Computational Linguistics</em> 3. MIT Press: 211–25.</p>
</div>
<div id="ref-luong2013better">
<p>Luong, Minh-Thang, Richard Socher, and Christopher D Manning. 2013. “Better Word Representations with Recursive Neural Networks for Morphology.” In <em>Proceedings of the Seventeenth Conference on Computational Natural Language Learning</em>, 104–13.</p>
</div>
<div id="ref-mikolov2013efficient">
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv Preprint arXiv:1301.3781</em>.</p>
</div>
<div id="ref-mikolov2013exploiting">
<p>Mikolov, Tomas, Quoc V Le, and Ilya Sutskever. 2013. “Exploiting Similarities Among Languages for Machine Translation.” <em>arXiv Preprint arXiv:1309.4168</em>.</p>
</div>
<div id="ref-Mikolov.2013c">
<p>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” <em>Advances in Neural Information Processing Systems</em>, 3111–9.</p>
</div>
<div id="ref-mikolov2013linguistic">
<p>Mikolov, Tomáš, Wen-tau Yih, and Geoffrey Zweig. 2013. “Linguistic Regularities in Continuous Space Word Representations.” In <em>Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 746–51.</p>
</div>
<div id="ref-morin2005hierarchical">
<p>Morin, Frederic, and Yoshua Bengio. 2005. “Hierarchical Probabilistic Neural Network Language Model.” In <em>Aistats</em>, 5:246–52. Citeseer.</p>
</div>
<div id="ref-patel2017towards">
<p>Patel, Kevin, and Pushpak Bhattacharyya. 2017. “Towards Lower Bounds on Number of Dimensions for Word Embeddings.” In <em>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, 31–36.</p>
</div>
<div id="ref-Pennington.2014">
<p>Pennington, Jeffrey, Richard Socher, Manning, and Christopher D. 2014. “GloVe: Global Vectors for Word Representation.” <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-deep-learning-for-nlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="recurrent-neural-networks-and-their-applications-in-nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/01-01-foundations-applications-of-moderln-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
