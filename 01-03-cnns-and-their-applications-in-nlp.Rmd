---
header-includes: \usepackage{booktabs} \usepackage{longtable} \usepackage{array} \usepackage{multirow}
  \usepackage[table]{xcolor} \usepackage{wrapfig} \usepackage{float} \floatplacement{figure}{H}
output:
  bookdown::html_document2:
     fig_caption: true
     keep_tex: yes
     number_sections: yes
bibliography: book.bib
link-citations: yes
---

# Convolutional neural networks and their applications in NLP

*Authors: Rui Yang*

*Supervisor: Prof. Dr. Christian Heumann*

## Introduction to Basic Architecture of CNN 

This section presents a brief introduction of the Convolutional neural network (CNN) and its main elements, based on which it would be more effective for further exploration of the applications of Convolutional neural network in the field of NLP.


```{r figs,message=FALSE, echo=FALSE, fig.cap="Basic structure of CNN", fig.pos="ht",out.width = '105%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/basic_structure.png")
```

As illustrated in Figure \@ref(fig:figs), a convolutional neural networks includes successively an input layer, multiple hidden layers, and an output layer, the input layer will be dissimilar according to various application. The hidden layers, which is the core block of a CNN architecture, consist of a series of **convolutional layers**, **pooling layers**, and finally export the output through the **fully-connected layer**. In the following sub-chapters, descriptions of the critical layer of CNN and their corresponding intuitive example will be provided in detail.
@Scherer2010EvaluationOP

### Convolutional Layer

The convolutional layer is the core building block of a CNN. In short, the input with a specific shape will be abstracted to a **feature map** after passing the convolutional layer. a set of learnable **filters (or kernels)** play an important role throughout this process. The following Figure \@ref(fig:figs-2) provide a more intuitive explanation of the convolutional layer.

```{r figs-2,message=FALSE, echo=FALSE, fig.cap="Basic operational structure of the convolutional layer", fig.pos="ht",fig.align="center",out.width = '65%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/Matrix.png")
```

The input of the Neural Networks might be assumed as a $6 \times 6$ matrix and each element of which can be presented as the integer '0','1'. As mentioned before, there is a set of learnable filters in the convolutional layer and each filter can be considered as a matrix, which is similar to a neuron in a fully-connected layer. In this instance, filters of size $3 \times 3$ slide back and forth with a specific stride across the entire input image and each element of the matrix or filter serves as a parameter (weight and bias) of Neural Networks. Traditionally, these parameter are not based on the initial setting but are trained through the training data.

An activated filter of size $3 \times 3$ has an ability to detect a pattern of the same size at some spatial position in the input. The algebraic operation explicates the transformation process from the input to the feature map.

$$ X  : = \begin{pmatrix}                            
X_{11} & X_{12} & \cdots & X_{16}\\ \\ 
X_{21} & X_{22} & \cdots &X_{26} \\ \\
\vdots & \vdots & \ddots & \vdots \\ \\
X_{61} & X_{62} & \cdots & X_{66}
\end{pmatrix}$$
where $X$ is the input matrix of size $6 \times 6$ as mentioned before;

$$ F  : = \begin{pmatrix}
w_{11} & w_{12}  & w_{13}\\ \\
w_{21} & w_{22}  & w_{23} \\ \\
w_{31} & w_{32}  & w_{33}
\end{pmatrix}$$
where $F$ denotes one filter of size $3 \times 3$;
$$ \beta  : = \begin{pmatrix}
w_{11} & w_{12} & \cdots & w_{16}\\
\end{pmatrix}$$
where $\beta$ is a unrolled matrix or filter;
$$ A_{11} = (F \times X)_{11}  : = 
\beta \cdot \begin{pmatrix}
w_{11} & w_{12} & w_{13} & w_{21} & w_{22} & w_{23} & w_{31} & w_{32} & w_{33}\\
\end{pmatrix}^{T}\\$$
Therefore, the first element of the feature map $A_{11}$ can been calculated through dot product operation shown as above. Sequentially, the second element of the feature map is determined by the sliding dot product of filter and the succeeding input matrix with the same size after setting a specific value of stride, which can be considered as a moving distance. After the whole process, a feature map of size $4 \times 4$ has been generated. Generally, there is more than one filter in the convolutional layer and each filter generate a feature map with the same size. The result of this convolutional layer is multiple feature maps (also referred to as activation map) and these feature maps correspond to different filters are stacked together along the depth dimension.

Another improved convolutional layer was proposed by (@Kalchbrenner2016NeuralMT) and this kind of convolution is named Dilated convolutionn, in order to solve the problem that the pooling operation in the pooling layer will lose a lot of information. The critical contribution of this convolution is that the receptive field of the network will not be reduced by removing the pooling operation. In other words, the units of feature maps in the deeper hidden layer can still map a larger region of the original input. As illustrated in Figure \@ref(fig:figs-dilated), although there is no pooling layer, the original input information is still increased as the layers are deeper.

```{r figs-dilated, message=FALSE, echo=FALSE, fig.cap="Visualization of dilated causal convolutional layers", fig.pos="ht",fig.align="center",out.width = '65%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/Temporal.png")
```

### ReLU layer
A non-linear layer (or activation layer) will be the subsequent process after each convolutional layer and the purpose of which is to introduce non-linearity to the neural networks because the operations during the convolutional layer are still linear (element-wise multiplications and summations). Generally, the major reason for introducing non-linearity is that there is a certain non-linear relationship between separate neurons. However, a convolutional layer is to perform basically a linear operation, and therefore, consecutive convolution layers are essentially equivalent to a single convolution layer, which is only used to reduce the representational power of the networks. As a result, the property of non-linearity between neurons has not been reflected and it is necessary to establish an activation function between the convolutional layer to avoid such an issue.

**Activation function**, which performs a non-linear transformation, plays a critical role in CNN to decide whether a neuron should be activated or ignored. Several activation functions are available after the convolutional layer, such as hyperbolic function and sigmoid function, etc., among of which ReLU is the most commonly used activation function in neural networks, especially in CNNs[@Krizhevsky2012ImageNetCW] because of its two properties: 

* Non-linearity: ReLU is the abbreviation of Rectified Linear Unit and defined mathematically as below:
$$ R(z)=z^{+}= max(0,z)$$
Where z denotes the output element of the previous convolutional layer. All negative values of feature map from the previous will be replaced by setting them to zero. 

* Non-Saturation: Saturation arithmetic is a kind of arithmetic in which all operations are limited to a fixed range between a minimum and maximum value.
   + $f$ is non-saturating iff $(|\displaystyle{\lim_{z \to -\infty}f(z)}|=+\infty) \cup (|\displaystyle{\lim_{z \to +\infty}f(z)}|=+\infty)$
   + $f$ is saturating iff $f$ is not non-saturating

As illustrated in Figure \@ref(fig:figs-3), compared with saturating activation sigmoid function that saturate at large values of input, ReLU activation function do not saturate[@Krizhevsky2012ImageNetCW] and the gradient of it is 0 on the negative x-axis and 1 on the positive side, which is a benefit of using this activation function because the updates to the weights of the neural networks at each iteration are consistent with the gradient of activation function. To be more specific, a neuron`s weights will stop updating if its gradient is close to zero. It is obviously problematic if such scenario appears too early in the training process.

```{r figs-3,message=FALSE, echo=FALSE, fig.cap="Comparison between saturating and non-saturating activation function ", fig.pos="ht",fig.align="center",out.width = '60%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/ReLU_sigmoid.png")
```

The following Figure \@ref(fig:figs-4) based on Figure \@ref(fig:figs-2) indicates a simplified version of the ReLU layer. Each single element of multiple feature maps, which is determined from the previous convolutional layer, will be further calculated by the ReLU activation function in this layer. Specifically, all positive values remain the same and negative values are replaced by setting them to zero. The output after the ReLU layer, which has the identical networks structure with the feature map from previous convolutional layer, will be used as an input for subsequent convolutional layer.

```{r figs-4,message=FALSE, echo=FALSE, fig.cap="Basic operational structure of the ReLU layer ", fig.pos="ht",fig.align="center",out.width = '50%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/ReLU.png")
```

### Pooling layer

Pooling layer is a concept that can be intuitively understood. The purpose of pooling layer is to reduce progressively the spatial size of feature map, which is generated from the previous convolutional layer, and identify important features. 
There are multiple pooling operations, such as average pooling, $l_{2}-norm$ pooling, and **max pooling**, Among which max pooling is the most commonly used function(@Scherer2010EvaluationOP), and the idea of max pooling is that the exact location of a feature is less important than its rough location relative to other features (@Yamaguchi1990ANN). Simultaneously, this process helps to control overfitting to a certain extent. The following Figure \@ref(fig:figs-5) illustrates an example that constructs a basic operational structure of the max pooling.

```{r figs-5,message=FALSE, echo=FALSE, fig.cap="Basic operational structure of the max pooling layer ", fig.pos="ht",fig.align="center",out.width = '80%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/Max_Pooling_finish.png")
```

The above mentioned example shows that two feature maps are generated according to two different filters. In this case, these feature maps of size $4\times4$ are separated into four non-overlapping sub-regions of size $2\times2$, and each single sub-region is names as depth slice. The Maximum value of each sub-region will be stored in the output of pooling layer. As a result, the input dimensions are further reduced from $4\times4$ to $2\times2$.

Some of the most critical reasons why adding max pooling layer to neural networks include the following:

* **Reducing computation complexity**: Since max pooling is reducing the dimension of the given output of a convolutional layer, the networks will be able to detect larger areas of the output. This process reduces the amount of parameters in the neural networks and consequently reduces computational load.
* **Controlling overfitting**: Overfitting appears when the model too complex or fits the training data too well. It may lose the true structure and then becomes difficult to generalize to new cases that are in the test data. With max pooling operation, not all features but the primary feature from each sub-region are extracted. Therefore, max pooling reduces the probability of overfitting to the great extent.

Except for this most commonly applied operation in NLP, several pooling operations for different intention include the following:

* **Average pooling** is usually used for topic models. If a sentence has different topics and the researchers assume that max pooling extracts insufficient information, average pooling can be considered as an alternative.

* **Dynamic pooling** proposed by (@Kalchbrenner2014ACN) has an ability to dynamically adjust the number of features according to the network structure. More specific, by combining the adjacent word information at the bottom and passing it gradually, new semantic information is recombined at the upper layer, so that words far away in the sentence also have interactive behavior (or some kind of semantic connection). Eventually, the most important semantic information in the sentence is extracted through the pooling layer.

### Fully-connected layer

As we mentioned in the previous section, one or more fully-connected layers are connected after multiple convolutional layers and pooling layers, each neuron in the fully connected layer is fully connected with all the neurons from the penultimate layer. The fully-connected layer, shown in Figure \@ref(fig:figs-6), can integrate local information with class distinction in the convolutional layer or pooling layer. In order to improve the CNN network performance, the excitation function of each neuron in the fully connected layer generally uses the ReLU function.

```{r figs-6,message=FALSE, echo=FALSE, fig.cap="Basic operational structure of the fully connected layer", fig.pos="ht",fig.align="center",out.width = '60%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/Fully_Connected.png")
```

## CNN for sentence classification

The explanation of CNN basic architecture provided in the first sub-
is based on a general example. Many researchers constructed their own specific CNN models based on this basic architecture in recent years and achieved outsanding results in the field of NLP. Therefore, this section explores five superior CNN architecture with some technical detail and their performanc comparison will be provided in later sub-chapter of this report.


### CNN-rand/CNN-static/CNN-non-static/CNN-multichannel

The first model to explore is published by (@Kim2014ConvolutionalNN), one of the highlights of this model is that the architecture is conceptually simple and efficient when dealing with the tasks of sentiment analysis and question classification. As illustrated in Figure \@ref(fig:figs-7), @Kim2014ConvolutionalNN utilizes a simple CNN architecture of (@Collobert2011NaturalLP) with single convolutional layer and the general architecture include the following sub-structure:

```{r figs-7,message=FALSE, echo=FALSE, fig.cap="Model architecture of CNN for sentence classification", fig.pos="ht",fig.align="center",out.width = '70%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/CNN_Sentence_Classification.png")
```

1. **Representation of sentence**: Assume that there are $n$ words in a sentence, and each word is denoted as $x_{i};\{i \in \mathbb{N} \mid 1 \leq i \leq n \}$ and $x_{i} \in \mathbb{R}^{k}$ to the $k$-dimensional word vector. Therefore, a sentence can be represented as:
$$ X_{1:n}=X_{1} \oplus X_{2} \oplus ... \oplus X_{n} $$
Where $\oplus$ is the concatenation operator.

2. **Convolutional layer**: Let a filter denote as $w \in \mathbb{R}^{hk}$, which is used to a window of $h$ words. A feature map $c=[c_1,c_2,…,c_{n-h+1}]$ can be generated by:
$$ c_i=f(w×x_{i:i+h-1}+b) $$
where $b \in \mathbb{R}$ is a bias term.

3. **Max-over-time pooling**: Pooling operation has been applied for respective filter to select the most important feature from each feature map $\hat{c} =max(\boldsymbol{c})$, notice that one feature $\hat{c}$ is generated by one filter, and these features will be passed to the last layer.

4. **Fully connected layer**: The selected features $\boldsymbol{Z}=[\hat{c}_1,\hat{c}_2,…,\hat{c}_j]$ from the previous layer have been flattened into a single vector, in order to aggregate each of them and therefore a specific class can be assigned to it based on the entire input. 

CNN is a feed forward model without cyclic connection. To be more specific, the direction of information flow in a forward model is in one direction (i.e from inputs to outputs). However, the models are trained or learned by the use of backward propagation (i.e from outputs to inputs), where the gradients are recalculated at each epoch to avoid co-adaptation.

In a forward propagation of this CNN, the output unit y based on the selected features $\boldsymbol{Z}$ is determined by using
$$y=w \cdot z+b$$
In a backward propagation, a dropout mechanism is applied as follow.
$$y=w \cdot (z \circ r)+b$$
Where $\circ$ is the element-wise multiplication operator and $r \in \mathbb{R}^{m}$  denotes a ‘masking’ vector of Bernoulli random variables with probability $p$ of being 1. As a result, gradients are backpropagated with probability $p$ and weights $\hat{w}$ are trained by using
$$\hat{w}=pw$$
On the basis of the model architecture described above, four derivative CNN models are introduced by (@Kim2014ConvolutionalNN) and the major difference among them are listed below: 

* **CNN-rand**: All words are randomly initialized and then modified during training.
* **CNN-static**: A model with pre-trained word vectors by using word2vec and kept them static.
* **CNN-non-static**: A model with pre-trained word vectors by using word2vec and these word vectors are fine-tuned for each tasks.
* **CNN-Multichannel**: A model with two channels generated by two sets of words vectors and each filter is employed to both channels.

### Character-level ConvNets

The second model is published by (@Zhang2015CharacterlevelCN), the two major differences of which compared with the previous model from (@Kim2014ConvolutionalNN) include the following:

1. The model architecture with 6 convolutional layers and 3 fully-connected layers (9 layers deep) is relatively more complex.
2. Different from the previous word-based Convolutional neural networks (ConvNets), this model is at character-level by implement character quantization.

```{r figs-8,message=FALSE, echo=FALSE, fig.cap="\\label{fig:fig_8} Model architecture of character-level CNN", fig.pos="ht",fig.align="center",out.width = '70%'}
knitr::include_graphics("figures/01-03-cnns-and-their-applications-in-nlp/Character_level_CNN.png")
```
The Figure \@ref(fig:figs-8) above shows the basic architecture of Character-level ConvNets, the corresponding explanation of the main components will be provided below:  

1. **Character quantization**: The input characters will be transformed into an encoding matrix of size $m \times l_0$ by using 1-of-$m$ encoding (or “one-hot” encoding). To be noticed that, The length of the character exceeds the determined value $l_0$ and will be ignored and the black characters as well as the characters that are not in the alphabet will be quantized as zero vectors.

2. **Temporal convolutional module**: A sequence of encoded characters followed by a temporal convolutional module, which is a variation over Convolutional Neural Networks works for sequence modelling tasks. To be more specific, when sentiment analysis is performed by using ConvNets, a fixed-size input will be a precondition, we can adjust the initial input length by truncating or padding the actual input to satisfy this criterion without affecting the sentiment and sequentially generate fixed-size outputs. Conceptually, this kind of 1-D convolution is called temporal convolution and the convolutional function defined as follow:
$$h(y)=\sum_{x=1}^{k}f(x)\cdot g(y \cdot d -x+c)$$
where $h_{j}(y)$ denotes outputs of the conmolutional layer; a discrete kernel functions $f_{i,j} \in [1,k] \to \mathbb{R}$ ($(i=1,...,m$ and $j=1,...,n)$) is also called weights; $d$ is denoted as stride; $c= k-d+1$ is used as an offset constant.

3. **Temporal max-pooling**: Based on the research of (@Boureau2010ATA), a 1-D version of the max-pooling $h(y)$ is employed in this ConvNets, which is defined as

$$h(y)=max (g(y \cdot d -x+c))$$
  With the help of this pooling function, it is possible to train ConvNets deeper than 6 layers.

4. **ReLU layer**: the activation function used in this modelis similar to ReLU $h(x)=max\{0,x\}$. More spesific, the algprithm is stochastic gradient descent (SGD). However, SGD is inflenced by strong curvature of the optimization function and moves slowly towards the minimum. Therefore, based on the research of @Sutskever2013OnTI, a momentum of 0.9 and initial step size 0.01 are established to reach the minimum more quickly.


### Very Deep CNN

### Deep Pyramid CNN

## Datasets and Experimental Evaluation

### Datasets

### Experimental Evaluation

## Conclusion and Discussion


# References
