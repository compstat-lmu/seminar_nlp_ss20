<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-09-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="transfer-learning-for-nlp-i.html"/>
<link rel="next" href="transfer-learning-for-nlp-ii.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-and-development"><i class="fa fa-check"></i><b>1.1</b> History and development</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#statistical-background"><i class="fa fa-check"></i><b>1.2</b> Statistical Background</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.3</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning-and-system-design-choices"><i class="fa fa-check"></i><b>3.3</b> Hyperparameter Tuning and System Design Choices</a></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.4</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#outlook-and-resources"><i class="fa fa-check"></i><b>3.5</b> Outlook and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extensions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extensions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.2</b> Encoder-Decoder Architecture</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>6</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>6.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>6.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>6.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>7</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="7.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#outline"><i class="fa fa-check"></i><b>7.1</b> Outline</a></li>
<li class="chapter" data-level="7.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>7.2</b> Sequential inductive transfer learning</a><ul>
<li class="chapter" data-level="7.2.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#types-of-transfer-learning"><i class="fa fa-check"></i><b>7.2.1</b> Types of transfer learning</a></li>
<li class="chapter" data-level="7.2.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#feature-extraction-vs.fine-tuning"><i class="fa fa-check"></i><b>7.2.2</b> Feature Extraction vs. Fine-tuning</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#models"><i class="fa fa-check"></i><b>7.3</b> Models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>7.3.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="7.3.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit"><i class="fa fa-check"></i><b>7.3.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="7.3.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>7.3.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>8</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="8.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>8.1</b> Attention</a><ul>
<li class="chapter" data-level="8.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>8.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>8.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="8.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#computational-difference-between-luong--and-bahdanau-attention"><i class="fa fa-check"></i><b>8.1.3</b> Computational Difference between Luong- and Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.4" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>8.1.4</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>8.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="8.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#the-transformer"><i class="fa fa-check"></i><b>8.2.1</b> The Transformer</a></li>
<li class="chapter" data-level="8.2.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers-as-rnns"><i class="fa fa-check"></i><b>8.2.2</b> Transformers as RNNs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>9</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="9.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>9.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>9.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="9.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>9.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="9.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>9.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="9.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>9.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="9.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-prediction"><i class="fa fa-check"></i><b>9.1.5</b> Next-sentence Prediction</a></li>
<li class="chapter" data-level="9.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>9.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>9.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>9.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>9.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="9.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>9.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="9.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>9.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="9.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>9.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="9.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>9.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="9.2.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>9.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>9.3</b> XLNet</a><ul>
<li class="chapter" data-level="9.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>9.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="9.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>9.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="9.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>9.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="9.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>9.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="9.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>9.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="9.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>9.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="9.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>9.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>9.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>10</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="11" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="11.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>11.1</b> Metrics</a></li>
<li class="chapter" data-level="11.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>11.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="11.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>11.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="11.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>11.2.2</b> CoQA</a></li>
<li class="chapter" data-level="11.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>11.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="11.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>11.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="11.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>11.2.5</b> SNLI</a></li>
<li class="chapter" data-level="11.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#overview"><i class="fa fa-check"></i><b>11.2.6</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-trained-models"><i class="fa fa-check"></i><b>11.3</b> Pre-Trained Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#bert"><i class="fa fa-check"></i><b>11.3.1</b> BERT</a></li>
<li class="chapter" data-level="11.3.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#openai-gpt-3"><i class="fa fa-check"></i><b>11.3.2</b> OpenAI GPT-3</a></li>
<li class="chapter" data-level="11.3.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#google-5t"><i class="fa fa-check"></i><b>11.3.3</b> Google 5T</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>11.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-cases-for-nlp.html"><a href="use-cases-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Use-Cases for NLP</a></li>
<li class="chapter" data-level="13" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>13</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="13.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>13.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="13.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>13.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="13.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>13.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="13.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>13.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="13.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>13.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="13.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>13.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="13.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>13.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>13.4</b> Dialog Systems</a><ul>
<li class="chapter" data-level="13.4.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>13.4.1</b> Types</a></li>
<li class="chapter" data-level="13.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>13.4.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#image-captioning-system"><i class="fa fa-check"></i><b>13.5</b> Image Captioning System</a><ul>
<li class="chapter" data-level="13.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#experiments"><i class="fa fa-check"></i><b>13.5.1</b> Experiments</a></li>
<li class="chapter" data-level="13.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#implementation"><i class="fa fa-check"></i><b>13.5.2</b> Implementation</a></li>
<li class="chapter" data-level="13.5.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#results"><i class="fa fa-check"></i><b>13.5.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>13.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>14</b> Epilogue</a><ul>
<li class="chapter" data-level="14.1" data-path="epilogue.html"><a href="epilogue.html#new-influentioal-architectures"><i class="fa fa-check"></i><b>14.1</b> New influentioal architectures</a></li>
<li class="chapter" data-level="14.2" data-path="epilogue.html"><a href="epilogue.html#improvements-of-the-self-attention-mechanism"><i class="fa fa-check"></i><b>14.2</b> Improvements of the Self-Attention mechanism</a></li>
<li class="chapter" data-level="14.3" data-path="epilogue.html"><a href="epilogue.html#evaluation-and-interpretability"><i class="fa fa-check"></i><b>14.3</b> Evaluation and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="attention-and-self-attention-for-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Attention and Self-Attention for NLP</h1>
<p><em>Authors: Joshua Wagner</em></p>
<p><em>Supervisor: Matthias Aßenmacher</em></p>
<p>Attention and Self-Attention models were some of the most influential developments in NLP.
The first part of this chapter is an overview of attention and different attention mechanisms.
The second part focuses on self-attention which enabled the commonly used models
for transfer learning that are used today. The final part of the chapter discusses the
developments made with Self-Attention and the most common transfer learning architecture today, the Transformer.</p>








<div id="attention" class="section level2">
<h2><span class="header-section-number">8.1</span> Attention</h2>
<p>In this part of the chapter, we revisit the Encoder-Decoder architecture that was introduced
in <a href="./recurrent-neural-networks-and-their-applications-in-nlp.html">chapter 3</a>. We focus on the improvements
that were made with the development of attention mechanisms on the example of neural machine translation (NMT).</p>
<p>As seen in chapter 3, vanilla
Encoder-Decoder architecture passes only the last hidden state from the encoder to the decoder.
This leads to the problem that information has to be compressed into a fixed length
vector and information can be lost in this compression.
Especially information found early in the sequence tends to be “forgotten” after
the entire sequence is processed. The addition of bi-directional layers remedies
this by processing the input in reversed order. While this helps for shorter sequences,
the problem still persists for long input sequences.
The development of attention enables the decoder to attend to the whole sequence and
thus use the context of the entire sequence during the decoding step.</p>
<div id="bahdanau-attention" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Bahdanau-Attention</h3>
<p>In 2014, <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-bahdanau2014neural">2014</a>)</span> proposed attention to fix the information problem that
the early encoder-decoder architecture faced. Decoders without attention are trained to predict <span class="math inline">\(y_{t}\)</span>
given a fixed length context vector <span class="math inline">\(c\)</span> and all earlier predicted words <span class="math inline">\(\{y_t, \dots, y_{t-1}\}\)</span>.
The fixed length context vector is computed with
<span class="math display">\[c=q(\{h_1,\dots,h_T\})\]</span> where <span class="math inline">\(h_1,\dots,h_T\)</span> are the the hidden states of the encoder for the input sequence
<span class="math inline">\(x_1,\dots, x_T\)</span> and <span class="math inline">\(q\)</span> is a non-linear function. <span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-sutskever2014sequence">2014</a>)</span> for example used
<span class="math inline">\(c = q(\{h_1,\dots,h_T\}) = h_T\)</span> as their non-linear transformation which remains a popular
choice for architectures without attention. It is also commonly used for the initialisation of the decoder hidden states.</p>
<p>Attention changes the context vector <span class="math inline">\(c\)</span> that a decoder uses for translation from a fixed
length vector <span class="math inline">\(c\)</span> of a sequence of hidden states <span class="math inline">\(h_1, \dots, h_T\)</span> to a sequence
of context vectors <span class="math inline">\(c_i\)</span>. The hidden state <span class="math inline">\(h_i\)</span> has a strong focus on the <em>i</em>-th
word in the input sequence and its surroundings.
If a bi-directional encoder is used, each <span class="math inline">\(h_i\)</span> is computed by a concatenation of the forward
<span class="math inline">\(\overrightarrow{h_i}\)</span> and backward <span class="math inline">\(\overleftarrow{h_i}\)</span> hidden states:</p>
<p><span class="math display">\[
h_i = [\overrightarrow{h_i}; \overleftarrow{h_i}], i = 1,\dots,n
\]</span>
These new variable context vectors <span class="math inline">\(c_i\)</span> are used for the computation of the decoder hidden state <span class="math inline">\(s_t\)</span>.
At time-point <span class="math inline">\(t\)</span> it is computed as <span class="math inline">\(s_t = f(s_{t-1},y_{t-1},c_t)\)</span> where <span class="math inline">\(f\)</span> is the function resulting from
the use of a LSTM- or GRU-cell.
The context vector <span class="math inline">\(c_t\)</span> is computed as a weighted sum of the hidden
states <span class="math inline">\(h_1,\dots, h_{T_x}\)</span>:</p>
<p><span class="math display">\[
c_t = \sum^{T_x}_{i=1}\alpha_{t,i}h_i.
\]</span>
Each hidden state <span class="math inline">\(h_i\)</span> is weighted by a <span class="math inline">\(\alpha_{t,i}\)</span>.
The weight <span class="math inline">\(\alpha_{t,i}\)</span> for each hidden state <span class="math inline">\(h_i\)</span> is also called the alignment score.
These alignment scores are computed as:</p>
<p><span class="math display">\[
\alpha_{t,i} = align(y_t, x_i) =\frac{exp(score(s_{t-1},h_i))}{\sum^{n}_{i&#39;=1}exp(score(s_{t-1},h_{i&#39;}))}
\]</span>
with <span class="math inline">\(s_{t-1}\)</span> being the hidden state of the decoder at time-step <span class="math inline">\(t-1\)</span>.
The alignment score <span class="math inline">\(\alpha_{t,i}\)</span> models how well input <span class="math inline">\(x_i\)</span> and output <span class="math inline">\(y_t\)</span> match
and assigns the weight to <span class="math inline">\(h_i\)</span>. <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-bahdanau2014neural">2014</a>)</span> parametrize their alignment
score with a single-hidden-layer feed-forward neural network which is jointly
trained with the other parts of the architecture. The score function used by Bahdanau et
al. is given as</p>
<p><span class="math display">\[
score(s_t,h_i) = v_\alpha^Ttanh(\mathbf{W}_\alpha[s_t;h_i])
\]</span>
were <em>tanh</em> is used as a non-linear activation function and <span class="math inline">\(v_\alpha\)</span> and <span class="math inline">\(W_\alpha\)</span>
are the weight matrices to be learned by the alignment model. The alignment score function
is called “concat” in <span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span> and “additive attention” in <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span>
because <span class="math inline">\(s_t\)</span> and <span class="math inline">\(h_i\)</span> are concatenated just like the forward and backward hidden states seen above.
The attention model proposed by Bahdanau et al. is also called a global attention model as it attends
to every input in the sequence. Another name for Bahdanaus attention model is soft attention
because the attention is spread thinly/weakly/softly over the input and does not have an inherent hard focus on specific inputs.
A nice by-product of attention mechanisms is the matrix of alignment scores
which can be visualised to show the correlation between source and target words as seen in <a href="attention-and-self-attention-for-nlp.html#fig:attention-plot-bahdanau">8.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:attention-plot-bahdanau"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/bahdanau-fig3.png" alt="Alignment Matrix visualised for a French to English translation. White squares indicate high aligment weights between input and output. Image source: Fig 3 in (Bahdanau, Cho, and Bengio 2014)" height="30%" />
<p class="caption">
FIGURE 8.1: Alignment Matrix visualised for a French to English translation. White squares indicate high aligment weights between input and output. Image source: Fig 3 in <span class="citation">(Bahdanau, Cho, and Bengio <a href="#ref-bahdanau2014neural">2014</a>)</span>
</p>
</div>
</div>
<div id="luong-attention" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Luong-Attention</h3>
<p>While <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-bahdanau2014neural">2014</a>)</span> were the first to use attention in neural machine translation,
<span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span> were the first to explore different attention mechanisms and their impact on
NMT. Luong et al. also generalise the attention mechanism for the decoder which enables
a quick switch between different attention functions.
Bahdanau et al. only consider</p>
<p><span class="math display">\[
score(s_t,h_i) = v_\alpha^Ttanh(\mathbf{W}_\alpha[s_t;h_i]),
\]</span>
while Luong et al. additionally introduce a general, a location-based and a dot-product score function for the global attention mechanism as described in table <a href="#tab:luong-score-functions">8.1</a>.</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Score-function</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \boldsymbol{s}_t^\top\mathbf{W}_a\boldsymbol{h}_i\)</span></td>
<td>General</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{t,i} = \text{softmax}(\mathbf{W}_a \boldsymbol{s}_t)\)</span></td>
<td>Location-based</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \boldsymbol{s}_t^\top\boldsymbol{h}_i\)</span></td>
<td>Dot-product</td>
</tr>
</tbody>
</table>
<p>Table <a href="#tab:luong-score-functions">8.1</a>: (#tab:luong-score-functions) Different score function proposed by Luong et al.</p>
<p>As <span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span> don’t use a bidirectional encoder, they simplify the hidden
state of the encoder from a concatenation of both forward and backward hidden states
to only the hidden state at the top layer of both encoder and decoder.</p>
<p>The attention mechanisms seen above attend to the entire input sequence. While
this fixes the problem of forgetful sequential models discussed in the beginning of the chapter,
it also has the drawback that it is expensive and can potentially be impractical
for long sequences e.g. the translation of entire paragraphs or documents. These problems encountered
with global or soft attention mechanisms can be mitigated with a local or hard attention
approach. While it was used by <span class="citation">Xu et al. (<a href="#ref-xu2015show">2015</a>)</span> for caption generation of images with a CNN
and by <span class="citation">Gregor et al. (<a href="#ref-gregor2015draw">2015</a>)</span> for the generation of images, the first application and differentiable version for NMT
is from <span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span>.</p>
<p>Different from the global attention mechanism, the local
attention mechanism at timestep <span class="math inline">\(t\)</span> first generates an aligned position <span class="math inline">\(p_t\)</span>.
The context vector is then computed as a weighted average over only the set of
hidden states in a window <span class="math inline">\([p_t-D,p_t+D]\)</span> with <span class="math inline">\(D\)</span> being an empirically selected
parameter. This constrains the above introduced computation for the context vector
to:</p>
<p><span class="math display">\[
c_t = \sum^{p_t+D}_{i=p_t-D}\alpha_{t,i}h_i.
\]</span></p>
<p>The parts outside of a sentence are ignored if the window crosses sentence
boundaries. The computation of the context vector changes compared to
the global model which can be seen in Figure <a href="attention-and-self-attention-for-nlp.html#fig:attention-plots-luong">8.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:attention-plots-luong"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/luong2015-fig2-3.png" alt="Global and local attention illustrated. Encoder in blue, Decoder in red. \(\overline{h}_s\) and $h_t$ in the image correspond to $h_t$ and $s_t$ in the previous text. Additional computations and differences to previous described architecture is found in the next sub-chapter. Image source: Fig. 2 and 3 in (Luong, Pham, and Manning 2015)" width="100%" />
<p class="caption">
FIGURE 8.2: Global and local attention illustrated. Encoder in blue, Decoder in red. <span class="math inline">\(\overline{h}_s\)</span> and <span class="math inline">\(h_t\)</span> in the image correspond to <span class="math inline">\(h_t\)</span> and <span class="math inline">\(s_t\)</span> in the previous text. Additional computations and differences to previous described architecture is found in the next sub-chapter. Image source: Fig. 2 and 3 in <span class="citation">(Luong, Pham, and Manning <a href="#ref-luong2015effective">2015</a>)</span>
</p>
</div>
<p>Luong et al. introduce two different concepts for the computation of the alignment position <span class="math inline">\(p_t\)</span>.</p>
<p>The first is the <em>monotonic</em> alignment(<strong>local-m</strong>).
This approach sets <span class="math inline">\(p_t= t\)</span> with the assumption that both input and output sequences
are roughly monotonically aligned.</p>
<p>The other approach, <em>predictive</em> alignment (<strong>local-p</strong>), predicts the aligned position with:
<span class="math display">\[
p_t = S \cdot sigmoid(v_p^\top tanh(W_ph_t))
\]</span>
where <span class="math inline">\(W_p\)</span> and <span class="math inline">\(v_p\)</span> are the parameters that are trained to predict the position.
<span class="math inline">\(S\)</span> is the length of the input sentence which leads, with the additional sigmoid function, to <span class="math inline">\(p_t \in [0,S]\)</span>.
A Gaussian distribution centred around <span class="math inline">\(p_t\)</span> is placed by <span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span> to favour alignment points closer to <span class="math inline">\(p_t\)</span>.
This changes the alignment weights to:
<span class="math display">\[
\alpha_{t,i} = align(y_t, x_i)exp(-\frac{(i-p_t)^2}{2\sigma^2})
\]</span>
where the standard deviation is empirically set to <span class="math inline">\(\sigma = \frac{D}{2}\)</span>. This
utilization of <span class="math inline">\(p_t\)</span> to compute <span class="math inline">\(\alpha_{t,i}\)</span> allows the computation of backpropagation
gradients for <span class="math inline">\(W_p\)</span> and <span class="math inline">\(v_p\)</span> and is thus ``differentiable almost everywhere’’ <span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span> while being less computationally expensive than global attention.</p>
</div>
<div id="computational-difference-between-luong--and-bahdanau-attention" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Computational Difference between Luong- and Bahdanau-Attention</h3>
<p>As previously mentioned, Luong et al. not only introduced different score functions in addition to Bahdanau et al.’s
concatenation/additive score function, they also generalized the computation for the context vector <span class="math inline">\(c_t\)</span>.
In Bahdanau’s version the attention mechanism computes the variable length context vector first which is then used as input for the decoder.
This necessitates the use of the last decoder hidden state <span class="math inline">\(s_{t-1}\)</span> as input for the computation of the context vector <span class="math inline">\(c_t\)</span>:</p>
<p><span class="math display">\[
\alpha_{t,i} = align(y_t, x_i) =\frac{exp(score(s_{t-1},h_i))}{\sum^{n}_{i&#39;=1}exp(score(s_{t-1},h_{i&#39;}))}
\]</span></p>
<p>Luong et al. compute their context vector with the current decoder hidden state <span class="math inline">\(s_t\)</span> and modify the decoder output with the
context vector before it is processed by the last softmax layer. This allows for easier implementation of different
score functions for the same attention mechanism. Implementations of both vary e.g. <a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb">this version</a> of Bahdanau attention in Pytorch concatenates the context back in after the GRU while <a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">this version</a> for an NMT model with Bahdanau attention does not. Readers that are trying to avoid a headache can build upon <a href="https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt#defining_nmt_model">this version</a> from Tensorflow which uses the <strong>AttentionWrapper</strong> function which handles the specifics of the implementation.</p>
<p>An illustration of a reduced version of the two different attention
concepts can be found in Figure <a href="attention-and-self-attention-for-nlp.html#fig:attention-mechanisms">8.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:attention-mechanisms"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/bahdanau-luong-illustrated.png" alt="Bahdanau and Luong attention mechanisms illustrated. The attention layer is bounded in the red box. Orange denotes inputs, outputs and weights. Blue boxes are layers. Green boxes denote operations e.g. softmax or concatenation([x;y])." width="100%" />
<p class="caption">
FIGURE 8.3: Bahdanau and Luong attention mechanisms illustrated. The attention layer is bounded in the red box. Orange denotes inputs, outputs and weights. Blue boxes are layers. Green boxes denote operations e.g. softmax or concatenation([x;y]).
</p>
</div>
</div>
<div id="attention-models" class="section level3">
<h3><span class="header-section-number">8.1.4</span> Attention Models</h3>
<p>Attention was in the beginning not directly developed for RNNs or even NLP.
Attention was first introduced in <span class="citation">Graves, Wayne, and Danihelka (<a href="#ref-GravesWD14">2014</a>)</span> with a content-based attention mechanism
(<span class="math inline">\(\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \text{cosine}[\boldsymbol{s}_t, \boldsymbol{h}_i]\)</span>)
for Neural Turing Machines. Their application for NLP related tasks were later developed by
<span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span>, <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-bahdanau2014neural">2014</a>)</span> and <span class="citation">Xu et al. (<a href="#ref-xu2015show">2015</a>)</span>.
<span class="citation">Xu et al. (<a href="#ref-xu2015show">2015</a>)</span> were the first publication to differentiate between soft/global and hard/local attention mechanisms and did this in the
context of Neural Image Caption with both mechanisms being close to what was used
by Luong et al. in the previous section. <span class="citation">Cheng, Dong, and Lapata (<a href="#ref-cheng2016long">2016</a>)</span> were the first to introduce the concept of self-attention, the third big
category of attention mechanisms.</p>
</div>
</div>
<div id="self-attention" class="section level2">
<h2><span class="header-section-number">8.2</span> Self-Attention</h2>
<p><span class="citation">Cheng, Dong, and Lapata (<a href="#ref-cheng2016long">2016</a>)</span> implement self-attention with a modified LSTM unit, the Long Short-Term
Memory-Network (LSTMN). The LSTMN replaces the memory cell with a memory network to enable
the storage of ``contextual representation of each input token with
a unique memory slot and the size of the memory
grows with time until an upper bound of the memory
span is reached’’ <span class="citation">Cheng, Dong, and Lapata (<a href="#ref-cheng2016long">2016</a>)</span>.
Self-Attention, as the name implies, allows an encoder to attend to other parts of the input during processing as seen in Figure <a href="attention-and-self-attention-for-nlp.html#fig:self-attention-cheng">8.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:self-attention-cheng"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/cheng2016-fig1.png" alt="Illustration of the self-attention mechanism. Red indicates the currently fixated word, Blue represents the memories of previous words. Shading indicates the degree of memory activation. Image source: Fig. 1 in (Cheng, Dong, and Lapata 2016)." width="100%" />
<p class="caption">
FIGURE 8.4: Illustration of the self-attention mechanism. Red indicates the currently fixated word, Blue represents the memories of previous words. Shading indicates the degree of memory activation. Image source: Fig. 1 in <span class="citation">(Cheng, Dong, and Lapata <a href="#ref-cheng2016long">2016</a>)</span>.
</p>
</div>
<p>While the LSTMN introduced self-attention, it retains the drawbacks that come from
the use of a RNN which are discussed at the end of the Transformer section. <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span> propose the Transformer architecture which uses
self-attention extensively to circumvent these drawbacks.</p>
<div id="the-transformer" class="section level3">
<h3><span class="header-section-number">8.2.1</span> The Transformer</h3>
<p>RNNs were, prior to Transformers, the state-of-the-art model for machine translation, language modelling
and other NLP tasks. But the sequential nature of a RNN precludes parallelization within
training examples. This becomes critical at longer sequence lengths as memory constraints
limit batching across examples. While much has been done to minimize these problems,
they are inherent in the architecture and thus still remain. An attention mechanism
allows the modelling of dependencies without regard for the distance in either input
or output sequences. Most attention mechanisms,
as seen in the previous sections of this chapter, use recurrent neural networks.
This limits their usefulness for transfer learning because of the previously mentioned
constraints that recurrent networks have. Models like ByteNet from <span class="citation">Kalchbrenner, Espeholt, Simonyan, Oord, et al. (<a href="#ref-kalchbrenner2016neural">2016</a><a href="#ref-kalchbrenner2016neural">a</a>)</span>
and ConvS2S from <span class="citation">Gehring et al. (<a href="#ref-gehring2017convolutional">2017</a>)</span> alleviate the problem with sequential models
by using convolutional neural networks as basic building blocks. ConvS2S has a
linear increase in number of operations to relate signals from two arbitrary
input or output positions with growing distance. ByteNet has a logarithmical increase
in number of operations needed. The Transformer architecture from <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span>
achieves the relation of two signals with arbitrary positions in input or output
with a constant number of operations. It was also the first model that relied entirely
on self-attention for the computation of representations of input or output without
using sequence-aligned recurrent networks or convolutions.</p>
<p>While the Transformer architecture doesn’t use recurrent or convolutional networks,
it retains the popular encoder-decoder architecture.</p>
<div class="figure" style="text-align: center"><span id="fig:encoder-transformer"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/transformer-encoder.png" alt="Single layer of the Encoder of a Transformer with two distinct sub-layers each with residual connections and a LayerNorm. Original image: (Vaswani et al. 2017),\ Additions and cropping: (Weng 2018)" width="60%" />
<p class="caption">
FIGURE 8.5: Single layer of the Encoder of a Transformer with two distinct sub-layers each with residual connections and a LayerNorm. Original image: <span class="citation">(Vaswani et al. <a href="#ref-vaswani2017attention">2017</a>)</span>, Additions and cropping: <span class="citation">(Weng <a href="#ref-weng2018attention">2018</a>)</span>
</p>
</div>
<p>The encoder is composed of a stack of N = 6 identical layers. Each of these layers
has two sub-layers: A multi-head self-attention mechanism and a position-wise fully
connected feed-forward network. The sub-layers have a residual connection around
the main components which is followed by a layer normalization. The output of each sub-layer
is <span class="math inline">\(\text{LayerNorm}(x + \text{Sublayer}(x))\)</span> where <span class="math inline">\(\text{Sublayer}(x)\)</span> is the output of the
function of the sublayer itself. All sub-layers and the embedding layer before the
encoder/decoder produce outputs of <span class="math inline">\(dim = d_{model} = 512\)</span> to allow these residual connections to work.
The position-wise feed-forward network used in the sublayer is applied to each position
separately and identically. This network consists of two linear transformations with
a ReLU activation function in between:</p>
<p><span class="math display">\[
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
\]</span></p>
The decoder is, as seen in Figure <a href="attention-and-self-attention-for-nlp.html#fig:decoder-transformer">8.6</a>, composed of a stack of <span class="math inline">\(N = 6\)</span> identical layers.
<div class="figure" style="text-align: center"><span id="fig:decoder-transformer"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/transformer-decoder.png" alt="Decoder of a Transformer, Original image: (Vaswani et al. 2017),\ Additions and cropping: (Weng 2018)" width="50%" />
<p class="caption">
FIGURE 8.6: Decoder of a Transformer, Original image: <span class="citation">(Vaswani et al. <a href="#ref-vaswani2017attention">2017</a>)</span>, Additions and cropping: <span class="citation">(Weng <a href="#ref-weng2018attention">2018</a>)</span>
</p>
</div>
<p>It inserts, in addition to the two already known sub-layers from the encoder,
a third sub-layer which also performs multi-head attention.
This third sub-layer uses the encoder output as two of its three input values, which will be described in the next part of the chapter,
for the multi-head attention. This sub-layer is in its function very close to the before seen attention mechanisms between encoders and decoders.
It uses, same as the encoder, residual connections around each of the sub-layers.
The decoder also uses a modified, masked self-attention sub-layer ``to prevent positions
from attending to subsequent positions’’ <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span>. This, coupled with the
fact that the output embeddings are shifted by one position to the right ensures that
the predictions for position <span class="math inline">\(i\)</span> only depend on previous known outputs.</p>
<div class="figure" style="text-align: center"><span id="fig:transformer-full"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/transformer-full-model.png" alt="The Transformer-model architecture, Image Source: Fig. 1 in (Vaswani et al. 2017)" height="50%" />
<p class="caption">
FIGURE 8.7: The Transformer-model architecture, Image Source: Fig. 1 in <span class="citation">(Vaswani et al. <a href="#ref-vaswani2017attention">2017</a>)</span>
</p>
</div>
<p>As seen in <a href="attention-and-self-attention-for-nlp.html#fig:transformer-full">8.7</a>, the Transformer uses positional encodings
added to the embeddings so the model can make use of the order of the sequence.
<span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span> use the sine and cosine function of different frequencies:</p>
<p><span class="math display">\[
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})
\]</span>
<span class="math display">\[
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})
\]</span>
For further reasoning why these functions were chosen see <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span>.</p>
<div id="the-self-attention-mechanisms" class="section level4">
<h4><span class="header-section-number">8.2.1.1</span> The self-attention mechanism(s)</h4>
<p><span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span> describe attention functions as “mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key”.
The <em>Q</em>uery and the <em>K</em>ey-<em>V</em>alue pairs are used in the
newly proposed attention mechanism that is used in Transformers.
These inputs for the attention mechanisms are obtained through multiplication of the
general input to the encoder/decoder with different weight matrices as can be seen in figure <a href="attention-and-self-attention-for-nlp.html#fig:self-attention-inputs">8.8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:self-attention-inputs"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/self-attention-matrix-calculation.png" alt="Input transformation used in a Transformer, \(Q = \text{Query}\), \(K = \text{Key}\), \(V = \text{Value}\). Image Source: (Alammar 2018)" width="50%" />
<p class="caption">
FIGURE 8.8: Input transformation used in a Transformer, <span class="math inline">\(Q = \text{Query}\)</span>, <span class="math inline">\(K = \text{Key}\)</span>, <span class="math inline">\(V = \text{Value}\)</span>. Image Source: <span class="citation">(Alammar <a href="#ref-alammar2018transformer">2018</a>)</span>
</p>
</div>
<p>These different transformations of the input are what enables the input to attend to itself.
This in turn allows the model to learn about context.
E.g. <strong>bank</strong> can mean very different things depending on the context which can be recognized by the self-attention mechanisms.</p>
<p>As seen in Figures <a href="attention-and-self-attention-for-nlp.html#fig:encoder-transformer">8.5</a>, <a href="attention-and-self-attention-for-nlp.html#fig:decoder-transformer">8.6</a>
and <a href="attention-and-self-attention-for-nlp.html#fig:transformer-full">8.7</a>, the Transformer uses an attention mechanism called
``Multi-Head Attention’’.</p>
<div class="figure" style="text-align: center"><span id="fig:multi-head-attention"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/multi-head-attention.png" alt="Multi-Head Attention, Image Source: Fig. 2 in (Vaswani et al. 2017)" width="50%" />
<p class="caption">
FIGURE 8.9: Multi-Head Attention, Image Source: Fig. 2 in <span class="citation">(Vaswani et al. <a href="#ref-vaswani2017attention">2017</a>)</span>
</p>
</div>
<p>The multi-head attention projects the queries, keys and values <span class="math inline">\(h\)</span> times instead of performing
a single attention on <span class="math inline">\(d_{model}\)</span>-dim. queries and key-value pairs. The projections
are learned, linear and project to <span class="math inline">\(d_k\)</span>, <span class="math inline">\(d_k\)</span> and <span class="math inline">\(d_v\)</span> dimensions. Next the
new <strong>scaled dot-product attention</strong> is used on each of these to yield a <span class="math inline">\(d_v\)</span>-dim. output.
These values are then concatenated and projected to yield the final values as can be
seen in <a href="attention-and-self-attention-for-nlp.html#fig:multi-head-attention">8.9</a>. This multi-dimensionality allows the attention
mechanism to jointly attend to different information from different representation
at different positions. The multi-head attention can be written as:</p>
<p><span class="math display">\[
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\dots, \text{head}_h)W^O
\]</span></p>
<p><span class="math display">\[
\text{where } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\]</span>
and projections are the parameter matrices <span class="math inline">\(W_i^Q\in \mathbb{R}^{d_{model}\times d_k}, W_i^K\in \mathbb{R}^{d_{model}\times d_k}, W_i^V\in \mathbb{R}^{d_{model}\times d_v}\text{ and } W^O\in \mathbb{R}^{hd_{v}\times d_{model}}\)</span>.
<span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span> use <span class="math inline">\(h = 8\)</span> and <span class="math inline">\(d_k = d_v = d_{model}/h = 64\)</span>. This reduced dimensionality
leads to a reduction in computational cost that is similar to that of a single scaled-dot-product attention head
with the full initial dimensionality of <span class="math inline">\(512\)</span>.</p>
The <em>scaled dot-product attention</em> is, as the name suggests, just a scaled version
of the dot-product attention seen previously in this chapter.
<div class="figure" style="text-align: center"><span id="fig:scaled-dot-prod-attention"></span>
<img src="figures/02-02-attention-and-self-attention-for-nlp/scaled-dot-prod-attention.png" alt="Scaled Dot-Product Attention, Image Source: Fig. 2 in (Vaswani et al. 2017)" width="50%" />
<p class="caption">
FIGURE 8.10: Scaled Dot-Product Attention, Image Source: Fig. 2 in <span class="citation">(Vaswani et al. <a href="#ref-vaswani2017attention">2017</a>)</span>
</p>
</div>
<p>The optional <em>Mask</em>-function seen in Fig. <a href="attention-and-self-attention-for-nlp.html#fig:scaled-dot-prod-attention">8.10</a> is
only used in the masked-multi-head attention of the decoder. The querys and
keys are of dim. <span class="math inline">\(d_k\)</span> and the values are of dim. <span class="math inline">\(d_v\)</span>. The attention
is for practical reasons computed for a set of queries, <em>Q</em>. The keys and values
are thus also used in matrix format, <em>K</em> and <em>V</em>. The matrix of outputs is then
computed as:</p>
<p><span class="math display">\[
\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^\top}{\sqrt{d_k}})V
\]</span>
where <span class="math inline">\(\text{Attention}(Q,K,V)\)</span> corresponds to an non-projected head of multi-head attention.
These attention mechanisms allow Transformers to learn different and distant dependencies in language
and are thus a good candidate for transfer learning.</p>
</div>
<div id="complexity-for-different-attention-models" class="section level4">
<h4><span class="header-section-number">8.2.1.2</span> Complexity for different attention models</h4>
<p>The architecture of a Transformer allows for parallel encoding of every part of the
input at the same time. This also enables the modelling of long-range dependencies regardless
of the distance.
The Transformer architecture, as a minimum, only needs a constant number of sequential operations regardless of
input length <span class="math inline">\(n\)</span> due to extensive parallelization as can be seen in Table <a href="#tab:complexity-operations">8.2</a>.
A RNN based model in comparison needs a linearly scaling number of sequential operations due to its architecture.
The Maximum Path Length between long-range dependencies for a transformer is <span class="math inline">\(O(1)\)</span> while the RNN
again has <span class="math inline">\(O(n)\)</span> due to its sequential input reading.</p>
<p>The fast modelling of long-range dependencies and the multiple attention heads which learn different
dependencies makes Transformers a favourable choice for Transfer Learning.
The transfer learning models that were developed from the Transformer architecture
enabled models which were trained on more data to gain a deeper understanding about
language and are state-of-the-art today (June, 2020).</p>
<p>They are also a popular research topic as they are quadratically scaling, with regard to input length, complexity per layer
inhibits their use for very long sequences and makes them time consuming to train.
This quadratic complexity comes from the self-attention mechanism <span class="math inline">\(\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^\top}{\sqrt{d_k}})V\)</span>.
The used softmax needs to calculate the attention score between the currently processed input <span class="math inline">\(x_i\)</span> and every
other input <span class="math inline">\(x_j\)</span> for each <span class="math inline">\(j \in {1, \dots ,n}\)</span> and <span class="math inline">\(i \in {1, \dots ,n}\)</span>. This limits the length of the
context that Transformers can process and increases the time they need for training in practical uses.
The computational costs are especially severe in very long tasks.</p>
<p>In the last few years new variations of the vanilla transformer, as described previously in the chapter, were published
which lower the computational cost (e.g. <span class="citation">Shen et al. (<a href="#ref-shen2018efficient">2018</a>)</span> and <span class="citation">Choromanski et al. (<a href="#ref-choromanski2020masked">2020</a>)</span>).
Some of these variations of the Transformer architecture managed to decrease the complexity
from quadratic to <span class="math inline">\(O(N\sqrt{N})\)</span> (<span class="citation">Child et al. (<a href="#ref-child2019generating">2019</a>)</span>), <span class="math inline">\(O(N\log N)\)</span> (<span class="citation">Kitaev, Kaiser, and Levskaya (<a href="#ref-kitaev2020reformer">2020</a>)</span>) or <span class="math inline">\(O(n)\)</span> (<span class="citation">Wang et al. (<a href="#ref-wang2020linformer">2020</a>)</span>) as seen in Table <a href="#tab:complexity-operations">8.2</a>.</p>
<table>
<thead>
<tr class="header">
<th>Layer-type</th>
<th>Complexity per Layer</th>
<th>Sequential Operations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Recurrent</td>
<td><span class="math inline">\(O(n \cdot d^2)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr class="even">
<td>Convolutional</td>
<td><span class="math inline">\(O(k \cdot n \cdot d^2)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="odd">
<td>Transfomer</td>
<td><span class="math inline">\(O(n^2 \cdot d)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="even">
<td>Sparse Transfomer</td>
<td><span class="math inline">\(O(n\sqrt{n})\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="odd">
<td>Reformer</td>
<td><span class="math inline">\(O(n \log (n))\)</span></td>
<td><span class="math inline">\(O(\log (n))\)</span></td>
</tr>
<tr class="even">
<td>Linformer</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="odd">
<td>Linear Transformer</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
</tbody>
</table>
<p>Table <a href="#tab:complexity-operations">8.2</a> : (#tab:complexity-operations) Complexity per layer and the number of sequential operations. Sparse Transformers are from <span class="citation">Child et al. (<a href="#ref-child2019generating">2019</a>)</span>, Reformer from <span class="citation">Kitaev, Kaiser, and Levskaya (<a href="#ref-kitaev2020reformer">2020</a>)</span>, Linformer from <span class="citation">Wang et al. (<a href="#ref-wang2020linformer">2020</a>)</span> and Linear Transformers from <span class="citation">Katharopoulos et al. (<a href="#ref-katharopoulos2020transformers">2020</a>)</span>. <span class="math inline">\(n\)</span> is the sequence length, <span class="math inline">\(k\)</span> the kernel size, <span class="math inline">\(d\)</span> the representation dimension and <span class="math inline">\(r\)</span> the size of the neighbourhood in restricted self-attention. Source: <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span> (Table 1) and <span class="citation">Wang et al. (<a href="#ref-wang2020linformer">2020</a>)</span> (Table 1)</p>
</div>
</div>
<div id="transformers-as-rnns" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Transformers as RNNs</h3>
<p>A version from <span class="citation">Katharopoulos et al. (<a href="#ref-katharopoulos2020transformers">2020</a>)</span> uses a linear attention mechanism for
autoregressive tasks. The vanilla Transformer uses, as described above, the softmax to
calculate the attention between values. This can also be seen as a similarity function.
<span class="math inline">\(V_i&#39; = \frac{\sum_{j=1}^N \text{sim}(Q_i, K_j)V_j}{\sum_{j=1}^N\text{sim}(Q_i, K_j)}\)</span> is equal to
the row-wise calculation of <span class="math inline">\(\text{Attention}(Q,K,V) = V&#39; = \text{softmax}(\frac{QK^\top}{\sqrt{d_k}})V\)</span>
if <span class="math inline">\(\text{sim}(q,k)=\exp(\frac{q^\top k}{\sqrt{d_k}})\)</span>. While we can use the exponentiated
dot-product to generate the vanilla scaled-dot-product attention, we can use any non-negative
function, including kernels, as similarity function to generate a new attention mechanism <span class="citation">Katharopoulos et al. (<a href="#ref-katharopoulos2020transformers">2020</a>)</span>.
Given the feature representation <span class="math inline">\(\phi(x)\)</span> of such a kernel, the previous row-wise calculation
can be rewritten as <span class="math inline">\(V_i&#39; = \frac{\phi(Q_i)^\top \sum_{j=1}^N\phi(K_j)V_j^\top}{\phi(Q_i)^\top \sum_{j=1}^N\phi(K_j)}\)</span>.
Or in vectorized form: <span class="math inline">\(\phi(Q)(\phi(K)^\top V)\)</span> where the feature map <span class="math inline">\(\phi(\cdot)\)</span> is applied row-wise to the matrices.
This new formulation shows that the computation with a feature map allows for linear time and memory scaling <span class="math inline">\(O(n)\)</span>
because <span class="math inline">\(\sum_{j=1}^N\phi(K_j)V_j^\top\)</span> and <span class="math inline">\(\sum_{j=1}^N\phi(K_j)\)</span> can be both computed once and reused for every
row of the query (<span class="citation">Katharopoulos et al. (<a href="#ref-katharopoulos2020transformers">2020</a>)</span>).
No finite dimensional feature map of the exponential function exists, which makes a linearisation of the
softmax attention impossible. This forces the use of a polynomial kernel which has been shown to work equally well with
the exponential kernel <span class="citation">Tsai et al. (<a href="#ref-tsai2019transformer">2019</a>)</span>. This results in a computational cost of
<span class="math inline">\(O(ND^2M)\)</span> which is favourable if <span class="math inline">\(N &gt; D^2\)</span>. <span class="citation">Katharopoulos et al. (<a href="#ref-katharopoulos2020transformers">2020</a>)</span> suggest <span class="math inline">\(\phi(x) = \text{elu}(x) + 1\)</span> for <span class="math inline">\(N &lt; D^2\)</span>.</p>
<p>For autoregressive tasks we want an attention mechanism that can’t look ahead of its position.
<span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span> introduced masked-self-attention for their decoder, which is adapted for linearised attention.
The addition of masking changes the previous formulation to <span class="math inline">\(V_i&#39; = \frac{\sum_{j=1}^i \text{sim}(Q_i, K_j)V_j}{\sum_{j=1}^i\text{sim}(Q_i, K_j)}\)</span>.
With the linearisation through kernels we get <span class="math inline">\(V_i&#39; = \frac{\phi(Q_i)^\top \sum_{j=1}^i\phi(K_j)V_j^\top}{\phi(Q_i)^\top \sum_{j=1}^i\phi(K_j)}\)</span>.</p>
<p><span class="math inline">\(S_i = \sum_{j=1}^i\phi(K_j)V_j^\top\)</span> and <span class="math inline">\(Z_i = \sum_{j=1}^i\phi(K_j)\)</span> can be used to simplify the formula to:</p>
<p><span class="math display">\[
V_i&#39; = \frac{\phi(Q_i)^\top Si}{\phi(Q_i)^\top Z_i}
\]</span></p>
<p>where <span class="math inline">\(S_i\)</span> and <span class="math inline">\(Z_i\)</span> can be computed from <span class="math inline">\(S_{i-1}\)</span> and <span class="math inline">\(Z_{i-1}\)</span> which allows the linearised attention
with masking to scale linearly with respect to the sequence length. The derivation of
the numerator as cumulative sums allows for the computation in linear time and constant memory,
which leads to computational complexity of <span class="math inline">\(O(NCM)\)</span> and memory <span class="math inline">\(O(N \max{(C,d_k)})\)</span> where <span class="math inline">\(C\)</span> is the dimensionality of
the feature map <span class="citation">Katharopoulos et al. (<a href="#ref-katharopoulos2020transformers">2020</a>)</span>.</p>
<p>Given the previous formalization of feature maps to replace the softmax we can rewrite the
layers of a Transformers as a RNN. This RNN has two hidden states, the attention state <span class="math inline">\(s\)</span> and the
normalizer state <span class="math inline">\(z\)</span> with subscripts denoting the timestep in recurrence.
With <span class="math inline">\(s_0 = 0\)</span> and <span class="math inline">\(z_0 = 0\)</span> we can define <span class="math inline">\(s_i\)</span> as <span class="math inline">\(s_i s_{i-1} + \phi(x_iW_K)(x_iW_V)^\top\)</span> and
<span class="math inline">\(z_i\)</span> as <span class="math inline">\(z_i = z_{i-1} + \phi(x_iW_K)\)</span> with <span class="math inline">\(x_i\)</span> as the <span class="math inline">\(i\)</span>-th input for the layer.
The <span class="math inline">\(i\)</span>-th output <span class="math inline">\(y_i\)</span> can then be written as <span class="math inline">\(y_i = f_l(\frac{\phi(x_iW_Q)^\top s_i}{\phi(x_iW_Q)^\top z_i} + x_i)\)</span>
where <span class="math inline">\(f_l\)</span> is the function given by the feed-forward network of a Transformer layer.</p>
<p>This shows that the Transformer layers can be rewritten into RNN layers, for all similarity functions
that can be represented with <span class="math inline">\(\phi\)</span> (<span class="citation">Katharopoulos et al. (<a href="#ref-katharopoulos2020transformers">2020</a>)</span>), which are the first models
that used attention for NLP tasks.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-alammar2018transformer">
<p>Alammar, Jay. 2018. “The Illustrated Transformer [Blog Post].” <em>Http://Jalammar.github.io/</em>. <a href="http://jalammar.github.io/illustrated-transformer/" class="uri">http://jalammar.github.io/illustrated-transformer/</a>.</p>
</div>
<div id="ref-bahdanau2014neural">
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” <em>arXiv Preprint arXiv:1409.0473</em>.</p>
</div>
<div id="ref-cheng2016long">
<p>Cheng, Jianpeng, Li Dong, and Mirella Lapata. 2016. “Long Short-Term Memory-Networks for Machine Reading.” <em>arXiv Preprint arXiv:1601.06733</em>.</p>
</div>
<div id="ref-child2019generating">
<p>Child, Rewon, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. “Generating Long Sequences with Sparse Transformers.”</p>
</div>
<div id="ref-choromanski2020masked">
<p>Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Jared Davis, Tamas Sarlos, David Belanger, Lucy Colwell, and Adrian Weller. 2020. “Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers.”</p>
</div>
<div id="ref-gehring2017convolutional">
<p>Gehring, Jonas, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. “Convolutional Sequence to Sequence Learning.”</p>
</div>
<div id="ref-GravesWD14">
<p>Graves, Alex, Greg Wayne, and Ivo Danihelka. 2014. “Neural Turing Machines.” <em>CoRR</em> abs/1410.5401. <a href="http://arxiv.org/abs/1410.5401" class="uri">http://arxiv.org/abs/1410.5401</a>.</p>
</div>
<div id="ref-gregor2015draw">
<p>Gregor, Karol, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. 2015. “Draw: A Recurrent Neural Network for Image Generation.” <em>arXiv Preprint arXiv:1502.04623</em>.</p>
</div>
<div id="ref-kalchbrenner2016neural">
<p>Kalchbrenner, Nal, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016a. “Neural Machine Translation in Linear Time.”</p>
</div>
<div id="ref-katharopoulos2020transformers">
<p>Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. “Transformers Are Rnns: Fast Autoregressive Transformers with Linear Attention.”</p>
</div>
<div id="ref-kitaev2020reformer">
<p>Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. 2020. “Reformer: The Efficient Transformer.”</p>
</div>
<div id="ref-luong2015effective">
<p>Luong, Minh-Thang, Hieu Pham, and Christopher D Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” <em>arXiv Preprint arXiv:1508.04025</em>.</p>
</div>
<div id="ref-shen2018efficient">
<p>Shen, Zhuoran, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. 2018. “Efficient Attention: Attention with Linear Complexities.”</p>
</div>
<div id="ref-sutskever2014sequence">
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3104–12.</p>
</div>
<div id="ref-tsai2019transformer">
<p>Tsai, Yao-Hung Hubert, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. “Transformer Dissection: A Unified Understanding of Transformer’s Attention via the Lens of Kernel.”</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, 5998–6008.</p>
</div>
<div id="ref-wang2020linformer">
<p>Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. “Linformer: Self-Attention with Linear Complexity.”</p>
</div>
<div id="ref-weng2018attention">
<p>Weng, Lilian. 2018. “Attention? Attention!” <em>Lilianweng.github.io/Lil-Log</em>. <a href="http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" class="uri">http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</a>.</p>
</div>
<div id="ref-xu2015show">
<p>Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” In <em>International Conference on Machine Learning</em>, 2048–57.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="transfer-learning-for-nlp-i.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="transfer-learning-for-nlp-ii.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/02-02-attention-and-self-attention-for-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
