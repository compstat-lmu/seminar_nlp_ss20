<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-06-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="transfer-learning-for-nlp-i.html"/>
<link rel="next" href="transfer-learning-for-nlp-ii.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#model-improvements"><i class="fa fa-check"></i><b>3.3</b> Model Improvements</a><ul>
<li class="chapter" data-level="3.3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#fasttext"><i class="fa fa-check"></i><b>3.3.1</b> fastText</a></li>
<li class="chapter" data-level="3.3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word-phrases"><i class="fa fa-check"></i><b>3.3.2</b> Word Phrases</a></li>
<li class="chapter" data-level="3.3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#multiple-meanings-per-word"><i class="fa fa-check"></i><b>3.3.3</b> Multiple Meanings per Word</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>3.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.5</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.6" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#sources-and-applications-of-word-embeddings"><i class="fa fa-check"></i><b>3.6</b> Sources and Applications of Word Embeddings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extentions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extentions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#bidirectional-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Bidirectional RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.2</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.3</b> Encoder-Decoder Architecture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
<li class="chapter" data-level="7" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>7</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>7.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>7.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="7.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>7.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>8</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="8.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#introduction-1"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#transfer-learning-in-nlp"><i class="fa fa-check"></i><b>8.2</b> Transfer Learning in NLP</a></li>
<li class="chapter" data-level="8.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#steps-in-sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>8.3</b> Steps in sequential inductive transfer learning</a></li>
<li class="chapter" data-level="8.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#most-popular-models"><i class="fa fa-check"></i><b>8.4</b> Most popular models</a><ul>
<li class="chapter" data-level="8.4.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>8.4.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="8.4.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit---cutting-edge-model-using-lstms"><i class="fa fa-check"></i><b>8.4.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="8.4.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>8.4.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>9</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="9.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>9.1</b> Attention</a><ul>
<li class="chapter" data-level="9.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>9.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="9.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>9.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="9.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>9.1.3</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#self-attention-1"><i class="fa fa-check"></i><b>9.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="9.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers"><i class="fa fa-check"></i><b>9.2.1</b> Transformers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>10</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="10.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>10.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>10.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="10.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>10.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="10.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>10.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="10.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>10.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="10.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-tasks"><i class="fa fa-check"></i><b>10.1.5</b> Next-sentence Tasks</a></li>
<li class="chapter" data-level="10.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>10.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="10.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>10.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="10.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>10.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="10.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>10.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>10.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>10.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="10.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>10.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="10.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>10.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="10.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>10.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="10.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>10.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="10.2.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion"><i class="fa fa-check"></i><b>10.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>10.3</b> XLNet</a><ul>
<li class="chapter" data-level="10.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>10.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="10.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>10.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="10.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>10.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="10.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>10.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="10.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>10.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="10.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>10.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="10.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>10.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>10.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="12" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="12.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>12.1</b> Metrics</a></li>
<li class="chapter" data-level="12.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>12.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="12.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>12.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="12.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>12.2.2</b> CoQA</a></li>
<li class="chapter" data-level="12.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>12.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="12.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>12.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="12.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>12.2.5</b> SNLI</a></li>
<li class="chapter" data-level="12.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#lambada"><i class="fa fa-check"></i><b>12.2.6</b> LAMBADA</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-training-resources"><i class="fa fa-check"></i><b>12.3</b> Pre-Training Resources</a></li>
<li class="chapter" data-level="12.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>12.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="software-for-nlp-the-huggingface-transformers-module.html"><a href="software-for-nlp-the-huggingface-transformers-module.html"><i class="fa fa-check"></i><b>13</b> Software for NLP: The huggingface transformers module</a></li>
<li class="chapter" data-level="14" data-path="use-bases-for-nlp.html"><a href="use-bases-for-nlp.html"><i class="fa fa-check"></i><b>14</b> Use-Bases for NLP</a></li>
<li class="chapter" data-level="15" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>15</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="15.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#introduction-2"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>15.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="15.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>15.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="15.3.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#encoder-decoder-architecture-1"><i class="fa fa-check"></i><b>15.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="15.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>15.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="15.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>15.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="15.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>15.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="15.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>15.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#question-answer-systems"><i class="fa fa-check"></i><b>15.4</b> Question-Answer Systems</a><ul>
<li class="chapter" data-level="15.4.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#datasets-1"><i class="fa fa-check"></i><b>15.4.1</b> Datasets</a></li>
<li class="chapter" data-level="15.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>15.4.2</b> Types</a></li>
<li class="chapter" data-level="15.4.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>15.4.3</b> Architectures</a></li>
<li class="chapter" data-level="15.4.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#evaluation-metrics"><i class="fa fa-check"></i><b>15.4.4</b> Evaluation Metrics</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>15.5</b> Dialog Systems</a><ul>
<li class="chapter" data-level="15.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types-1"><i class="fa fa-check"></i><b>15.5.1</b> Types</a></li>
<li class="chapter" data-level="15.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures-1"><i class="fa fa-check"></i><b>15.5.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="natural-language-generation.html"><a href="natural-language-generation.html#conclusion-2"><i class="fa fa-check"></i><b>15.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="use-case-ii.html"><a href="use-case-ii.html"><i class="fa fa-check"></i><b>16</b> Use-Case II</a></li>
<li class="chapter" data-level="17" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>17</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="attention-and-self-attention-for-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Attention and Self-Attention for NLP</h1>
<p><em>Authors: Joshua Wagner</em></p>
<p><em>Supervisor: Matthias Aßenmacher</em></p>
<p>Both attention and self-attention were important for the advances made in NLP.
The first part is an overview of attention as it is a building block for self-attention.
The second part focuses on self-attention which enabled the commonly used models
for transfer learning that are seen today.</p>
<div id="attention" class="section level2">
<h2><span class="header-section-number">9.1</span> Attention</h2>
<p>In this part of the chapter, we revisit the Encoder-Decoder architecture that was introduced
in chapter <a href="01-02-rnns-and-their-applications-in-nlp">3</a>. We focus on the improvements
that were made with the development of attention mechanisms on the example of neural machine translation (nmt).</p>
<p>As seen in chapter <a href="01-02-rnns-and-their-applications-in-nlp">3</a>, traditional early
encoder-decoder architecture passes the last hidden state of the encoder to the decoder.
This leads to the problem that information is lost in long input sequences.
Especially information found early in the sequence tends to be “forgotten” after
the entire sequence is processed. The addition of bi-directional layers remedies
this by processing the input in reversed order. The problem still persists for mid
sections of very long input sequences. The development of attention enables the decoder to
attend to the entire input sequence.</p>
<div id="bahdanau-attention" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Bahdanau-Attention</h3>
<p>In 2015, <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-bahdanau2014neural">2014</a>)</span> proposed attention to fix the information problem that
the before seen encoder-decoder architecture faced. Early decoders are trained to predict <span class="math inline">\(y_{t&#39;}\)</span>
given a context vector <span class="math inline">\(c\)</span> and all earlier predicted words <span class="math inline">\(\{y_t, \dots, y_{t&#39;-1}\}\)</span>.
<span class="math display">\[c=q(\{h_1,\dots,h_T\})\]</span> where <span class="math inline">\(h_1,\dots,h_T\)</span> are the the hidden states of the encoder for the input sequence
<span class="math inline">\(x_1,\dots, x_T\)</span> and <span class="math inline">\(q\)</span> is a non-linear function. <span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-sutskever2014sequence">2014</a>)</span> for example used
<span class="math inline">\(q(\{h_1,\dots,h_T\}) = h_T\)</span> as their non-linear transformation which remains a popular
choice for architecture without attention.
Attention changes the context vector <span class="math inline">\(c\)</span> that a decoder uses for translation from a fixed
length vector <span class="math inline">\(c\)</span> of a sequence of hidden states <span class="math inline">\(h_1, \dots, h_T\)</span> to a sequence
of context vectors <span class="math inline">\(c_i\)</span>. The hidden state <span class="math inline">\(h_i\)</span> has a strong focus on the <em>i</em>-th
word in the input sequence and its surroundings.
Each <span class="math inline">\(h_i\)</span> is computed by a concatenation of the forward
<span class="math inline">\(\overrightarrow{h_i}\)</span> and backward <span class="math inline">\(\overleftarrow{h_i}\)</span> hidden states of the
bi-directional encoder.</p>
<p><span class="math display">\[
h_i = [\overrightarrow{h_i}; \overleftarrow{h_i}], i = 1,\dots,n
\]</span>
The hidden states of the decoder <span class="math inline">\(s_t\)</span> at time-point <span class="math inline">\(t\)</span> is computed as <span class="math inline">\(s_t = f(s_{t-1},y_{t-1},c_t)\)</span>.
The context vector <span class="math inline">\(c_t\)</span> is computed as a weighted sum of the hidden
states <span class="math inline">\(h_1,\dots, h_{T_x}\)</span>:</p>
<p><span class="math display">\[
c_t = \sum^{T_x}_{i=1}\alpha_{t,i}h_i.
\]</span>
The weight <span class="math inline">\(\alpha_{t,i}\)</span> of each hidden state <span class="math inline">\(h_i\)</span> is also called the alignment score.
These alignment scores are computed as:</p>
<p><span class="math display">\[
\alpha_{t,i} = align(y_t, x_i) =\frac{exp(score(s_{t-1},h_i))}{\sum^{n}_{i&#39;=1}exp(score(s_{t-1},h_{i&#39;}))}
\]</span>
with <span class="math inline">\(s_{t-1}\)</span> being the hidden state of the decoder at time-step <span class="math inline">\(t-1\)</span>.
The alignment score <span class="math inline">\(\alpha_{t,i}\)</span> models how well input <span class="math inline">\(x_i\)</span> and output <span class="math inline">\(y_t\)</span> match
and assigns the weight to <span class="math inline">\(h_i\)</span>. <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-bahdanau2014neural">2014</a>)</span> parametrize their alignment
score with a single-hidden-layer feed-forward neural network which is jointly
trained with the other parts of the architecture. The score function used by Bahdanau et
al. is given as</p>
<p><span class="math display">\[
score(s_t,h_i) = v_\alpha^Ttanh(\mathbf{W}_\alpha[s_t;h_i])
\]</span>
were tanh is used as a non-linear activation function and <span class="math inline">\(v_\alpha\)</span> and <span class="math inline">\(W_\alpha\)</span>
are the weight matrices to be learned by the alignment model. The alignment score function
is called “concat” in <span class="citation">Luong, Pham, and Manning (<a href="#ref-luong2015effective">2015</a>)</span> and “additive attention” in <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span>
because of the concatenation seen above. A nice by-product of attention is a matrix of alignment scores
which can be visualised to show the correlation between source and target words.</p>
<div class="figure">
<img src="figures/02-02-attention-and-self-attention-for-nlp/bahdanau-fig3.png" alt="Alignment Matrix visualised for a French to English translation. Image source: Fig 3 in Bahdanau, Cho, and Bengio (2014)" />
<p class="caption">Alignment Matrix visualised for a French to English translation. Image source: Fig 3 in <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-bahdanau2014neural">2014</a>)</span></p>
</div>
<p>The attention model proposed by Bahdanau et al. is also called a soft/global attention model as it attends
to every input in the sequence.</p>
</div>
<div id="luong-attention" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Luong-Attention</h3>
<ul>
<li><p>intro loung</p></li>
<li><p>different proposed score functions</p></li>
<li><p>differences to bahdanau</p></li>
<li><p>explanation for local/hard attention and differences to global/soft attention</p></li>
</ul>
</div>
<div id="attention-models" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Attention Models</h3>
<p>Overview over models that use attention and different attentions used, segway to self-attention</p>
<ul>
<li>just a short overview over different attention models/score functions</li>
</ul>
</div>
</div>
<div id="self-attention-1" class="section level2">
<h2><span class="header-section-number">9.2</span> Self-Attention</h2>
<p>general intro to self-attention</p>
<ul>
<li>intro self attention, explanation is done in the transformers part</li>
</ul>
<div id="transformers" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Transformers</h3>
<p>explain transformer architecture, multi-head attention, dot-prod. attention, explain the differences in computational cost to rnns and conv. models</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bahdanau2014neural">
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” <em>arXiv Preprint arXiv:1409.0473</em>.</p>
</div>
<div id="ref-luong2015effective">
<p>Luong, Minh-Thang, Hieu Pham, and Christopher D Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” <em>arXiv Preprint arXiv:1508.04025</em>.</p>
</div>
<div id="ref-sutskever2014sequence">
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3104–12.</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, 5998–6008.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="transfer-learning-for-nlp-i.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="transfer-learning-for-nlp-ii.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/02-02-attention-and-self-attention-for-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
