<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Transfer Learning for NLP II | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Transfer Learning for NLP II | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Transfer Learning for NLP II | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-09-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="attention-and-self-attention-for-nlp.html"/>
<link rel="next" href="introduction-resources-for-nlp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-and-development"><i class="fa fa-check"></i><b>1.1</b> History and development</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#statistical-background"><i class="fa fa-check"></i><b>1.2</b> Statistical Background</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.3</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning-and-system-design-choices"><i class="fa fa-check"></i><b>3.3</b> Hyperparameter Tuning and System Design Choices</a></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.4</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#outlook-and-resources"><i class="fa fa-check"></i><b>3.5</b> Outlook and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extensions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extensions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.2</b> Encoder-Decoder Architecture</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>6</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>6.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>6.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>6.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>7</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="7.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#outline"><i class="fa fa-check"></i><b>7.1</b> Outline</a></li>
<li class="chapter" data-level="7.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>7.2</b> Sequential inductive transfer learning</a><ul>
<li class="chapter" data-level="7.2.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#types-of-transfer-learning"><i class="fa fa-check"></i><b>7.2.1</b> Types of transfer learning</a></li>
<li class="chapter" data-level="7.2.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#feature-extraction-vs.fine-tuning"><i class="fa fa-check"></i><b>7.2.2</b> Feature Extraction vs. Fine-tuning</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#models"><i class="fa fa-check"></i><b>7.3</b> Models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>7.3.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="7.3.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit"><i class="fa fa-check"></i><b>7.3.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="7.3.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>7.3.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>8</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="8.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>8.1</b> Attention</a><ul>
<li class="chapter" data-level="8.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>8.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>8.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="8.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#computational-difference-between-luong--and-bahdanau-attention"><i class="fa fa-check"></i><b>8.1.3</b> Computational Difference between Luong- and Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.4" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>8.1.4</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>8.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="8.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#the-transformer"><i class="fa fa-check"></i><b>8.2.1</b> The Transformer</a></li>
<li class="chapter" data-level="8.2.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers-as-rnns"><i class="fa fa-check"></i><b>8.2.2</b> Transformers as RNNs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>9</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="9.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>9.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>9.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="9.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>9.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="9.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>9.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="9.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>9.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="9.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-prediction"><i class="fa fa-check"></i><b>9.1.5</b> Next-sentence Prediction</a></li>
<li class="chapter" data-level="9.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>9.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>9.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>9.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>9.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="9.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>9.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="9.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>9.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="9.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>9.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="9.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>9.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="9.2.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>9.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>9.3</b> XLNet</a><ul>
<li class="chapter" data-level="9.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>9.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="9.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>9.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="9.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>9.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="9.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>9.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="9.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>9.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="9.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>9.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="9.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>9.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>9.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>10</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="11" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="11.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>11.1</b> Metrics</a></li>
<li class="chapter" data-level="11.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>11.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="11.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>11.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="11.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>11.2.2</b> CoQA</a></li>
<li class="chapter" data-level="11.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>11.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="11.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>11.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="11.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>11.2.5</b> SNLI</a></li>
<li class="chapter" data-level="11.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#overview"><i class="fa fa-check"></i><b>11.2.6</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-trained-models"><i class="fa fa-check"></i><b>11.3</b> Pre-Trained Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#bert"><i class="fa fa-check"></i><b>11.3.1</b> BERT</a></li>
<li class="chapter" data-level="11.3.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#openai-gpt-3"><i class="fa fa-check"></i><b>11.3.2</b> OpenAI GPT-3</a></li>
<li class="chapter" data-level="11.3.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#google-5t"><i class="fa fa-check"></i><b>11.3.3</b> Google 5T</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>11.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-cases-for-nlp.html"><a href="use-cases-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Use-Cases for NLP</a></li>
<li class="chapter" data-level="13" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>13</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="13.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>13.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="13.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>13.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="13.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>13.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="13.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>13.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="13.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>13.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="13.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>13.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="13.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>13.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>13.4</b> Dialog Systems</a><ul>
<li class="chapter" data-level="13.4.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>13.4.1</b> Types</a></li>
<li class="chapter" data-level="13.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>13.4.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#image-captioning-system"><i class="fa fa-check"></i><b>13.5</b> Image Captioning System</a><ul>
<li class="chapter" data-level="13.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#experiments"><i class="fa fa-check"></i><b>13.5.1</b> Experiments</a></li>
<li class="chapter" data-level="13.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#implementation"><i class="fa fa-check"></i><b>13.5.2</b> Implementation</a></li>
<li class="chapter" data-level="13.5.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#results"><i class="fa fa-check"></i><b>13.5.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>13.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>14</b> Epilogue</a><ul>
<li class="chapter" data-level="14.1" data-path="epilogue.html"><a href="epilogue.html#new-influentioal-architectures"><i class="fa fa-check"></i><b>14.1</b> New influentioal architectures</a></li>
<li class="chapter" data-level="14.2" data-path="epilogue.html"><a href="epilogue.html#improvements-of-the-self-attention-mechanism"><i class="fa fa-check"></i><b>14.2</b> Improvements of the Self-Attention mechanism</a></li>
<li class="chapter" data-level="14.3" data-path="epilogue.html"><a href="epilogue.html#evaluation-and-interpretability"><i class="fa fa-check"></i><b>14.3</b> Evaluation and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transfer-learning-for-nlp-ii" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Transfer Learning for NLP II</h1>
<p><em>Authors: Bailan He</em></p>
<p><em>Supervisor: M. Aßenmacher</em></p>
<p>Unsupervised representation learning has been highly successful in NLP. Typically, these methods first pre-train neural networks on large-scale unlabeled text corpora and then fine-tune the models on downstream tasks. Here we introduce the three remarkable models, BERT, GPT-2, and XLNet. <a href="%22https://github.com/huggingface/transformers%22">Transformers</a> is an excellent Github repository, where readers can find their implementations.</p>
<div id="bidirectional-encoder-representations-from-transformers-bert" class="section level2">
<h2><span class="header-section-number">9.1</span> Bidirectional Encoder Representations from Transformers (BERT)</h2>
<div id="autoencoding" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Autoencoding</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure01"></span>
<img src="figures/02-03-transfer-learning-for-nlp/autoencoding.png" alt="Autoencoding" width="50%" />
<p class="caption">
FIGURE 9.1: Autoencoding
</p>
</div>
<p>Autoencoding(AE) have been most successful pre-training objectives and figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure01">9.1</a> shows the modeling of it. AE based model does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. Specifically, given a text sequence <span class="math inline">\(X = (x_1,...,x_T)\)</span>, AE factorizes the log-likelihood into a partial sum <span class="math display">\[log p(\bar{X}|\hat{X}) = \sum^T_{t=1} mask_t p(x_t|\hat{X})\]</span>, where <span class="math inline">\(mask_t\)</span> is an indicator to show if a token is masked,<span class="citation">Yang et al. (<a href="#ref-yang2019xlnet">2019</a>)</span>.</p>
<p>The training objective is to reconstruct the masked tokens <span class="math inline">\(\bar{X}\)</span> given the sequence <span class="math inline">\(\hat{X}\)</span>. AE tries to find the best model <span class="math inline">\(P\)</span> to predict the masked tokens. The BERT introduced next is one of the most important AE models.</p>
</div>
<div id="introduction-of-bert" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Introduction of BERT</h3>
<p>BERT is published by researchers at Google AI in 2018. It is regarded as a milestone in the NLP community by proposing a bidirectional Language model based on Transformer.</p>
<p>BERT is a notable example of AE and it uses the structure of the AE model just mentioned, which means that some tokens in the training data will be masked. In BERT 15% of tokens are replaced by a special symbol [MASK], and the model is trained to reconstruct the original sequence from the masked tokens. By contrast with previous efforts that looked at a text sequence either from left to right(RNN <a href="#Recurrent%20neural%20networks%20and%20their%20applications%20in%20NLP"><strong>chapter 5</strong></a>) or combined left-to-right and right-to-left training (ELMO <a href="#Transfer-Learning-for-NLP-i"><strong>chapter 8</strong></a>), the Transformer Encoder (<a href="#%20Attention%20and%20Self-Attention%20for%20NLP"><strong>chapter 9</strong></a>) utilizes bidirectional contexts simultaneously. Therefore BERT uses the Transformer Encoder as the structure of the pre-train model and addresses the unidirectional constraints by proposing new pre-training objectives: the Masked Language Model(MLM) and Next-sentence Prediction(NSP).</p>
<p>The special pre-training structure of BERT enables it to pre-train deep bidirectional representations and after fine-tuning based the representations, BERT advances state-of-the-art performance for eleven NLP tasks. Its unique structural ideas and excellent model performance make BERT the most important NLP model at present.</p>
</div>
<div id="input-representation-of-bert" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Input Representation of BERT</h3>
<p>For NLP models, the input representation of the sequence is the basis of excellent model performance, many scholars have conducted in-depth research on methods to obtain word embeddings for a long time <a href="#Foundations/Applications%20of%20Modern%20NLP"><strong>chapter 4</strong></a>. As for BERT, due to the model structure, the input representations need to be able to unambiguously represent both a single text sentence or a pair of text sentences in one token sequence. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings, <span class="citation">Devlin et al. (<a href="#ref-bert">2018</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure02"></span>
<img src="figures/02-03-transfer-learning-for-nlp/bert_input_representation.png" alt="BERT input representation" width="80%" />
<p class="caption">
FIGURE 9.2: BERT input representation
</p>
</div>
<p>Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure02">9.2</a> is the visual representation of input representations of BERT tokens. The specifics are:</p>
<ul>
<li><p>As for Token Embeddings (yellow block): Use WordPiece embeddings <span class="citation">Wu et al. (<a href="#ref-wu2016google">2016</a>)</span> with a 30,000 token vocabulary and split word pieces denoted with ##. e.g.[ playing = play and ##ing ] and the first token of every sequence is always the special classification embedding [CLS]. For non-classification tasks, this vector is ignored. Sentence pairs are packed together into a single sequence and are separated with a special token [SEP].</p></li>
<li><p>As for Segment Embeddings (green block): For the input is a sequence with two sentences, different learned sentence embedding[e.g., A and B] will be added to every token of each sentence. For single-sentence inputs, we only use the sentence A embeddings.</p></li>
<li><p>As for Position Embeddings (grey block): For languages, the order of each word in a sentence is quite important, so the position of each token will be marked as Position embeddings.</p></li>
<li><p>BERT limits the length of the entire sequence to no more than 512 tokens. Whether it is a one-sentence sequence or a sentence-pairs sequence, sequences exceeding 512 will be divided at intervals of 512 tokens. In practice, considering computational efficiency, BERT mostly divides the sequence with a length of 128 tokens.</p></li>
</ul>
<p>Finally, BERT adds these three types of embeddings to get the final input representation. And BERT will use the input representation obtained above to pre-train the model, below we respectively introduce how BERT uses MLM and NSP for pre-training.</p>
</div>
<div id="masked-language-model" class="section level3">
<h3><span class="header-section-number">9.1.4</span> Masked Language Model</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure03"></span>
<img src="figures/02-03-transfer-learning-for-nlp/bert_masked_task.png" alt="BERT Masked Language Model  
 Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-bert/" width="70%" />
<p class="caption">
FIGURE 9.3: BERT Masked Language Model<br />
Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from <a href="http://jalammar.github.io/illustrated-bert/" class="uri">http://jalammar.github.io/illustrated-bert/</a>
</p>
</div>
<p>Masked Language Model is the most important model structure of BERT and it mainly combines the ideas of Transformer Encoder and masked tokens.</p>
<p>Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure03">9.3</a> is the visual representation of the Masked Language Model.
As shown in the figure, when word embeddings are fed into BERT, 15% of the tokens in each sequence will be replaced by a special token [MASK]. Here, token [improvisation] is replaced with [MASK]. The output of MLM is the word embeddings of corresponding tokens, then feed the word embedding of [MASK] token into a simple softmax classifier and get the final prediction of [MASK] token. The task of MLM is to predict the original value of masked tokens, we hope that the result obtained by the softmax classifier is close to the true value. And the most important part is the “yellow” block, it’s basically a multi-layer bidirectional Transformer Encoder based on implementation described in <span class="citation">Kaiser et al. (<a href="#ref-kaiser2017one">2017</a>)</span>.</p>
<p>Here I summarize the main points of MLM in the form of question and answer as follows:</p>
<ul>
<li>What is the Masked Language Model?
<ul>
<li>15% of all WordPiece tokens in each sequence will be randomly masked.</li>
<li>Input: token embedding(one sequence, begin with [CLS])</li>
<li>Output: BERT token embedding.</li>
<li>Using softmax classifier to predict the masked token. (words match each other may have the same BERT embedding)</li>
</ul></li>
<li>How to mask?
<ul>
<li>80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy <span class="math inline">\(\Rightarrow\)</span> my dog is [MASK]</li>
<li>10% of the time: Replace the word with a random word,e.g., my dog is hairy <span class="math inline">\(\Rightarrow\)</span> my dog is apple.</li>
<li>10% of the time: Keep the word unchanged, e.g., my dog is hairy <span class="math inline">\(\Rightarrow\)</span> my dog is hairy.</li>
</ul></li>
<li>Why there are two other methods to replace the word?
<ul>
<li>Why keep 10% of masked tokens unchanged?<br />
In some downstream tasks like POSTagging, all the tokens are known, if BERT only trained the Masked sequences, then the model only uses the information of context, exclude the information of the masked words. It will lose a part of the information, then weakens the performance of the model.</li>
<li>Why replace 10% of the masked tokens with random words?<br />
Since we keep 10% of the masked token unchanged, if we do not add random noise, the model will be “lazy” in our training, the model will plagiarize current tokens, rather than learning.</li>
</ul></li>
</ul>
</div>
<div id="next-sentence-prediction" class="section level3">
<h3><span class="header-section-number">9.1.5</span> Next-sentence Prediction</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure04"></span>
<img src="figures/02-03-transfer-learning-for-nlp/bert_nsp.png" alt="BERT Next-sentence Tasks  
 Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-bert/" width="70%" />
<p class="caption">
FIGURE 9.4: BERT Next-sentence Tasks<br />
Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from <a href="http://jalammar.github.io/illustrated-bert/" class="uri">http://jalammar.github.io/illustrated-bert/</a>
</p>
</div>
<p>Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure04">9.4</a> is the visual representation of Next-sentence Prediction(NSP).
Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. So BERT proposes the NSP by using the special token [CLS] as the first token of every sequence. The pre-training structure of NSP and MLM are the same, and NSP and MLM are trained together. NSP using token [CLS] to get the result. For example, token [CLS] will get a binary classification prediction through the softmax classifier, which represents whether the model believes that the sentiment of the two sentences in the input sequence is the same.</p>
<p>Here I also use the question and answer format to summarize the main points of NSP:</p>
<ul>
<li>What is Next-sentence Prediction?
<ul>
<li>Input: token embedding (two sentences, begin with [CLS], each sentence ends with [SEP])</li>
<li>Output: BERT token embedding.</li>
<li>Using a softmax classifier to explain the relationship between two sentences.</li>
<li>Using [CLS] token to pre-train classification tasks.</li>
<li>Sentences can be trivially generated from a monolingual corpus.</li>
<li>Choose sentences A and B for each example, 50% of B is the actual next sentence that follows A, and 50% of B is a random sentence from the corpus.</li>
</ul></li>
<li>For example:
<ul>
<li>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk[SEP]<br />
Label = IsNext.</li>
<li>Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP].<br />
Label = NotNext.</li>
</ul></li>
</ul>
</div>
<div id="pre-training-procedure-of-bert" class="section level3">
<h3><span class="header-section-number">9.1.6</span> Pre-training Procedure of BERT</h3>
<p>For the pre-training corpus, BERT uses the concatenation of BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words) to create two versions of BERT (L stands for the number of layers, H stands for the hidden size, A stands for the number of self-attention heads):</p>
<ul>
<li>BERT-Base: L = 12, H = 768, A = 12, Total parameters = 110M</li>
<li>BERT-Large: L = 24, H = 1024, A = 16, Total parameters = 340M</li>
</ul>
</div>
<div id="fine-tuning-procedure-of-bert" class="section level3">
<h3><span class="header-section-number">9.1.7</span> Fine-tuning Procedure of BERT</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure05"></span>
<img src="figures/02-03-transfer-learning-for-nlp/bert_based_model.png" alt="BERT Task Specific Models" width="60%" />
<p class="caption">
FIGURE 9.5: BERT Task Specific Models
</p>
</div>
<p>For the BERT model obtained by pre-training, different types of tasks require different modifications to the model, and the modification of the model before fine-tuning is quite simple. As shown in Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure05">9.5</a>:</p>
<p>As for the sequence-level classification problem, such as sentiment analysis, task (a) and (b) in the figure, BERT takes the output representation of the first token[CLS] then feeds it to a softmax classifier to get the classification prediction and uses this prediction as model output for fine-tuning.</p>
<p>As for the Question Answering Task (e.g.Reading comprehension, task(c)), BERT needs to find the correct answer in the latter sentence to answer the question raised by the previous sentence. For each token in the second sentence, BERT will use the output embeddings of the token to make two predictions, representing whether the token is the beginning or the end of the answer. For example, if the third token of the second sentence is considered to be the beginning of the answer, and the fifth token is considered to be the end of the answer, then BERT will use the third token to the fifth token as the result to answer the questions raised by the first sentence. If the fifth token is considered to be the beginning of the answer and the third token is considered to be the end of the answer, that is, the end appears before the beginning, then the resulting output by BERT will be: No Answer. The true answer in reading comprehension may happen to be No Answer. BERT performs fine-tuning by comparing the loss between the prediction and the true value. It is worth noting that in reading comprehension or summarization tasks, BERT is not doing real summarization. It cannot generate new vocabulary by itself, but can only choose from the vocabulary of the latter sentence. This may be the reason why BERT does not perform so well in corresponding tasks.</p>
<p>As for token-level classification (e.g. NER, Task (d) in the figure), BERT takes the output of the last layer transformer of all tokens then feeds it to the softmax layer for classification and uses the prediction of each token of the model to compare with the real answer and fine-tune the parameters.</p>
</div>
<div id="feature-extraction" class="section level3">
<h3><span class="header-section-number">9.1.8</span> Feature Extraction</h3>
<p>Like many other Language models, the pre-trained BERT can create contextualized word embeddings. Then the word embeddings can be used as features in other models. Readers can try out BERT through <a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb">BERT FineTuning with Cloud TPUs</a>.</p>
</div>
<div id="bert-like-models" class="section level3">
<h3><span class="header-section-number">9.1.9</span> BERT-like models</h3>
<p>The state-of-the-art performance of BERT reveals the deep bidirectional language model can significantly improve the model performance in NLP tasks, and BERT chart a new course that how a real bidirectional model should be.</p>
<p>However, BERT has also the following weaknesses:</p>
<ul>
<li>First of all, the input to BERT contains artificial symbols like [MASK] that never occur in downstream tasks, which creates a pre-train-fine-tuning discrepancy problem.</li>
<li>Secondly, BERT assumes the predicted tokens are independent of others given the unmasked tokens, which is oversimplified for natural language.</li>
</ul>
<p>Several models are inspired by BERT to solve these problems:</p>
<p>Roberta <span class="citation">Liu et al. (<a href="#ref-liu2019roberta">2019</a>)</span> shows hyperparameter choices have a significant impact on the final results. It improves BERT pre-training in the following aspects to get better performance:</p>
<ul>
<li>Changing the input embedding to Byte Pair Encoding (BPE) <span class="citation">Sennrich, Haddow, and Birch (<a href="#ref-sennrich2015neural">2015</a>)</span>.</li>
<li>Using dynamic masking: each train has different training data.</li>
<li>Using full sentence without NSP.</li>
<li>More Data, larger batch size (8k), and longer training (100k to 300k steps).</li>
</ul>
<p>ALBERT <span class="citation">Lan et al. (<a href="#ref-lan2019albert">2019</a>)</span> mainly makes three improvements to BERT, which reduces the overall parameter amount, accelerates the training speed, and improves the model performance under the same training time.</p>
<ul>
<li>Using factorized embedding parameterization.</li>
<li>Cross-layer parameter sharing, which significantly reduces the number of parameters.</li>
<li>Replacing NSP with Sentence-order prediction loss (SOP).</li>
</ul>
<p>There are also BERT-like models pre-trained on domain-specific corpora, for example, SciBERT <span class="citation">Beltagy, Lo, and Cohan (<a href="#ref-beltagy2019scibert">2019</a>)</span> on scientific publications, ERNIE <span class="citation">Zhang et al. (<a href="#ref-zhang2019ernie">2019</a>)</span> on a large corpus incorporating knowledge graph in the input. In comparison to fine-tune original BERT, training on the domain-specific corpora then fine-tuning them on downstream NLP tasks has shown to yield better performance.</p>
</div>
</div>
<div id="generative-pre-traininggpt-2" class="section level2">
<h2><span class="header-section-number">9.2</span> Generative Pre-Training(GPT-2)</h2>
<p>The GDP-2 mentioned next is also the most important NLP model in recent years. It and BERT are often mentioned at the same time because the model structure it uses is also based on the transformer, but it is the transformer decoder. As mentioned in <a href="#%20Attention%20and%20Self-Attention%20for%20NLP"><strong>chapter 9</strong></a>, the transformer decoder can be regarded as a unidirectional language model, so GDP-2 represents a different way of thinking from BERT and has the surprising ability in writing tasks.</p>
<div id="auto-regressive-language-modelar" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Auto-regressive Language Model(AR)</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure06"></span>
<img src="figures/02-03-transfer-learning-for-nlp/autoregressive.png" alt="Autoregressive" width="50%" />
<p class="caption">
FIGURE 9.6: Autoregressive
</p>
</div>
<p>GPT-2 is a unidirectional language model, such model structure is also called Auto-regressive language model. Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure06">9.6</a> shows the modelling of Auto-regressive language model, it tries to estimate the probability distribution of a sequence with a auto-regressive pattern. Specifically, given a text sequence <span class="math inline">\(X = (x_1,...,x_T)\)</span>, AR language model factorizes the log-likelihood into a forward sum <span class="math inline">\(logp(x) = \sum^T_{t=1} p(x_t|x&lt;t)\)</span> or a backward one <span class="math inline">\(logp(x) = \sum^{t=1}_{T} p(x_t|x&gt;t)\)</span> <span class="citation">Yang et al. (<a href="#ref-yang2019xlnet">2019</a>)</span>. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information.</p>
</div>
<div id="introduction-of-gpt-2" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Introduction of GPT-2</h3>
<p>GPT-2 is proposed by researchers at OpenAI in 2019. It captures the attention of the NLP community for the following characters:</p>
<ul>
<li><p>Instead of the fine-tuning model with specific tasks, GPT-2 demonstrates language models can perform down-stream tasks in a zero-shot setting, which means without any parameter or architecture modification.</p></li>
<li><p>To perform better under the zero-shot setting, GPT-2 becomes extremely large. The result is that training GPT-2 needs enormous data, so researchers also create a new dataset <a href="https://skylion007.github.io/OpenWebTextCorpus/">WebText</a>, which contains millions of webpages.</p></li>
</ul>
<p>With the characters above, GPT-2 achieves state-of-the-art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText.</p>
<p>Readers can experiment with GPT-2 by using <a href="https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is">AllenAI GPT-2</a>. You can input a sentence and see the prediction of the next words.</p>
</div>
<div id="input-representation-of-gpt-2" class="section level3">
<h3><span class="header-section-number">9.2.3</span> Input Representation of GPT-2</h3>
<p>GPT-2 uses a human-curated dataset called <a href="https://skylion007.github.io/OpenWebTextCorpus/">WebText</a>, that contains text scraped from 45 million web-links. All results presented in paper use a preliminary version of WebText, which contains slightly over 8 million documents for a total of 40GB of text after de-duplication and some heuristic-based cleaning <span class="citation">Radford et al. (<a href="#ref-radford2019gpt2">2019</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure07"></span>
<img src="figures/02-03-transfer-learning-for-nlp/gpt_input_representation.png" alt="GPT-2 Input Representation  
 Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-gpt2/" width="70%" />
<p class="caption">
FIGURE 9.7: GPT-2 Input Representation<br />
Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from <a href="http://jalammar.github.io/illustrated-gpt2/" class="uri">http://jalammar.github.io/illustrated-gpt2/</a>
</p>
</div>
<p>Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure07">9.7</a> shows the input representation of GPT-2. The input sequence has a start token [S], and input embedding of each token is the corresponding specially designed Byte Pair Encoding (BPE) <span class="citation">Sennrich, Haddow, and Birch (<a href="#ref-sennrich2015neural">2015</a>)</span>, adding up the positional encoding vector.</p>
</div>
<div id="the-decoder-only-block" class="section level3">
<h3><span class="header-section-number">9.2.4</span> The Decoder-Only Block</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure08"></span>
<img src="figures/02-03-transfer-learning-for-nlp/gpt_decoder.png" alt="GPT-2 Model  
 Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-gpt2/" width="70%" />
<p class="caption">
FIGURE 9.8: GPT-2 Model<br />
Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from <a href="http://jalammar.github.io/illustrated-gpt2/" class="uri">http://jalammar.github.io/illustrated-gpt2/</a>
</p>
</div>
<p>Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure08">9.8</a> shows the model of GPT-2. This model essentially is the Transformer decoder, except they threw away the second self-attention layer. In each decoder, Layer normalization <span class="citation">Ba, Kiros, and Hinton (<a href="#ref-ba2016layer">2016</a>)</span> is moved to the input of each sub-block, similar to a pre-activation residual network <span class="citation">He et al. (<a href="#ref-he2016identity">2016</a><a href="#ref-he2016identity">b</a>)</span>, and an additional layer normalization is added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used <span class="citation">Radford et al. (<a href="#ref-radford2019gpt2">2019</a>)</span>.</p>
<p>GPT-2 reads from the start token [s] till the last predicted token to predict the next token. For example, in the first round, model uses [s] to predict [robot], in the next round the input is updated as {[s],[robot]} since [robot] has been predicted. This is how Masked self-Attention in Decoder block works.</p>
<p>Otherwise, since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed.
The model of GPT-2 should be :
<span class="math display">\[log p(x)=log\sum^{n}_{i=1}p(s_n|s_1,...,s_{n-1};task_i)\]</span></p>
<p>For example, a translation training example can be represented as the sequence (Translate to the french, English text, French text). Likewise, a reading comprehension training example can be written as (Answer the question, Document, Question, Answer).</p>
<p>By using the specially designed WebText, GPT-2 can be used by following patterns for different tasks.</p>
<ul>
<li><p>Reading Comprehension: data sequence, “Q:”, question sequence, “A:”</p></li>
<li><p>Summarization: data sequence, “TL;DR:”</p></li>
<li><p>Translation: English sentence 1 = French sentence 1, English sentence 2 = French sentence 2, “English sentence 3 =”</p></li>
</ul>
</div>
<div id="gpt-2-models" class="section level3">
<h3><span class="header-section-number">9.2.5</span> GPT-2 Models</h3>
<table>
<caption><span id="tab:ch02-03-table01">TABLE 9.1: </span>GPT-2 models size</caption>
<thead>
<tr class="header">
<th align="left">Parameters</th>
<th align="right">Layers</th>
<th align="right">Dimensionality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">117M</td>
<td align="right">12</td>
<td align="right">768</td>
</tr>
<tr class="even">
<td align="left">345M</td>
<td align="right">24</td>
<td align="right">1024</td>
</tr>
<tr class="odd">
<td align="left">762M</td>
<td align="right">36</td>
<td align="right">1280</td>
</tr>
<tr class="even">
<td align="left">1542M</td>
<td align="right">48</td>
<td align="right">1600</td>
</tr>
</tbody>
</table>
<p>Four versions models are trained, the architectures are summarized in <a href="transfer-learning-for-nlp-ii.html#tab:ch02-03-table01">9.1</a>. The smallest model is equivalent to the original GPT, and the second smallest equivalent to the largest model from BERT <span class="citation">Devlin et al. (<a href="#ref-bert">2018</a>)</span>. The largest model is called GPT-2, which has 1.5 billion parameters.</p>
</div>
<div id="conclusion" class="section level3">
<h3><span class="header-section-number">9.2.6</span> Conclusion</h3>
<p>The framework of GPT-2 is the combination of pre-training based on Transformer Decoder and fine-tuning based on unsupervised downstream tasks.</p>
<p>After the great success of bidirectional models like BERT, GPT-2 insists on using unidirectional models and still achieves state-of-the-art performance. It proves that the performance of language models can be significantly improved by simply increasing the size of training datasets and models, which is exactly what GPT-2 did and even GPT-2, which has 1.5 billion parameters, still underfits WebText. This result also suggests that datasets are as important as models.</p>
</div>
</div>
<div id="xlnet" class="section level2">
<h2><span class="header-section-number">9.3</span> XLNet</h2>
<div id="introduction-of-xlnet" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Introduction of XLNet</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure09"></span>
<img src="figures/02-03-transfer-learning-for-nlp/xlnet_twomodels.png" alt="AR Language Modeling  and AE" width="90%" />
<p class="caption">
FIGURE 9.9: AR Language Modeling and AE
</p>
</div>
<p>XLNet is proposed by researchers at Google in 2019. Since the autoregressive language model (e.g.GPT-2) is only trained to encode a unidirectional context and not effective at modeling deep bidirectional contexts and autoencoding (e.g.BERT) suffers from the pre-train-fine-tune discrepancy, XLNet borrows ideas from the two types of objectives while avoiding their limitations.</p>
<p>It is a new objective called Permutation Language Modeling. By using a permutation operation during training time, bidirectional context information can be captured and makes it a generalized order-aware autoregressive language model. Besides, XLNet introduces a two-stream self-attention to solve the problem that standard parameterization will reduce the model to bag-of-words.</p>
<p>Two XLNet are released, i.e. XLNet-Base and XLNet-Large, and include the similar settings of corresponding BERT. Empirically, XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.</p>
</div>
<div id="permutation-language-modelingplm" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Permutation Language Modeling(PLM)</h3>
<p>Now Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure010">9.10</a> illustrates the permutation language modeling objective.</p>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure010"></span>
<img src="figures/02-03-transfer-learning-for-nlp/xlnet_pml.png" alt="Illustration of the permutation language modeling objective for predicting x3 given the same input sequence x but with different factorization orders." width="70%" />
<p class="caption">
FIGURE 9.10: Illustration of the permutation language modeling objective for predicting x3 given the same input sequence x but with different factorization orders.
</p>
</div>
<p>Specifically, for a sequence <span class="math inline">\(X\)</span> of length <span class="math inline">\(T\)</span>, there are <span class="math inline">\(T!\)</span> different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides. Let <span class="math inline">\(P_T\)</span> be the set of all possible permutations of a sequence [1,2,…, T] and use <span class="math inline">\(z_t\)</span> and <span class="math inline">\(z_{&lt;t}\)</span> to denote the t-th element and the first t−1 elements of a permutation <span class="math inline">\(p\in P_T\)</span>. Then the permutation language modeling objective can be expressed as follows:</p>

<p>For instance, assume we have a input sequence {I love my dog}.</p>
<p>In the upper left plot of the above Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure010">9.10</a>, when we have a factorization order: {3, 2, 4, 1}, the probability of sequence can be expressed as follows:</p>

<p>as for the third token: {my}, it cannot use the information of all other tokens, so only one arrow from the starting token points to the third token in the plot.</p>
<p>In the upper right plot of the above Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure010">9.10</a>, when we have a factorization order: {2, 4, 3, 1}, the probability of sequence can be expressed as follows:</p>

<p>as for the third token: {my}, it can use the information of the second and fourth tokens because it places after these two tokens in the factorization order. Correspondingly, it cannot use the information of the first token. So in the plot, in addition to the arrow from the starting token, there are arrows from the second and fourth tokens pointing to the third token. The rest two plots in the figure have the same interpretation.</p>
<p>During training, for a fixed factorization order, XL-Net is a unidirectional language model based on the transformer decoder, which performs normal model training. But different factorization order makes the model see different order of words when traversing sentences. In this way, although the model is unidirectional, it can also learn the bidirectional information of the sentence.</p>
<p>It is noteworthy that the sequence order is not actually shuffled but only attention masks are changed to reflect factorization order. With PLM, XLNet can model bidirectional context and the dependency within each token of the sequence.</p>
</div>
<div id="the-problem-of-standard-parameterization" class="section level3">
<h3><span class="header-section-number">9.3.3</span> The problem of Standard Parameterization</h3>
<p>As just mentioned, XL-Net is a language model when the factorization order is fixed, which means that we want the model to be able to predict the t-th word under the condition of knowing the word before t.</p>
<p>The Standard Parameterization can be expressed as follows:</p>
<p><span class="math display">\[p_{\theta}(X_{p_t}=x|x_{p&lt;t})=\frac{e(x)^Th_{\theta}(x_{p&lt;t})}{\sum_{x^{&#39;}}e(x^{&#39;})^Th_{\theta}(x_{p&lt;t})}\]</span>
where <span class="math inline">\(e(x)\)</span> denotes the embedding of input token and <span class="math inline">\(h_{\theta}(x_{p&lt;t})\)</span> denotes the hidden representation of <span class="math inline">\(x_{p&lt;t}\)</span>.</p>
<p>While the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. Specifically, let’s consider two different permutations <span class="math inline">\(p^1\text{:{I love my dog}}\)</span> and <span class="math inline">\(p^2:\text{{I love dog my}}\)</span>. The probability of <span class="math inline">\(\text{{my}}\)</span> in <span class="math inline">\(p^1\)</span>: <span class="math inline">\(\text{P(my|I, love)}\)</span> and the probability of <span class="math inline">\(\text{{dog}}\)</span> in <span class="math inline">\(p^2\)</span>: <span class="math inline">\(\text{P(dog|I, love)}\)</span>are identical. The model will be reduced to predicting a bag-of-words, because <span class="math inline">\(h_{\theta}(x_{z&lt;t})\)</span> does not contain the position of the target.</p>
<p>XLNet resolves the problem by reparameterizing with positions:</p>
<p><span class="math display">\[p_{\theta}(X_{p_t}=x|x_{p&lt;t})=\frac{e(x)^Tg_{\theta}(x_{p&lt;t},p_t)}{\sum_{x^{&#39;}}e(x^{&#39;})^Tg_{\theta}(x_{p&lt;t},p_t)}\]</span>
where <span class="math inline">\(e(x)\)</span> denotes the embedding of input token and <span class="math inline">\(g_{\theta}(x_{p&lt;t},p_t)\)</span> denotes the hidden representation of <span class="math inline">\(x_{p&lt;t}\)</span> and position <span class="math inline">\(p_t\)</span>.
But reparameterization with positions brings another contradiction <span class="citation">Yang et al. (<a href="#ref-yang2019xlnet">2019</a>)</span>:<br />
(1) To predict the token <span class="math inline">\(x_{p_t}\)</span> , <span class="math inline">\(g_{\theta}(x_{p&lt;t},p_t)\)</span> should only use the position <span class="math inline">\(p_t\)</span> and not the content <span class="math inline">\(x_{p_t}\)</span>, otherwise the objective becomes trivial.<br />
(2) To predict the other tokens <span class="math inline">\(x_{p_j}\)</span> with j &gt; t, <span class="math inline">\(g_{\theta}(x_{p&lt;t},p_t)\)</span> should also encode the content <span class="math inline">\(x_{p_t}\)</span> to provide full contextual information.</p>
<p>XLNet proposes the Two-Stream Self-Attention to resolve the contradiction.</p>
</div>
<div id="two-stream-self-attention" class="section level3">
<h3><span class="header-section-number">9.3.4</span> Two-Stream Self-Attention</h3>
<div class="figure" style="text-align: center"><span id="fig:ch02-03-figure011"></span>
<img src="figures/02-03-transfer-learning-for-nlp/xlnet_ts.png" alt="Two-Stream Self-Attention" width="90%" />
<p class="caption">
FIGURE 9.11: Two-Stream Self-Attention
</p>
</div>
<p>Instead of one, two sets of hidden representation are proposed:</p>
<ul>
<li>The content representation <span class="math inline">\(h_{\theta}(x_{p \leq t})\)</span>, this representation encodes both the context and <span class="math inline">\(x_{p_t}\)</span> itself.</li>
<li>The query representation <span class="math inline">\(g_{\theta}(x_{p&lt;t},p_t)\)</span>, which only has information <span class="math inline">\(x_{p&lt;t}\)</span> and the position <span class="math inline">\(p_t\)</span> but not the content <span class="math inline">\(x_{p_t}\)</span>.</li>
</ul>
<p>Figure <a href="transfer-learning-for-nlp-ii.html#fig:ch02-03-figure011">9.11</a> is an example with the Factorization order: <span class="math inline">\(3,2,4,1\)</span>:</p>
<ul>
<li><p><span class="math inline">\(h_i^{(t)}\)</span> denotes the content representation of the i-th token in the t-th layer of self-attention. It is the same as the standard self-attention. For instance, <span class="math inline">\(h_1^{(1)}\)</span> can see all the <span class="math inline">\(h_i^{(0)}\)</span> since the 1-st token is after token <span class="math inline">\({3,2,4}\)</span>.</p></li>
<li><p><span class="math inline">\(g_i^{(t)}\)</span> denote the query representation of the i-th token on the t-th layer of self-attention. It does not have access information about the content <span class="math inline">\(x_{p_t}\)</span>, other trivial.</p></li>
<li><p>Computationally, <span class="math inline">\(h_i^{(0)}\)</span> is the word embeddings, and <span class="math inline">\(g_i^{(0)}\)</span> is a trainable parameter initialized with a trainable vector. Only <span class="math inline">\(h_i^{(t)}\)</span> is used during fine-tuning. The last <span class="math inline">\(g_i^{(t)}\)</span> is used for optimizing the LM loss. A self-attention layer <span class="math inline">\(m=1,..., M\)</span> are schematically updated with a shared set of parameters as follows:</p></li>
</ul>
<p><span class="math display">\[
h_{p_t}^{m} \leftarrow Attention (Q=h_{p_t}^{m-1}, KV=h^{(m-1)}_{p \leq t};\theta)
\text{  (content stream: use both $p_t$ and $x_{p_t}$)}
\\
g_{p_t}^{m}\leftarrow Attention (Q=g_{p_t}^{(m-1)}, KV=h^{(m-1)}_{p&lt;t};\theta)
\text{  (query stream: use $p_t$ but cannot see }x_{p_t})
\]</span></p>
<p>where Q, K, V denote the query, key, and value in an attention operation <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention">2017</a>)</span>.
More details are included in Appendix A.2 for reference <span class="citation">Yang et al. (<a href="#ref-yang2019xlnet">2019</a>)</span>.</p>
</div>
<div id="partial-prediction" class="section level3">
<h3><span class="header-section-number">9.3.5</span> Partial Prediction</h3>
<p>While the PLM has several benefits, optimization is challenging due to the permutation operator. To reduce the optimization difficulty, XLNet only predicts the last tokens in a factorization order. It sets a cutting point <span class="math inline">\(c\)</span> and split the permutation <span class="math inline">\(p\)</span> into a non-target subsequence <span class="math inline">\(p_{\leq c}\)</span> and a target subsequence <span class="math inline">\(p_{&gt;c}\)</span>. The objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e.,</p>
<p><span class="math display">\[\begin{equation}
\max_{\theta}   \mathbb{E}_{p\sim P_T} \left[logp_{\theta}(x_{z_{&gt;c}|x_{z_{\leq c}}})\right] =
\mathbb{E}_{p\sim P_T} \left[\sum_{t=c+1}^{|z|} logp_{\theta}(x_{z_t|x_{z_{\leq t}}})\right]
\end{equation}\]</span>
For unselected tokens, their query representations need not be computed, which saves speed and memory. XLNet incorporates ideas from Transformer-XL and inherits two important characters of it, i.e. Segment-Level Recurrence and Relative Position Encoding, to enable the learning of long-term dependency and resolve the context fragmentation <span class="citation">Dai et al. (<a href="#ref-dai2019transformer">2019</a>)</span>. There is also a good <a href="https://medium.com/@shoray.goel/transformer-xl-9fc13473e0a4">Blog</a> to introduce Transformer-XL, Readers can read if interested.</p>
</div>
<div id="xlnet-pre-training-model" class="section level3">
<h3><span class="header-section-number">9.3.6</span> XLNet Pre-training Model</h3>
<p>After tokenization with SentencePiece <span class="citation">Kudo and Richardson (<a href="#ref-kudo2018sentencepiece">2018</a>)</span>, Researchers obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total, to pre-train the XLNet.</p>
<p>Analogous to BERT, two versions of XLNet have been trained:</p>
<ul>
<li>XLNet-Base: L = 12, H = 768, A = 12, Total parameters = 110M (on BooksCorpus and Wikipedia only)</li>
<li>XLNet-Large: L = 24, H = 1024, A = 16, Total parameters = 340M (on total datasets)</li>
</ul>
</div>
<div id="conclusion-1" class="section level3">
<h3><span class="header-section-number">9.3.7</span> Conclusion</h3>
<p>Language modeling has been a rapidly developing research area. However, most language modelings are unidirectional and BERT <span class="citation">Devlin et al. (<a href="#ref-bert">2018</a>)</span> shows that bidirectional modeling can significantly improve model performance. Unidirectional modelings without specific structure are hard to capture the bidirectional context. Now with the permutation operator, the unidirectional language modelings can become bidirectional modeling. XLNet has built a bridge between language modeling and bidirectional models. Overall, XLNet is a generalized AR pre-training method that uses a permutation language modeling objective to combine the advantages of AR and AE methods.</p>
</div>
</div>
<div id="latest-nlp-models" class="section level2">
<h2><span class="header-section-number">9.4</span> Latest NLP models</h2>
<p>Nowadays NLP has become a competition between big companies. When BERT first came, people talked about it may cost thousands of dollars to train it. Then came GPT-2, which has 1.5 billion parameters and is trained on 40GB data. As I mentioned above, GPT-2 of Open-AI shows that increasing the size of models and datasets is at least as important as proposing a new model architecture.</p>
<p>After GPT-2, researchers at Google did the same thing, they proposed a general language model called T5 <span class="citation">Raffel et al. (<a href="#ref-raffel2019exploring">2019</a>)</span>, which is trained on 750GB corpus - <a href="https://www.tensorflow.org/datasets/catalog/c4">“C4 (Colossal Clean Crawled Corpus)”</a>. If you read the paper, you will find that the last page of it is a table of several experience results, which may cost millions of dollars to reproduce it.</p>
<p>On 28th May 2020, the “Arms race” goes into another level, GPT-3 <span class="citation">Brown et al. (<a href="#ref-brown2020language">2020</a>)</span> emerged, the new paper takes GPT to the next level by making it even bigger - GPT-3 has 175 billion parameters and is trained on a dataset that has 450 billion of tokens <a href="https://github.com/openai/gpt-3">“GPT-3 Dataset”</a>. GPT-3 experiments with the three different settings: zero-shot, one-shot, and Few-shot to show that scaling up language models can greatly improve performance, sometimes even reaching competitiveness with prior SOTA approaches. However, it is conservatively estimated that training GPT-3 will cost one hundred million dollars.</p>
<p>Models like T5 and GPT-3 are very impressive, but the biggest problem at the moment is to find a way to make the current model put into use in the industry. If it can’t bring benefits, the AI industry can’t be sustained by burning money. As for researchers, the truth is, with the resources it is also possible to fail, but it is certainly impossible to succeed without resources now.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ba2016layer">
<p>Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. “Layer Normalization.” <em>arXiv Preprint arXiv:1607.06450</em>.</p>
</div>
<div id="ref-beltagy2019scibert">
<p>Beltagy, Iz, Kyle Lo, and Arman Cohan. 2019. “SciBERT: A Pretrained Language Model for Scientific Text.” In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)</em>, 3606–11.</p>
</div>
<div id="ref-brown2020language">
<p>Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.”</p>
</div>
<div id="ref-dai2019transformer">
<p>Dai, Zihang, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. “Transformer-Xl: Attentive Language Models Beyond a Fixed-Length Context.” <em>arXiv Preprint arXiv:1901.02860</em>. <a href="https://arxiv.org/abs/1901.02860" class="uri">https://arxiv.org/abs/1901.02860</a>.</p>
</div>
<div id="ref-bert">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” <em>CoRR</em> abs/1810.04805. <a href="http://arxiv.org/abs/1810.04805" class="uri">http://arxiv.org/abs/1810.04805</a>.</p>
</div>
<div id="ref-he2016identity">
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016b. “Identity Mappings in Deep Residual Networks.” In <em>European Conference on Computer Vision</em>, 630–45. Springer.</p>
</div>
<div id="ref-kaiser2017one">
<p>Kaiser, Lukasz, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit. 2017. “One Model to Learn Them All.” <em>arXiv Preprint arXiv:1706.05137</em>.</p>
</div>
<div id="ref-kudo2018sentencepiece">
<p>Kudo, Taku, and John Richardson. 2018. “Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” <em>arXiv Preprint arXiv:1808.06226</em>.</p>
</div>
<div id="ref-lan2019albert">
<p>Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. “Albert: A Lite Bert for Self-Supervised Learning of Language Representations.” <em>arXiv Preprint arXiv:1909.11942</em>.</p>
</div>
<div id="ref-liu2019roberta">
<p>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “Roberta: A Robustly Optimized Bert Pretraining Approach.” <em>arXiv Preprint arXiv:1907.11692</em>. <a href="https://arxiv.org/abs/1907.11692" class="uri">https://arxiv.org/abs/1907.11692</a>.</p>
</div>
<div id="ref-radford2019gpt2">
<p>Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.”</p>
</div>
<div id="ref-raffel2019exploring">
<p>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” <em>arXiv Preprint arXiv:1910.10683</em>. <a href="https://arxiv.org/abs/1909.11942" class="uri">https://arxiv.org/abs/1909.11942</a>.</p>
</div>
<div id="ref-sennrich2015neural">
<p>Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2015. “Neural Machine Translation of Rare Words with Subword Units.” <em>arXiv Preprint arXiv:1508.07909</em>.</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, 5998–6008.</p>
</div>
<div id="ref-wu2016google">
<p>Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” <em>arXiv Preprint arXiv:1609.08144</em>.</p>
</div>
<div id="ref-yang2019xlnet">
<p>Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” In <em>Advances in Neural Information Processing Systems 32</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, 5753–63. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf" class="uri">http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf</a>.</p>
</div>
<div id="ref-zhang2019ernie">
<p>Zhang, Zhengyan, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. “ERNIE: Enhanced Language Representation with Informative Entities.” <em>arXiv Preprint arXiv:1905.07129</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="attention-and-self-attention-for-nlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-resources-for-nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/02-03-transfer-learning-for-nlp2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
