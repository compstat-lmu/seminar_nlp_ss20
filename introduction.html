<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-09-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="foreword.html"/>
<link rel="next" href="introduction-deep-learning-for-nlp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-and-development"><i class="fa fa-check"></i><b>1.1</b> History and development</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#statistical-background"><i class="fa fa-check"></i><b>1.2</b> Statistical Background</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.3</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning-and-system-design-choices"><i class="fa fa-check"></i><b>3.3</b> Hyperparameter Tuning and System Design Choices</a></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.4</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#outlook-and-resources"><i class="fa fa-check"></i><b>3.5</b> Outlook and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extentions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extentions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#bidirectional-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Bidirectional RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.2</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.3</b> Encoder-Decoder Architecture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>6</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>6.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>6.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>6.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>7</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#transfer-learning-in-nlp"><i class="fa fa-check"></i><b>7.2</b> Transfer Learning in NLP</a></li>
<li class="chapter" data-level="7.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#steps-in-sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>7.3</b> Steps in sequential inductive transfer learning</a></li>
<li class="chapter" data-level="7.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#most-popular-models"><i class="fa fa-check"></i><b>7.4</b> Most popular models</a><ul>
<li class="chapter" data-level="7.4.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>7.4.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="7.4.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit---cutting-edge-model-using-lstms"><i class="fa fa-check"></i><b>7.4.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="7.4.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>7.4.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>8</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="8.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>8.1</b> Attention</a><ul>
<li class="chapter" data-level="8.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>8.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>8.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="8.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>8.1.3</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>8.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="8.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers"><i class="fa fa-check"></i><b>8.2.1</b> Transformers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>9</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="9.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>9.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>9.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="9.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>9.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="9.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>9.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="9.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>9.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="9.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-tasks"><i class="fa fa-check"></i><b>9.1.5</b> Next-sentence Tasks</a></li>
<li class="chapter" data-level="9.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>9.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>9.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>9.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>9.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="9.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>9.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="9.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>9.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="9.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>9.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="9.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>9.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="9.2.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion"><i class="fa fa-check"></i><b>9.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>9.3</b> XLNet</a><ul>
<li class="chapter" data-level="9.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>9.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="9.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>9.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="9.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>9.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="9.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>9.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="9.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>9.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="9.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>9.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="9.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>9.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>9.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>10</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="11" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="11.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>11.1</b> Metrics</a></li>
<li class="chapter" data-level="11.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>11.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="11.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>11.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="11.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>11.2.2</b> CoQA</a></li>
<li class="chapter" data-level="11.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>11.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="11.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>11.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="11.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>11.2.5</b> SNLI</a></li>
<li class="chapter" data-level="11.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#overview"><i class="fa fa-check"></i><b>11.2.6</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-trained-models"><i class="fa fa-check"></i><b>11.3</b> Pre-Trained Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#bert"><i class="fa fa-check"></i><b>11.3.1</b> BERT</a></li>
<li class="chapter" data-level="11.3.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#openai-gpt-3"><i class="fa fa-check"></i><b>11.3.2</b> OpenAI GPT-3</a></li>
<li class="chapter" data-level="11.3.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#google-5t"><i class="fa fa-check"></i><b>11.3.3</b> Google 5T</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>11.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-cases-for-nlp.html"><a href="use-cases-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Use-Cases for NLP</a></li>
<li class="chapter" data-level="13" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>13</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="13.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>13.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="13.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>13.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="13.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>13.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="13.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>13.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="13.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>13.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="13.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>13.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="13.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>13.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#question-answer-systems"><i class="fa fa-check"></i><b>13.4</b> Question-Answer Systems</a><ul>
<li class="chapter" data-level="13.4.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>13.4.1</b> Datasets</a></li>
<li class="chapter" data-level="13.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>13.4.2</b> Types</a></li>
<li class="chapter" data-level="13.4.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>13.4.3</b> Architectures</a></li>
<li class="chapter" data-level="13.4.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#evaluation-metrics"><i class="fa fa-check"></i><b>13.4.4</b> Evaluation Metrics</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>13.5</b> Dialog Systems</a><ul>
<li class="chapter" data-level="13.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types-1"><i class="fa fa-check"></i><b>13.5.1</b> Types</a></li>
<li class="chapter" data-level="13.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures-1"><i class="fa fa-check"></i><b>13.5.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion"><i class="fa fa-check"></i><b>13.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>14</b> Epilogue</a><ul>
<li class="chapter" data-level="14.1" data-path="epilogue.html"><a href="epilogue.html#new-influentioal-architectures"><i class="fa fa-check"></i><b>14.1</b> New influentioal architectures</a></li>
<li class="chapter" data-level="14.2" data-path="epilogue.html"><a href="epilogue.html#improvements-of-the-selfattention-mechanism"><i class="fa fa-check"></i><b>14.2</b> Improvements of the SelfAttention mechanism</a></li>
<li class="chapter" data-level="14.3" data-path="epilogue.html"><a href="epilogue.html#evaluation-and-interpretability"><i class="fa fa-check"></i><b>14.3</b> Evaluation and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<p><em>Author: Xiao-Yin To</em></p>
<p><em>Supervisor: Daniel Schalk, Matthias Aßenmacher</em></p>
<p>Over the course of the past decades the importance and utilization of artificial intelligence technology has continuously gained traction. In the present times, it is already inextricably linked with most of the surroundings that constitute the human shaped environment. Consequently, a myriad of sectors such as commerce, research and development, information services, engineering, social services, and medical science have already been irreversibly impacted by the capabilities of artificial intelligence.
There are three major fields of artificial intelligence that comprise the technology: speech recognition, computer vision, and natural language processing (see <span class="citation">Yeung (<a href="#ref-yeung2020">2020</a>)</span>).
In this book we will take a closer look at the modern approaches in natural language processing (NLP).</p>
<div id="history-and-development" class="section level2">
<h2><span class="header-section-number">1.1</span> History and development</h2>
<p>The history of artificial intelligence and NLP dates back to the 18th century, where well-known philosophers such as Leibniz, Spinoza, Hobbes, Locke, Kant, and Hume as well as scientists such as La Mettrie and Hartley tried to formulate laws of thought (see <span class="citation">Mccorduck and Cfe (<a href="#ref-mccorduck2004">2004</a>)</span>).</p>
<p>However, the first steps of development started in the 20th century. Alan <span class="citation">Turing (<a href="#ref-turing1937">1937</a>)</span> was the first to propose an abstract Universal Computing Machine and became one of the most defining scientists who shaped the path of the scientific development of artificial intelligence in the following years. He further developed these ideas in his work “Intelligent machinery” (see <span class="citation">Turing (<a href="#ref-turing1948">1948</a>)</span>) and “Computing Machinery And Intelligence” (see <span class="citation">Turing (<a href="#ref-turing1950">1950</a>)</span>).
In 1949, Warren Weaver proposed that “given that all humans are the same (inspite of speaking a variety of languages), a document in one language could be viewed as having been written in code. Once this code was broken, it would be possible to output the document in another language. From this point of view, German was English in code” (see <span class="citation">Weaver (<a href="#ref-weaver1949">1949</a>)</span>).</p>
<p>In the post World War II era, numerous German documents needed to be translated into the English language. Due to the sheer number of documents, an automatic decryption system was required in order to ensure time efficiency regarding the herculean task, which further accelerated the research concerning NLP. The first teams were comprised of numerous bilingual programmers. The idea behind this was that the knowledge of multiple languages might facilitate the process of creating programs, which could understand languages and their structures, and would subsequently be able to translate texts. While working on the first NLP programs the main difficulty that crystallized related to the complexity and irregularity of many languages (see <span class="citation">Hancox (<a href="#ref-hancox1996">1996</a>)</span>).
Beginning in the 1950s, linguists and machine learning teams congregated and introduced new ideas. During this time period, Georgetown University and International Business Machines Corporation (IBM) published the Georgetown Experiment, the first public demonstration of machine translation, which included the first fully automatic translation, being able to translate more than sixty Russian sentences into English (see <span class="citation">Hutchins (<a href="#ref-hutchins2005">2005</a>)</span>).
Moreover, Noam Chomsky, one of the most important and influential scientists in linguistics, introduced the idea of Generative Grammar, which describes syntactic structures based on rules (see <span class="citation">Chomsky (<a href="#ref-chomsky1957">1957</a>)</span>). The most successful NLP systems that were developed back then were the first chatbot ELIZA (see <span class="citation">Weizenbaum (<a href="#ref-weizenbaum1966">1966</a>)</span>), STUDENT (see <span class="citation">Bobrow (<a href="#ref-bobrow1964">1964</a>)</span>), and SHRDLU, a language program that allowed user interaction with a block world (see <span class="citation">Winograd (<a href="#ref-winograd1972">1972</a>)</span>). As the resources that were available for computing in the past were extremely undeveloped – access to computers was restricted, the machines were still really slow, storage was limited, and there were no suitable higher-level programming languages – the creation of such programs was considerably more difficult. The fact that any progress in this field was attained makes the achievements of those scientists all the more remarkable.</p>
<p>In addition to limited resources, researchers encountered the problem that research regarding the development of NLP software came with high costs: By the mid-1960s, machine translation research expenses amounted to 20 million USD, which were paid by U.S. government funding. Those two obstacles, resource limitations along with high costs, were the main reason for the slow advancement of research in this area. The history of NLP reached its lowest point, when in 1966 the Automatic Language Processing Advisory Committee evaluated the results that were attained through the funding and reported, that “there had been no machine translation of general scientific text, and none is in immediate prospect” (<span class="citation">(“Language and Machines” <a href="#ref-alpac1966">1966</a>)</span>). This report caused U.S. funding to be discontinued, which is the reason why in the following decade the quantity of NLP in scientific literature decreased enormously. Nevertheless, compelling developments such as Augmented Transition Networks, which aid in the analysis of sentence structures, Case Grammar, which facilitates comprehension of linguistic structures by using the link between different components of sentences, and Semantic Representation, which signifies an abstract language in which meanings can be represented, originated in that time (see <span class="citation">Hancox (<a href="#ref-hancox1996">1996</a>)</span>).</p>
<p>In the 1980s, the so-called <em>Statistical Revolution</em> took place. Prior to that, NLP was a primarily “grammar-based approach”, which denotes that systems were created by hand-coding rules and parameters. Via the statistical revolution, the empirical “statistical approach” was introduced (see <span class="citation">Johnson (<a href="#ref-johnson2009">2009</a>)</span>) and consequently “NLP was characterized by the exploitation of data corpora and of (shallow) machine learning, statistical or otherwise, to make use of such data” (see <span class="citation">Deng and Liu (<a href="#ref-deng2018">2018</a>)</span>). This approach has dominated NLP ever since, as the amount of machine-readable data and computational power has continuously expanded. Since simple Machine Learning techniques are often not sufficient for creating NLP applications that can fulfil the requirements of real-life tasks, nowadays most of the methods are based on Deep Learning designs (see <span class="citation">Deng and Liu (<a href="#ref-deng2018">2018</a>)</span>).</p>
</div>
<div id="statistical-background" class="section level2">
<h2><span class="header-section-number">1.2</span> Statistical Background</h2>
<p>Ever since the Statistical Revolution, many challenging aspects could be tackled using statistical approaches and artificial intelligence. Statistics shaped a substantial part of the path of NLP, conjointly with fundamental knowledge of linguistics. The statistical approaches used in this booklet presuppose mathematical foundations such as elementary probability theory and essential information theory. In order facilitate the comprehension of the approaches explained in the later chapters, now some of the basic schemes that lie the foundations to those modern approaches in NLP will be introduced.</p>
<p>Human language courses usually consist of two elementary parts: vocabulary and grammar. The language skills are often measured by the number of words a person knows, while grammar allows using the words and form sentences correctly. Further, NLP systems basically consist of learning and understanding words as well as recognizing the patterns in which they occur.</p>
<p>The first step is characterized by the recognition and comprehension of words. One difficulty arising when trying to understand words is that many words possess multiple meanings. It might be challenging to ascertain which of the meanings is implied. For word sense, disambiguation methods such as bag of words models or Bayesian classification can be used, which inspect the words around the ambiguous word.
Bag of words models consider the dependence of words in a so-called bag, a vector of words that appear in a sentence, so co-occurrences can be learned without understanding grammar.
By applying the Bayesian decision rule, the meaning of the word will be decided by choosing the meaning with the highest conditional probability while minimizing the probability of error.
Another difficulty is that in many languages words exist, which do not (only) have a meaning themselves, but also possess combined meanings in a collocation, an expression consisting of two or more words. One way for NLP systems to implement this is by using basic statistics such as frequency, mean and variance, and hypothesis testing. If two or more words often occur together in a sentence, it may be concluded that these words together possess a special function in this sentence and cannot be explained by the combination of their respective meanings. Mean and variance can help finding the affiliation between words, which do not always appear in the same structure or with the same distance within a phrase, by calculating the mean distance between words in a sentence and the variance of this distance during training, in order to enable correct classification of these words into a collocation in later applications of the model.</p>
<p>The second step is understanding not only the words themselves, but also their meaning given their contexts. As an example, Markov Models can be used for the classification of texts depending on the surrounding context as well as grammatical structure finding. A Markov Model is a sequence classifier which assigns a sequence of classes to a sequence of observations, enabling the classification of texts depending on the class of the previous texts. Markov Models can be used for designing a part-of-speech tagger. Part-of-speech tagging allows assigning words to their part-of-speech in a sentence, allowing for the comprehension of a sentence without requiring complete understanding (see <span class="citation">Manning and Schutze (<a href="#ref-manning2008">2008</a>)</span>).</p>
<p>Combining the understanding of words and grammar, many NLP problems can be solved. In this booklet more advanced methods, of which some are based on the described basic methods, will be introduced.</p>
</div>
<div id="outline-of-the-booklet" class="section level2">
<h2><span class="header-section-number">1.3</span> Outline of the Booklet</h2>
<p>This booklet circumstantiate modern approaches used for natural language processing, such as Deep Learning and Transfer Learning. Moreover, the resources that are available for the training of NLP tasks will be investigated and a use-case where NLP will be applied for generation of natural language will be shown.</p>
<p>For the analysis and comprehension of human language, NLP programs need to extract information from words and sentences. As neural networks and other machine learning algorithms require a numeric input for training, word embeddings, using dense vector representations for words, are applied. Those are usually learned by neural networks with multiple hidden layers, deep neural networks. In order to solve easy tasks, simple structured neural networks can be applied. In order to overcome the limitations of those simple structures, recurrent and convolutional neural networks are utilized. Thereby, recurrent neural networks are used for models that can learn sequences without pre-defined optimal fixed dimensions while convolutional neural networks are applied for sentence classification.
Chapter <a href="Chapter%20Introduction:%20Deep%20Learning%20for%20NLP">2</a> of the booklet gives a short introduction to Deep Learning in NLP. The Foundations and Applications of Modern NLP will be described in chapter <a href="Foundations/Applications%20of%20Modern%20NLP">3</a>. In chapter <a href="Recurrent%20neural%20networks%20and%20their%20applications%20in%20NLP">4</a> and <a href="Convolutional%20neural%20networks%20and%20their%20applications%20in%20NLP">5</a> recurrent neural networks and convolutional neural networks and their applications in NLP will be explained and discussed.</p>
<p>Transfer learning is an alternative to learning models for every task or domain. Here, existing labeled data of related tasks or domains can be used for training a model and applying it onto the task or domain of interest. The advantage of this approach is that there is no need for a long training in the target domain, and that time for training of the model can be saved, while still resulting in a (mostly) better performance. A concept used in transfer learning is Attention, which enables the decoder to attend to the entire input sequence, or Self-Attention, which allows a transformer model to process all input words at once and model the relationships between all words in a sentence, which renders fast modelling of long-range dependencies in a sentence possible.
The concepts of transfer learning will be briefly introduced in chapter <a href="Introduction:%20Transfer%20Learning%20for%20NLP">6</a> of the booklet. Chapter <a href="Transfer%20Learning%20for%20NLP%20I">7</a> will describe transfer learning and LSTMs by presenting the models ELMo, ULMFiT, and GPT. Chapter <a href="Attention%20and%20Self-Attention%20for%20NLP">8</a> will illustrate the concepts of Attention and Self-Attention for NLP in detail. In chapter <a href="Transfer%20Learning%20for%20NLP%20II">9</a>, transfer learning is combined with Self-Attention, introducing the models BERT, GTP2, and XLNet.</p>
<p>For NLP modelling, resources are needed. In order to find the best model for a task, benchmarks can be used. For comparing different models within a benchmark experiment, metrics such as exact match, Fscore, perplexity, or bilingual evaluation understudy, or accuracy, are required.
Chapter <a href="Introduction:%20Resources%20for%20NLP">10</a> of the booklet provides a brief introduction to the resources for NLP and the manner in which they are used. Chapter <a href="Resources%20and%20Benchmarks%20for%20NLP">11</a> will explain the different metrics, give an insight into the benchmark datasets SQuAD, CoQa, GLUE and SuperGLUE, AQuA-Rat, SNLI, and LAMBADA as well as pre-trained models and databases where resources can be found, such as “Papers with Code” and “The Big Bad NLP Database”.</p>
<p>In the <a href="Natural%20Language%20Generation">last</a> chapter of the booklet, the generative NLP process Natural Language Generation, thus the generation of understandable text in a human language, is presented. Therefore, different algorithms will be described and chatbots as well as image captioning will be shown for illustrating the possibilities of application.</p>
<p>This introduction to the various methods in NLP functions as the foundation for the following deliberations. The individual chapters of the booklet will present modern methods in NLP and provide a more detailled discussion of the potential as well as the limitations along with various examples.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bobrow1964">
<p>Bobrow, Daniel G. 1964. “Natural Language Input for a Computer Problem Solving System.”</p>
</div>
<div id="ref-chomsky1957">
<p>Chomsky, Noam. 1957. <em>Syntactic Structures</em>. The Hague: Mouton.</p>
</div>
<div id="ref-deng2018">
<p>Deng, Li, and Yang Liu. 2018. <em>Deep Learning in Natural Language Processing</em>. Springer Nature.</p>
</div>
<div id="ref-hancox1996">
<p>Hancox, Peter. 1996. <em>Natural Language Processing</em>. <a href="https://www.cs.bham.ac.uk/~pjh/sem1a5/pt1/pt1_history.html" class="uri">https://www.cs.bham.ac.uk/~pjh/sem1a5/pt1/pt1_history.html</a>.</p>
</div>
<div id="ref-hutchins2005">
<p>Hutchins, John. 2005. “The First Public Demonstration of Machine Translation: The Georgetown-Ibm System, 7th January 1954.” <em>Noviembre de</em>.</p>
</div>
<div id="ref-johnson2009">
<p>Johnson, Mark. 2009. “How the Statistical Revolution Changes (Computational) Linguistics.” <em>Proceedings of the EACL 2009 Workshop on the Interaction Between Linguistics and Computational Linguistics Virtuous, Vicious or Vacuous? - ILCL 09</em>, March. <a href="https://doi.org/10.3115/1642038.1642041" class="uri">https://doi.org/10.3115/1642038.1642041</a>.</p>
</div>
<div id="ref-alpac1966">
<p>“Language and Machines.” 1966. <em>National Academy of Sciences National Research Council</em>. <a href="https://doi.org/10.17226/9547" class="uri">https://doi.org/10.17226/9547</a>.</p>
</div>
<div id="ref-manning2008">
<p>Manning, Christopher D., and Hinrich Schutze. 2008. <em>Foundations of Statistical Natural Language Processing</em>. MIT.</p>
</div>
<div id="ref-mccorduck2004">
<p>Mccorduck, Pamela, and Cli Cfe. 2004. “Machines Who Think.” <em>A K Peters/CRC Press</em>, March. <a href="https://doi.org/10.1201/9780429258985" class="uri">https://doi.org/10.1201/9780429258985</a>.</p>
</div>
<div id="ref-turing1950">
<p>Turing, Alan M. 1950. “Computing Machinery and Intelligence.” <em>Mind</em> LIX (236): 433–60. <a href="https://doi.org/10.1093/mind/lix.236.433" class="uri">https://doi.org/10.1093/mind/lix.236.433</a>.</p>
</div>
<div id="ref-turing1948">
<p>Turing, Alan Mathison. 1948. “Intelligent Machinery.” NPL. Mathematics Division.</p>
</div>
<div id="ref-turing1937">
<p>Turing, A. M. 1937. “On Computable Numbers, with an Application to the Entscheidungsproblem.” <em>Proceedings of the London Mathematical Society</em> s2-42 (1): 230–65. <a href="https://doi.org/10.1112/plms/s2-42.1.230" class="uri">https://doi.org/10.1112/plms/s2-42.1.230</a>.</p>
</div>
<div id="ref-weaver1949">
<p>Weaver, Warren. 1949. “The Mathematics of Communication.” <em>Scientific American</em> 181 (1): 11–15. <a href="https://doi.org/10.1038/scientificamerican0749-11" class="uri">https://doi.org/10.1038/scientificamerican0749-11</a>.</p>
</div>
<div id="ref-weizenbaum1966">
<p>Weizenbaum, Joseph. 1966. “ELIZA-a Computer Program for the Study of Natural Language Communication Between Man and Machine.” <em>Communications of the ACM</em> 9 (1). ACM New York, NY, USA: 36–45.</p>
</div>
<div id="ref-winograd1972">
<p>Winograd, Terry. 1972. “SHRDLU: A System for Dialog.” CUMINCAD.</p>
</div>
<div id="ref-yeung2020">
<p>Yeung, Joshua. 2020. “Three Major Fields of Artificial Intelligence and Their Industrial Applications.” <em>Medium</em>. Towards Data Science. <a href="https://towardsdatascience.com/three-major-fields-of-artificial-intelligence-and-their-industrial-applications-8f67bf0c2b46" class="uri">https://towardsdatascience.com/three-major-fields-of-artificial-intelligence-and-their-industrial-applications-8f67bf0c2b46</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="foreword.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-deep-learning-for-nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/00-introduction.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
