<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-06-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="foundationsapplications-of-modern-nlp.html"/>
<link rel="next" href="convolutional-neural-networks-and-their-applications-in-nlp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter-1.html"><a href="chapter-1.html"><i class="fa fa-check"></i><b>2</b> Chapter 1</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter-1.html"><a href="chapter-1.html#lorem-ipsum"><i class="fa fa-check"></i><b>2.1</b> Lorem Ipsum</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-1.html"><a href="chapter-1.html#using-figures"><i class="fa fa-check"></i><b>2.2</b> Using Figures</a></li>
<li class="chapter" data-level="2.3" data-path="chapter-1.html"><a href="chapter-1.html#using-tex"><i class="fa fa-check"></i><b>2.3</b> Using Tex</a></li>
<li class="chapter" data-level="2.4" data-path="chapter-1.html"><a href="chapter-1.html#using-stored-results"><i class="fa fa-check"></i><b>2.4</b> Using Stored Results</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>3</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>3.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>3.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>3.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>4</b> Foundations/Applications of Modern NLP</a></li>
<li class="chapter" data-level="5" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#rnns"><i class="fa fa-check"></i><b>5.1</b> RNNs</a><ul>
<li class="chapter" data-level="5.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure"><i class="fa fa-check"></i><b>5.1.1</b> Structure</a></li>
<li class="chapter" data-level="5.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation-and-drawbacks"><i class="fa fa-check"></i><b>5.1.2</b> Backpropagation and Drawbacks</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>5.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="5.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>5.2.1</b> LSTM</a></li>
<li class="chapter" data-level="5.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>5.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#versions"><i class="fa fa-check"></i><b>5.3</b> Versions</a><ul>
<li class="chapter" data-level="5.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#bidirectional-and-deep-rnns"><i class="fa fa-check"></i><b>5.3.1</b> Bidirectional and Deep RNNs</a></li>
<li class="chapter" data-level="5.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#applications"><i class="fa fa-check"></i><b>5.3.2</b> Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>6</b> Convolutional neural networks and their applications in NLP</a></li>
<li class="chapter" data-level="7" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>7</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>7.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>7.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="7.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>7.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>8</b> Transfer Learning for NLP I</a></li>
<li class="chapter" data-level="9" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>9</b> Attention and Self-Attention for NLP</a></li>
<li class="chapter" data-level="10" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>10</b> Transfer Learning for NLP II</a></li>
<li class="chapter" data-level="11" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="12" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Resources and Benchmarks for NLP</a></li>
<li class="chapter" data-level="13" data-path="software-for-nlp-the-huggingface-transformers-module.html"><a href="software-for-nlp-the-huggingface-transformers-module.html"><i class="fa fa-check"></i><b>13</b> Software for NLP: The huggingface transformers module</a></li>
<li class="chapter" data-level="14" data-path="use-bases-for-nlp.html"><a href="use-bases-for-nlp.html"><i class="fa fa-check"></i><b>14</b> Use-Bases for NLP</a></li>
<li class="chapter" data-level="15" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>15</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="15.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#introduction-1"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition"><i class="fa fa-check"></i><b>15.2</b> Definition</a></li>
<li class="chapter" data-level="15.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#domains-of-nlg-systems"><i class="fa fa-check"></i><b>15.3</b> Domains of NLG Systems</a></li>
<li class="chapter" data-level="15.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#commercial-activity"><i class="fa fa-check"></i><b>15.4</b> Commercial Activity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="the-architectures.html"><a href="the-architectures.html"><i class="fa fa-check"></i><b>16</b> The Architectures</a><ul>
<li class="chapter" data-level="16.1" data-path="the-architectures.html"><a href="the-architectures.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>16.1</b> Encoder-Decoder Architecture</a><ul>
<li class="chapter" data-level="16.1.1" data-path="the-architectures.html"><a href="the-architectures.html#encoder"><i class="fa fa-check"></i><b>16.1.1</b> Encoder :</a></li>
<li class="chapter" data-level="16.1.2" data-path="the-architectures.html"><a href="the-architectures.html#decoder"><i class="fa fa-check"></i><b>16.1.2</b> Decoder :</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="the-architectures.html"><a href="the-architectures.html#attention-architecture"><i class="fa fa-check"></i><b>16.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="16.3" data-path="the-architectures.html"><a href="the-architectures.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>16.3</b> Decoding Algorithm at Inference</a><ul>
<li class="chapter" data-level="16.3.1" data-path="the-architectures.html"><a href="the-architectures.html#beam-search"><i class="fa fa-check"></i><b>16.3.1</b> Beam Search</a></li>
<li class="chapter" data-level="16.3.2" data-path="the-architectures.html"><a href="the-architectures.html#pure-sampling-decoder"><i class="fa fa-check"></i><b>16.3.2</b> Pure Sampling Decoder</a></li>
<li class="chapter" data-level="16.3.3" data-path="the-architectures.html"><a href="the-architectures.html#k-sampling-decoder"><i class="fa fa-check"></i><b>16.3.3</b> K-sampling Decoder</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="the-architectures.html"><a href="the-architectures.html#memory-networks"><i class="fa fa-check"></i><b>16.4</b> Memory Networks</a></li>
<li class="chapter" data-level="16.5" data-path="the-architectures.html"><a href="the-architectures.html#language-models"><i class="fa fa-check"></i><b>16.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="question-answer-systems.html"><a href="question-answer-systems.html"><i class="fa fa-check"></i><b>17</b> Question-Answer Systems</a><ul>
<li class="chapter" data-level="17.1" data-path="question-answer-systems.html"><a href="question-answer-systems.html#datasets"><i class="fa fa-check"></i><b>17.1</b> Datasets</a></li>
<li class="chapter" data-level="17.2" data-path="question-answer-systems.html"><a href="question-answer-systems.html#types"><i class="fa fa-check"></i><b>17.2</b> Types</a></li>
<li class="chapter" data-level="17.3" data-path="question-answer-systems.html"><a href="question-answer-systems.html#architectures"><i class="fa fa-check"></i><b>17.3</b> Architectures</a></li>
<li class="chapter" data-level="17.4" data-path="question-answer-systems.html"><a href="question-answer-systems.html#evaluation-metrics"><i class="fa fa-check"></i><b>17.4</b> Evaluation Metrics</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="dialog-systems.html"><a href="dialog-systems.html"><i class="fa fa-check"></i><b>18</b> Dialog Systems</a><ul>
<li class="chapter" data-level="18.1" data-path="dialog-systems.html"><a href="dialog-systems.html#types-1"><i class="fa fa-check"></i><b>18.1</b> Types</a></li>
<li class="chapter" data-level="18.2" data-path="dialog-systems.html"><a href="dialog-systems.html#architectures-1"><i class="fa fa-check"></i><b>18.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>19</b> Conclusion</a></li>
<li class="chapter" data-level="20" data-path="use-case-ii.html"><a href="use-case-ii.html"><i class="fa fa-check"></i><b>20</b> Use-Case II</a></li>
<li class="chapter" data-level="21" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>21</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recurrent-neural-networks-and-their-applications-in-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Recurrent neural networks and their applications in NLP</h1>
<p><em>Author: Marianna Plesiak</em></p>
<p><em>Supervisor: Christian Heumann</em></p>
<div id="rnns" class="section level2">
<h2><span class="header-section-number">5.1</span> RNNs</h2>
<div id="structure" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Structure</h3>
<p>Recurrent neural networks allow to relax the condition of non-cyclical connections in the classical feedforward neural networks which were described in the previous chapter. This means, while simple MLP (multilayer perceptrons) can only map from input to output vectors, RNNs allow the entire history of previous inputs to influence the network output. (@ Graves 2012)</p>
<p>The repetitive structure of RNNs can be visualised with help of an <strong>unfolded</strong> computational graph(see @ figure 1).</p>
<div class="figure" style="text-align: center"><span id="fig:pressure"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/02_unfolded_graph.png" alt="Unfolded computatinal graph of a RNN." width="80%" />
<p class="caption">
FIGURE 5.1: Unfolded computatinal graph of a RNN.
</p>
</div>
<p>Each node is associated with a network layer at a particular time instance. Inputs <span class="math inline">\(x^{(t)}\)</span> must be encoded as numeric vectors, for instance word embeddings or one-hot encoded vectors, see previous chapter. Reccurently connected vectors <span class="math inline">\(h\)</span> are called hidden states and represent the outputs of the hidden layers. At time <span class="math inline">\(t\)</span>, a hidden state <span class="math inline">\(h^{(t)}\)</span> combines information from the previous hidden state <span class="math inline">\(h^{(t-1)}\)</span> as well as the new input <span class="math inline">\(x^{(t)}\)</span> and passes it through to the next hidden layer. Obviously, such an architecture requires the initialization of <span class="math inline">\(h^{(0)}\)</span> since there is no memory at the very beginning of the sequence processing. Given the hidden sequences, output vectors <span class="math inline">\(\hat{y}^{(t)}\)</span> are used to build the predictive distribution <span class="math inline">\(Pr(x^{(t+1)}|y^{(t)})\)</span> for the next input. Since the predictions are created at each time instance <span class="math inline">\(t\)</span>, the total output has a shape [time_steps, output_features]. However in some cases this is not needed, for example in sentiment analysis the last output of the loop is sufficient because it contains the entire information about the sequence. (@ Chollet) (@ Graves 2012)</p>
<p>The unfolded recurrence can be formalized as following:</p>

<p>After <span class="math inline">\(t\)</span> steps, the function <span class="math inline">\(g^{(t)}\)</span> takes into account the whole sequence <span class="math inline">\((x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)})\)</span> and produces the hidden state <span class="math inline">\(h^{(t)}\)</span>. Because of its cyclical structure, <span class="math inline">\(g^{(t)}\)</span> can be factorized into the repeated application of a same function <span class="math inline">\(f\)</span>. This function can be considered a universal model which is shared across all time steps and is generalized for all sequence lengths. This is called parameter sharing and is illustrated in the unfolded computational graph as a reuse of the same matrices <span class="math inline">\(U\)</span>, <span class="math inline">\(W\)</span> and <span class="math inline">\(V\)</span> through the entire network. (@ Goodfellow book)</p>
<p>Update equations:</p>
</div>
<div id="backpropagation-and-drawbacks" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Backpropagation and Drawbacks</h3>
</div>
</div>
<div id="gated-rnns" class="section level2">
<h2><span class="header-section-number">5.2</span> Gated RNNs</h2>
<div id="lstm" class="section level3">
<h3><span class="header-section-number">5.2.1</span> LSTM</h3>
</div>
<div id="gru" class="section level3">
<h3><span class="header-section-number">5.2.2</span> GRU</h3>
</div>
</div>
<div id="versions" class="section level2">
<h2><span class="header-section-number">5.3</span> Versions</h2>
<div id="bidirectional-and-deep-rnns" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Bidirectional and Deep RNNs</h3>
</div>
<div id="applications" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Applications</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="foundationsapplications-of-modern-nlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="convolutional-neural-networks-and-their-applications-in-nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/01-02-rnns-and-their-applications-in-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
