<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-09-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="foundationsapplications-of-modern-nlp.html"/>
<link rel="next" href="convolutional-neural-networks-and-their-applications-in-nlp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-and-development"><i class="fa fa-check"></i><b>1.1</b> History and development</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#statistical-background"><i class="fa fa-check"></i><b>1.2</b> Statistical Background</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.3</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning-and-system-design-choices"><i class="fa fa-check"></i><b>3.3</b> Hyperparameter Tuning and System Design Choices</a></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.4</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#outlook-and-resources"><i class="fa fa-check"></i><b>3.5</b> Outlook and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extensions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extensions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.2</b> Encoder-Decoder Architecture</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>6</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>6.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>6.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>6.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>7</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="7.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#outline"><i class="fa fa-check"></i><b>7.1</b> Outline</a></li>
<li class="chapter" data-level="7.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>7.2</b> Sequential inductive transfer learning</a><ul>
<li class="chapter" data-level="7.2.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#types-of-transfer-learning"><i class="fa fa-check"></i><b>7.2.1</b> Types of transfer learning</a></li>
<li class="chapter" data-level="7.2.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#feature-extraction-vs.fine-tuning"><i class="fa fa-check"></i><b>7.2.2</b> Feature Extraction vs. Fine-tuning</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#models"><i class="fa fa-check"></i><b>7.3</b> Models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>7.3.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="7.3.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit"><i class="fa fa-check"></i><b>7.3.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="7.3.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>7.3.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>8</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="8.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>8.1</b> Attention</a><ul>
<li class="chapter" data-level="8.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>8.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>8.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="8.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#computational-difference-between-luong--and-bahdanau-attention"><i class="fa fa-check"></i><b>8.1.3</b> Computational Difference between Luong- and Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.4" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>8.1.4</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>8.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="8.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#the-transformer"><i class="fa fa-check"></i><b>8.2.1</b> The Transformer</a></li>
<li class="chapter" data-level="8.2.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers-as-rnns"><i class="fa fa-check"></i><b>8.2.2</b> Transformers as RNNs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>9</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="9.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>9.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>9.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="9.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>9.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="9.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>9.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="9.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>9.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="9.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-prediction"><i class="fa fa-check"></i><b>9.1.5</b> Next-sentence Prediction</a></li>
<li class="chapter" data-level="9.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>9.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>9.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>9.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>9.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="9.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>9.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="9.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>9.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="9.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>9.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="9.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>9.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="9.2.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>9.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>9.3</b> XLNet</a><ul>
<li class="chapter" data-level="9.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>9.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="9.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>9.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="9.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>9.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="9.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>9.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="9.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>9.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="9.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>9.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="9.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>9.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>9.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>10</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="11" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="11.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>11.1</b> Metrics</a></li>
<li class="chapter" data-level="11.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>11.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="11.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>11.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="11.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>11.2.2</b> CoQA</a></li>
<li class="chapter" data-level="11.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>11.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="11.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>11.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="11.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>11.2.5</b> SNLI</a></li>
<li class="chapter" data-level="11.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#overview"><i class="fa fa-check"></i><b>11.2.6</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-trained-models"><i class="fa fa-check"></i><b>11.3</b> Pre-Trained Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#bert"><i class="fa fa-check"></i><b>11.3.1</b> BERT</a></li>
<li class="chapter" data-level="11.3.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#openai-gpt-3"><i class="fa fa-check"></i><b>11.3.2</b> OpenAI GPT-3</a></li>
<li class="chapter" data-level="11.3.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#google-5t"><i class="fa fa-check"></i><b>11.3.3</b> Google 5T</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>11.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-cases-for-nlp.html"><a href="use-cases-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Use-Cases for NLP</a></li>
<li class="chapter" data-level="13" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>13</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="13.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>13.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="13.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>13.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="13.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>13.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="13.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>13.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="13.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>13.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="13.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>13.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="13.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>13.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>13.4</b> Dialog Systems</a><ul>
<li class="chapter" data-level="13.4.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>13.4.1</b> Types</a></li>
<li class="chapter" data-level="13.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>13.4.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#image-captioning-system"><i class="fa fa-check"></i><b>13.5</b> Image Captioning System</a><ul>
<li class="chapter" data-level="13.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#experiments"><i class="fa fa-check"></i><b>13.5.1</b> Experiments</a></li>
<li class="chapter" data-level="13.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#implementation"><i class="fa fa-check"></i><b>13.5.2</b> Implementation</a></li>
<li class="chapter" data-level="13.5.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#results"><i class="fa fa-check"></i><b>13.5.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#conclusion"><i class="fa fa-check"></i><b>13.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>14</b> Epilogue</a><ul>
<li class="chapter" data-level="14.1" data-path="epilogue.html"><a href="epilogue.html#new-influentioal-architectures"><i class="fa fa-check"></i><b>14.1</b> New influentioal architectures</a></li>
<li class="chapter" data-level="14.2" data-path="epilogue.html"><a href="epilogue.html#improvements-of-the-selfattention-mechanism"><i class="fa fa-check"></i><b>14.2</b> Improvements of the SelfAttention mechanism</a></li>
<li class="chapter" data-level="14.3" data-path="epilogue.html"><a href="epilogue.html#evaluation-and-interpretability"><i class="fa fa-check"></i><b>14.3</b> Evaluation and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recurrent-neural-networks-and-their-applications-in-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Recurrent neural networks and their applications in NLP</h1>
<p><em>Author: Marianna Plesiak</em></p>
<p><em>Supervisor: Prof. Dr. Christian Heumann</em></p>
<div id="structure-and-training-of-simple-rnns" class="section level2">
<h2><span class="header-section-number">4.1</span> Structure and Training of Simple RNNs</h2>
<p><strong>R</strong>ecurrent <strong>n</strong>eural <strong>n</strong>etworks (<strong>RNNs</strong>) enable to relax the condition of non-cyclical connections in the classical feedforward neural networks which were described in the previous chapter. This means, while simple multilayer perceptrons can only map from input to output vectors, RNNs allow the entire history of previous inputs to influence the network output. <span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span></p>
<p>The first part of this chapter provides the structure definition of RNNs, presents the principles of their training and explains problems with backpropagation. The second part covers gated units, an improved way to calculate hidden states. The third part gives an overview of some extended versions of RNNs and their applications in NLP.</p>
<div id="network-structure-and-forwardpropagation" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Network Structure and Forwardpropagation</h3>
<p>An <strong>unfolded computational graph</strong> can visualize the repetitive structure of RNNs:</p>
<div class="figure" style="text-align: center"><span id="fig:01-02-unfold"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/1_unfolded_graph.png" alt="Unfolded computatinal graph of an RNN. Source: Own figure." width="100%" />
<p class="caption">
FIGURE 4.1: Unfolded computatinal graph of an RNN. Source: Own figure.
</p>
</div>
<p>Each node is associated with a network layer at a particular time instance. Inputs <span class="math inline">\(x^{(t)}\)</span> must be encoded as numeric vectors, for instance word embeddings or one-hot encoded vectors, see the previous chapter. Recurrently connected vectors <span class="math inline">\(h\)</span> are called <strong>hidden states</strong> and represent the outputs of the hidden layer. At time <span class="math inline">\(t\)</span>, a hidden state <span class="math inline">\(h^{(t)}\)</span> combines information from the previous hidden state <span class="math inline">\(h^{(t-1)}\)</span> as well as the new input <span class="math inline">\(x^{(t)}\)</span> and transmits it to the next hidden state. Obviously, such an architecture requires the initialization of <span class="math inline">\(h^{(0)}\)</span> since there is no memory at the very beginning of the sequence processing. Given the hidden sequences, output vectors <span class="math inline">\(\hat{y}^{(t)}\)</span> are used to build the predictive distribution <span class="math inline">\(Pr(x^{(t+1)}|y^{(t)})\)</span> for the next input <span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span>. Since the predictions are created at each time instance <span class="math inline">\(t\)</span>, the total output has a shape <span class="math inline">\([\#\ time\_steps, \#\ output\_features]\)</span>. However, in some cases the whole history of output features is not necessary. For example, in sentiment analysis the last output of the loop is sufficient because it contains the entire information about the sequence. <span class="citation">(Chollet <a href="#ref-chollet2018deep">2018</a>)</span></p>
<p>The unfolded recurrence can be formalized as following:</p>

<p>After <span class="math inline">\(t\)</span> steps, the function <span class="math inline">\(g^{(t)}\)</span> takes into account the whole sequence <span class="math inline">\((x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)})\)</span> and produces the hidden state <span class="math inline">\(h^{(t)}\)</span>. Because of its cyclical structure, <span class="math inline">\(g^{(t)}\)</span> can be factorized into the repeated application of the same function <span class="math inline">\(f\)</span>. This function can be considered to be a universal model with parameters <span class="math inline">\(\theta\)</span> which are shared across all time steps and generalized for all sequence lengths. This concept is called parameter sharing and is illustrated in the unfolded computational graph as a reuse of the same matrices <span class="math inline">\(W_{xh}\)</span>, <span class="math inline">\(W_{hh}\)</span> and <span class="math inline">\(W_{hy}\)</span> through the entire network. <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016deep">2016</a>)</span></p>
<p>Considering a recurrent neural network with one hidden layer that is used to predict words or characters, the output should be discrete and the model maps input sequence to output sequence of the same length. Then, the forward propagation is computed by iterating the following equations:</p>

<p>where, according to <span class="citation">Graves (<a href="#ref-graves2013generating">2013</a>)</span>, the parameters and functions denote the following:</p>
<ul>
<li><span class="math inline">\(x^{(t)}\)</span>, <span class="math inline">\(h^{(t)}\)</span> and <span class="math inline">\(y^{(t)}\)</span>: input, hidden state and output at time step <span class="math inline">\(t\)</span> respectively;</li>
<li><span class="math inline">\(\mathcal{f}\)</span>: activation function of the hidden layer. Usually it is a saturating nonlinear function such as a sigmoid activation function ( <span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-sutskever2014sequence">2014</a>)</span> and <span class="citation">Mikolov et al. (<a href="#ref-mikolov2010recurrent">2010</a>)</span>);<br />
</li>
<li><span class="math inline">\(W_{hh}\)</span>: weight matrix connecting recurrent connections between hidden states;<br />
</li>
<li><span class="math inline">\(W_{xh}\)</span>: weight matrix connecting inputs to a hidden layer;<br />
</li>
<li><span class="math inline">\(W_{hy}\)</span>: weight matrix connecting hidden states to outputs;<br />
</li>
<li><span class="math inline">\(\mathcal{s}\)</span>: output layer function. If the model is used to predict words, the softmax function is usually chosen as it returns valid probabilities over the possible outputs <span class="citation">(Mikolov et al. <a href="#ref-mikolov2010recurrent">2010</a>)</span>;<br />
</li>
<li><span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>: input and output bias vectors.</li>
</ul>
<p>Since inputs <span class="math inline">\(x^{(t)}\)</span> are usually encoded as one-hot-vectors, the dimension of a vector representing one word corresponds to the size of vocabulary. The size of a hidden layer must reflect the size of training data. The model training requires initialization of the initial state <span class="math inline">\(h^{(0)}\)</span> as well as the weight matrices, which are usually set to small random values <span class="citation">(Mikolov et al. <a href="#ref-mikolov2010recurrent">2010</a>)</span>. Since the model is used to compute the predictive distributions <span class="math inline">\(Pr(x^{(t+1)}|y^{(t)})\)</span> at each time instance <span class="math inline">\(t\)</span>, the network distribution is denoted as following:</p>

<p>and the total loss <span class="math inline">\(\mathcal{L(x)}\)</span>, which must be minimized during the training, is simply the sum of losses over all time steps denoted as the negative log-likelihood of <span class="math inline">\(Pr(x)\)</span>:</p>

<p><span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span></p>
</div>
<div id="backpropagation" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Backpropagation</h3>
<p>To train the model, one must calculate the gradients for the three weight matrices <span class="math inline">\(W_{xh}\)</span>, <span class="math inline">\(W_{hh}\)</span> and <span class="math inline">\(W_{hy}\)</span>. The algorithm differs from a regular backpropagation because a chain rule must be applied recursively, and the gradients are summed up through the network <span class="citation">(Boden <a href="#ref-boden2002guide">2002</a>)</span>. Using the notation <span class="math inline">\(\mathcal{L^{(t)}}\)</span> as the output at time <span class="math inline">\(t\)</span>, one must first estimate single losses at each time step and then sum them up in order to obtain total loss <span class="math inline">\(\mathcal{L(x)}\)</span>.</p>
<p><span class="citation">Chen (<a href="#ref-chen2016gentle">2016</a>)</span> provides a nice guide to backpropagation for a simple RNN in the meaning of equations <a href="#eq:input-to-hidden">(<strong>??</strong>)</a> and <a href="#eq:hidden-to-output">(<strong>??</strong>)</a>. According to him, the gradient w.r.t. <span class="math inline">\(W_{hy}\)</span> for a single time step <span class="math inline">\(t\)</span> is calculated as follows:</p>

<p>Since <span class="math inline">\(W_{hy}\)</span> is shared across all time sequence, the total loss w.r.t. the weight matrix connecting hidden states to outputs is simply a sum of single losses:</p>

<p>Similarly, derivation of the gradient w.r.t. <span class="math inline">\(W_{hh}\)</span> for a single time step is obtained as follows:</p>

<p>However, the last part <span class="math inline">\(h^{(t)}\)</span> also depends on <span class="math inline">\(h^{(t-1)}\)</span> and the gradient can be rewritten according to the <strong>B</strong>ack<strong>p</strong>ropagation <strong>T</strong>hrough <strong>Time</strong> algorithm (<strong>BPTT</strong>) starting from <span class="math inline">\(t\)</span> and going back to the initial hidden state at time step <span class="math inline">\(k=0\)</span>:</p>

<p>Single gradients are again aggregated to yield the overall loss w.r.t <span class="math inline">\(W_{hh}\)</span>:</p>

<p>The derivation of gradients w.r.t. <span class="math inline">\(W_{xh}\)</span> is similar to those w.r.t <span class="math inline">\(W_{hh}\)</span> because both <span class="math inline">\(h^{(t-1)}\)</span> and <span class="math inline">\(x^{(t)}\)</span> contribute to <span class="math inline">\(h^{(t)}\)</span> at time step <span class="math inline">\(t\)</span>. The derivative w.r.t. <span class="math inline">\(W_{xh}\)</span> for the whole sequence is then obtained by summing up all contributions from <span class="math inline">\(t\)</span> to <span class="math inline">\(0\)</span> via backpropagation for all time steps:</p>

<p>The derivatives w.r.t. the bias vectors <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are calculated based on the same principles and thus are not shown here explicitly.</p>
</div>
<div id="vanishing-and-exploding-gradients" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Vanishing and Exploding Gradients</h3>
<p>In order to better understand the mathematical challenges of BPTT, one should consider equation <a href="#eq:rnn-back-hh-one-one">(<strong>??</strong>)</a>, and in particular the factor <span class="math inline">\(\frac{\partial h^{(t)}}{\partial h^{(k)}}\)</span>. <span class="citation">Pascanu, Mikolov, and Bengio (<a href="#ref-pascanu2013difficulty">2013</a>)</span> go into detail and show that it is a chain rule in itself and can be rewritten as a product <span class="math inline">\(\frac{\partial h^{(t)}}{\partial h^{(t-1)}} \frac{\partial h^{(t-1)}}{\partial h^{(t-2)}} \ldots \frac{\partial h^{(2)}}{\partial h^{(1)}}\)</span>. Since one computes the derivative of a vector w.r.t another vector, the result will be a product of <span class="math inline">\(t-k\)</span> Jacobian matrices whose elements are the pointwise derivatives:</p>

<p>Thus, with small values in <span class="math inline">\(W_{hh}\)</span> and many matrix multiplications the norm of the gradient shrinks to zero exponentially fast with <span class="math inline">\(t-k\)</span> which results in a vanishing gradient problem and the loss of long term contributions. Exploding gradients refer to the opposite behaviour when the norm of the gradient increases largely and leads to a crash of the model. <span class="citation">Pascanu, Mikolov, and Bengio (<a href="#ref-pascanu2013difficulty">2013</a>)</span> give an overview of techniques for dealing with the exploding and vanishing gradients. Among other solutions they mention proper initialisation of weight matrices, sampling <span class="math inline">\(W_{hh}\)</span> and <span class="math inline">\(W_{xh}\)</span> instead of learning them, rescaling or clipping the gradient’s components (putting a maximum limit on it) or using L1 or L2 penalties on the recurrent weights. Even more popular models are gated RNNs which explicitly address the vanishing gradients problem and will be explained in the next subchapter.</p>
</div>
</div>
<div id="gated-rnns" class="section level2">
<h2><span class="header-section-number">4.2</span> Gated RNNs</h2>
<p>Main feature of gated RNNs is the ability to store long-term memory for a long time and at the same time to account for new inputs as effectively as possible. In modern NLP, two types of gated RNNs are used widely: Long Short-Term Memory networks and Gated Recurrent Units.</p>
<div id="lstm" class="section level3">
<h3><span class="header-section-number">4.2.1</span> LSTM</h3>
<p><strong>L</strong>ong <strong>S</strong>hort-<strong>T</strong>erm <strong>Memory</strong> (<strong>LSTM</strong>) networks were introduced by <span class="citation">Hochreiter and Schmidhuber (<a href="#ref-hochreiter1997long">1997</a>)</span> with the purpose of dealing with problems of long term dependencies. Figure <a href="recurrent-neural-networks-and-their-applications-in-nlp.html#fig:01-02-lstm-gru">4.2</a> illustrates the complex architecture of LSTM hidden state. Instead of a simple hidden unit that combines inputs and previous hidden states linearly and outputs their nonlinear transformation to the next step, hidden units are now extended by special input, forget and output gates that help to control the flow of information. Such more complex units are called memory cells, and the following equations show how a LSTM uses the gating mechanism to calculate the hidden state within a memory cell:</p>

<p>First, the forget gate <span class="math inline">\(f^{(t)}\)</span> decides which values of the previous output <span class="math inline">\(h^{(t-1)}\)</span> to forget. The next step involves deciding which information will be stored in the internal cell state <span class="math inline">\(c^{(t)}\)</span>. This step consists of the following two parts: 1) the old cell state <span class="math inline">\(c^{(t-1)}\)</span> is multiplied by <span class="math inline">\(f^{(t)}\)</span> in order to forget information; 2) new candidates are calculated in <span class="math inline">\(g^{(t)}\)</span> and multiplied by the input gate <span class="math inline">\(i^{(t)}\)</span> in order to add new information. Finally, the output <span class="math inline">\(h^{(t)}\)</span> is produced with help of the output gate <span class="math inline">\(o^{(t)}\)</span> and applying a <span class="math inline">\(tanh\)</span> function to the cell state in order to only output selected values. (<span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow2016deep">2016</a>)</span>, <span class="citation">Graves (<a href="#ref-graves2013generating">2013</a>)</span>)</p>
</div>
<div id="gru" class="section level3">
<h3><span class="header-section-number">4.2.2</span> GRU</h3>
<p>In 2014, <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span> introduced <strong>G</strong>ated <strong>R</strong>ecurrent <strong>U</strong>nits (<strong>GRU</strong>) whose structure is simpler than that of LSTM because they have only two gates, namely reset and update gate.
The hidden state is thus calculated as follows:</p>

<p>First, the reset gate <span class="math inline">\(r^{(t)}\)</span> decides how much the previous hidden state <span class="math inline">\(h^{(t-1)}\)</span> is ignored, so if <span class="math inline">\(r^{(t)}\)</span> is close to <span class="math inline">\(0\)</span>, the hidden state is forced to drop past irrelevant information and reset with current input <span class="math inline">\(x^{(t)}\)</span>. A candidate hidden state <span class="math inline">\(\tilde{h}^{(t)}\)</span> is then obtained in the following three steps: 1) weight matrix <span class="math inline">\(W_{xh}\)</span> is multiplied by current input <span class="math inline">\(x^{(t)}\)</span>; 2) weight matrix <span class="math inline">\(W_{hh}\)</span> is multiplied by the element-wise product (denoted as <span class="math inline">\(\odot\)</span>) of reset gate <span class="math inline">\(r^{(t)}\)</span> and previous hidden state <span class="math inline">\(h^{(t)}\)</span>; 3) both products are added and a <span class="math inline">\(tanh\)</span> function is applied in order to output the candidate values for a hidden state.
On the other hand, the update gate <span class="math inline">\(z^{(t)}\)</span> decides how much of the previous information stored in <span class="math inline">\(h^{(t-1)}\)</span> will carry over to <span class="math inline">\(h^{(t)}\)</span> and how much the new hidden state must be updated with the candidate <span class="math inline">\(\tilde{h}^{(t)}\)</span>. Thus, the update gate combines the functionality of LSTM forget and input gates and helps to capture long-term memory. <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span> used the GRU in the machine translation task and showed that the novel hidden units can achieve good results in term of BLEU score (see chapter <a href="./resources-and-benchmarks-for-nlp.html">11</a> for definition) while being computationally cheaper than LSTM.</p>
<p>Figure <a href="recurrent-neural-networks-and-their-applications-in-nlp.html#fig:01-02-lstm-gru">4.2</a> visualizes the architecture of hidden states of gated RNNs mentioned above and illustrates their differences:</p>
<div class="figure" style="text-align: center"><span id="fig:01-02-lstm-gru"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/2_lstm_vs_gru.png" alt="Structure of a hidden unit. LSTM on the right and GRU on the left. Source: Own figure inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/" width="100%" />
<p class="caption">
FIGURE 4.2: Structure of a hidden unit. LSTM on the right and GRU on the left. Source: Own figure inspired by <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
</p>
</div>
</div>
</div>
<div id="extensions-of-simple-rnns" class="section level2">
<h2><span class="header-section-number">4.3</span> Extensions of Simple RNNs</h2>
<p>In most challenging NLP tasks, such as machine translation or text generation, one-layer RNNs even with gated hidden states cannot often deliver successful results. Therefore more complicated architectures are necessary. Deep RNNs with several hidden layers may improve model performance significantly. On the other hand, completely new designs such as Encoder-Decoder architectures are most effective when it comes to mapping sequences of an arbitrary length to a sequence of another arbitrary length.</p>
<div id="deep-rnns" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Deep RNNs</h3>
<p>The figure <a href="recurrent-neural-networks-and-their-applications-in-nlp.html#fig:01-02-unfold">4.1</a> shows that each layer is associated with one parameter matrix so that the model is considered shallow. This structure can be extended to a deep RNN, although the depth of an RNN is not a trivial concept since its units are already expressed as a nonlinear function of multiple previous units.</p>
<div id="stacked-rnns" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> Stacked RNNs</h4>
<p>In their paper <span class="citation">Graves, Mohamed, and Hinton (<a href="#ref-graves2013speech">2013</a>)</span> use a deep RNN for speech recognition where the depth is defined as stacking <span class="math inline">\(N\)</span> hidden layers on top of each other with <span class="math inline">\(N&gt;1\)</span>. In this case the hidden vectors are computed iteratively for all hidden layers <span class="math inline">\(n=1\)</span> to <span class="math inline">\(N\)</span> and for all time instances <span class="math inline">\(t=1\)</span> to <span class="math inline">\(T\)</span>:</p>

<p>As a hidden layer function <span class="citation">Graves, Mohamed, and Hinton (<a href="#ref-graves2013speech">2013</a>)</span> choose <strong>bidirectional LSTM</strong>. Compared to regular LSTM, BiLSTM can train on inputs in their original as well as reversed order. The idea is to stack two separate hidden layers one on another while one of the layers is responsible for the forward information flow and another one for the backward information flow. Especially in speech recognition one must take in consideration future context, too, because pronouncement depends both on previous and next phonemes. Thus, BiLSTMs are able to access long-time dependencies in both input directions. Finally, <span class="citation">Graves, Mohamed, and Hinton (<a href="#ref-graves2013speech">2013</a>)</span> compare different deep BiLSTM models (from 1 to 5 hidden layers) with unidirectional LSTMs and a pre-trained RNN transducer (BiLSTM with 3 hidden layers pretrained to predict each phoneme given the previous ones) and show clear advantage of deep networks over shallow designs.</p>
</div>
<div id="deep-transition-rnns" class="section level4">
<h4><span class="header-section-number">4.3.1.2</span> Deep Transition RNNs</h4>
<p><span class="citation">Pascanu et al. (<a href="#ref-pascanu2013construct">2013</a>)</span> proposed another way to make an RNN deeper by introducing <strong>transitions</strong>, one or more intermediate nonlinear layers between input to hidden, hidden to output or two consecutive hidden states. They argue that extending input-to-hidden functions helps to better capture temporal structure between successive inputs. A deeper hidden-to-output function, DO-RNN, can make hidden states more compact and therefore enables the model to summarize the previous inputs more efficiently. A deep hidden-to-hidden composition, DT-RNN, allows for the hidden states to effectively add new information to the accumulated summaries from the previous steps. They note though, because of including deep transitions, the distances between two variables at <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span> become longer and the problem of loosing long-time dependencies may occur. One can add shortcut connections to provide shorter paths for gradients, such networks are referred to as DT(S)-RNNs. If deep transitions with shortcuts are implemented both in hidden and output layers, the resulting model is called DOT(S)-RNNs. <span class="citation">Pascanu et al. (<a href="#ref-pascanu2013construct">2013</a>)</span> evaluate these designs on the tasks of polyphonic music prediction and character- or word-level language modelling. Their results reveal that deep transition RNNs clearly outperform shallow RNNs in terms of perplexity (see chapter <a href="./resources-and-benchmarks-for-nlp.html">11</a> for definition) and negative log-likelihood.</p>
</div>
</div>
<div id="encoder-decoder-architecture" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Encoder-Decoder Architecture</h3>
<div id="design-and-training" class="section level4">
<h4><span class="header-section-number">4.3.2.1</span> Design and Training</h4>
<p>The problem of mapping variable-length input sequences to variable-length output sequences is known as Sequence-to-Sequence or <strong>seq2seq</strong> learning in NLP. Although originally applied in machine translation tasks (<span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-sutskever2014sequence">2014</a>)</span>, <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>), the seq2seq approach achieved state-of-the-art results also in speech recognition <span class="citation">(Prabhavalkar et al. <a href="#ref-prabhavalkar2017comparison">2017</a>)</span> and video captioning <span class="citation">(Venugopalan et al. <a href="#ref-venugopalan2015sequence">2015</a>)</span>. According to <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>, the seq2seq model is composed of two parts as illustrated below:</p>
<div class="figure" style="text-align: center"><span id="fig:01-02-enc-dec"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/3_encoder_decoder.png" alt="Encoder-Decoder architecture. Source: Own figure based on @cho2014learning." width="100%" />
<p class="caption">
FIGURE 4.3: Encoder-Decoder architecture. Source: Own figure based on <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>.
</p>
</div>
<p>The first part is an <strong>encoder</strong>, an RNN which is trained on input sequences in order to obtain a large summary vector <span class="math inline">\(c\)</span> with a fixed dimension. This vector is called context and is usually a simple function of the last hidden state. <span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-sutskever2014sequence">2014</a>)</span> used the final encoder hidden state as context such that <span class="math inline">\(c=h_{e}^{(T)}\)</span>. The second part of the model is a <strong>decoder</strong>, another RNN which generates predictions given the context <span class="math inline">\(c\)</span> and all the previous outputs. In contrast to a simple RNN described at the beginning of this chapter, decoder hidden states <span class="math inline">\(h_{d}^{(t)}\)</span> are now conditioned on the previous outputs <span class="math inline">\(y^{(t)}\)</span>, previous hidden states <span class="math inline">\(h_{d}^{(t)}\)</span> and the summary vector <span class="math inline">\(c\)</span> from the encoder part. Therefore, the conditional distribution of the one-step prediction is obtained by:</p>

<p>Both parts are trained simultaneously to maximize the conditional log-likelihood <span class="math inline">\(\frac{1}{N}\sum_{n=1}^{N}\log{p_{\theta}(y_{n}|x_{n})}\)</span>, where <span class="math inline">\(\theta\)</span> denotes the set of model parameters and <span class="math inline">\((y_{n},x_{n})\)</span> is an (input sequence, output sequence) pair from the training set with size <span class="math inline">\(N\)</span> (<span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>).</p>
</div>
<div id="multi-task-seq2seq-learning" class="section level4">
<h4><span class="header-section-number">4.3.2.2</span> Multi-task seq2seq Learning</h4>
<p><span class="citation">Luong et al. (<a href="#ref-luong2015multi">2015</a>)</span> extended the idea of encoder-decoder architecture even further by allowing <strong>multi-task learning</strong> (<strong>MLT</strong>) for seq2seq models. MLT aims to improve performance of one task using other related tasks such that one task complements another. In their paper, they investigate the following three settings: a) <em>one-to-many</em> - where the encoder is shared between different tasks such as translation and syntactic parsing, b) <em>many-to-one</em> - where the encoders learn different tasks such as translation and image captioning and the decoder is shared, c) <em>many-to-many</em> - where the model consists of multiple encoders and decoders which is the case of autoencoders, an unsupervised task used to learn a representation of monolingual data.</p>
<div class="figure" style="text-align: center"><span id="fig:01-02-multi-task-seq2seq"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/4_multitask_seq2seq.png" alt="Multi-task settings. Source: @luong2015multi." width="100%" />
<p class="caption">
FIGURE 4.4: Multi-task settings. Source: <span class="citation">Luong et al. (<a href="#ref-luong2015multi">2015</a>)</span>.
</p>
</div>
<p><span class="citation">Luong et al. (<a href="#ref-luong2015multi">2015</a>)</span> consider German-to-English and English-to-German translations as the primary task and try to determine whether other tasks can improve their performance and vice versa. After training deep LSTM models with four layers for different task combinations, they conclude that MLT can improve the performance of seq2seq models substantially. For instance, the translation quality improves after adding a small number of parsing minibatches (one-to-many setting) or after the model have been trained to generate image captions (many-to-one setting). In turn, translation task helps to parse large data corpus much better (one-to-many setting). In contrast to these achievements, autoencoder task does not show significant improvements in translation after two unsupervised learning tasks on English and German language data.</p>
</div>
</div>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">4.4</span> Conclusion</h2>
<p>RNNs are powerful models for sequential data. Even in their simple form they can show valid results in different NLP tasks but their shortcomings led to introduction of more flexible networks such as gated units and encoder-decoder designs described in this chapter. As NLP has become a highly discussed topic in the recent years, even more advanced concepts such as Transfer Learning and Attention have been introduced, which still base on an RNN or its extension. However, RNNs are not the only type of neural networks used in NLP. Convolutional neural networks also find their application in modern NLP, and the next chapter will describe them.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boden2002guide">
<p>Boden, Mikael. 2002. “A Guide to Recurrent Neural Networks and Backpropagation.” <em>The Dallas Project</em>.</p>
</div>
<div id="ref-chen2016gentle">
<p>Chen, Gang. 2016. “A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation.” <em>arXiv Preprint arXiv:1610.02583</em>.</p>
</div>
<div id="ref-cho2014learning">
<p>Cho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine Translation.” <em>arXiv Preprint arXiv:1406.1078</em>.</p>
</div>
<div id="ref-chollet2018deep">
<p>Chollet, Francois. 2018. <em>Deep Learning Mit Python Und Keras: Das Praxis-Handbuch Vom Entwickler Der Keras-Bibliothek</em>. MITP-Verlags GmbH &amp; Co. KG.</p>
</div>
<div id="ref-goodfellow2016deep">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div id="ref-graves2013generating">
<p>Graves, Alex. 2013. “Generating Sequences with Recurrent Neural Networks.” <em>arXiv Preprint arXiv:1308.0850</em>.</p>
</div>
<div id="ref-graves2013speech">
<p>Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. “Speech Recognition with Deep Recurrent Neural Networks.” In <em>2013 Ieee International Conference on Acoustics, Speech and Signal Processing</em>, 6645–9. IEEE.</p>
</div>
<div id="ref-hochreiter1997long">
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8). MIT Press: 1735–80.</p>
</div>
<div id="ref-luong2015multi">
<p>Luong, Minh-Thang, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015. “Multi-Task Sequence to Sequence Learning.” <em>arXiv Preprint arXiv:1511.06114</em>.</p>
</div>
<div id="ref-mikolov2010recurrent">
<p>Mikolov, Tomáš, Martin Karafiát, Lukáš Burget, Jan Černocky, and Sanjeev Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In <em>Eleventh Annual Conference of the International Speech Communication Association</em>.</p>
</div>
<div id="ref-pascanu2013construct">
<p>Pascanu, Razvan, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2013. “How to Construct Deep Recurrent Neural Networks.” <em>arXiv Preprint arXiv:1312.6026</em>.</p>
</div>
<div id="ref-pascanu2013difficulty">
<p>Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. 2013. “On the Difficulty of Training Recurrent Neural Networks.” In <em>International Conference on Machine Learning</em>, 1310–8.</p>
</div>
<div id="ref-prabhavalkar2017comparison">
<p>Prabhavalkar, Rohit, Kanishka Rao, Tara N Sainath, Bo Li, Leif Johnson, and Navdeep Jaitly. 2017. “A Comparison of Sequence-to-Sequence Models for Speech Recognition.” In <em>Interspeech</em>, 939–43.</p>
</div>
<div id="ref-sutskever2014sequence">
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3104–12.</p>
</div>
<div id="ref-venugopalan2015sequence">
<p>Venugopalan, Subhashini, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2015. “Sequence to Sequence-Video to Text.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 4534–42.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="foundationsapplications-of-modern-nlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="convolutional-neural-networks-and-their-applications-in-nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/01-02-rnns-and-their-applications-in-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
