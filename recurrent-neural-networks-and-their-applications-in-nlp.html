<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Recurrent neural networks and their applications in NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-06-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="foundationsapplications-of-modern-nlp.html"/>
<link rel="next" href="convolutional-neural-networks-and-their-applications-in-nlp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#model-improvements"><i class="fa fa-check"></i><b>3.3</b> Model Improvements</a><ul>
<li class="chapter" data-level="3.3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#fasttext"><i class="fa fa-check"></i><b>3.3.1</b> fastText</a></li>
<li class="chapter" data-level="3.3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word-phrases"><i class="fa fa-check"></i><b>3.3.2</b> Word Phrases</a></li>
<li class="chapter" data-level="3.3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#multiple-meanings-per-word"><i class="fa fa-check"></i><b>3.3.3</b> Multiple Meanings per Word</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>3.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.5</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.6" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#sources-and-applications-of-word-embeddings"><i class="fa fa-check"></i><b>3.6</b> Sources and Applications of Word Embeddings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extentions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extentions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#bidirectional-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Bidirectional RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.2</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.3</b> Encoder-Decoder Architecture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
<li class="chapter" data-level="7" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>7</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>7.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>7.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="7.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>7.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>8</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="8.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#introduction-1"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#transfer-learning-in-nlp"><i class="fa fa-check"></i><b>8.2</b> Transfer Learning in NLP</a></li>
<li class="chapter" data-level="8.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#steps-in-sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>8.3</b> Steps in sequential inductive transfer learning</a></li>
<li class="chapter" data-level="8.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#most-popular-models"><i class="fa fa-check"></i><b>8.4</b> Most popular models</a><ul>
<li class="chapter" data-level="8.4.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>8.4.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="8.4.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit---cutting-edge-model-using-lstms"><i class="fa fa-check"></i><b>8.4.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="8.4.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>8.4.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>9</b> Attention and Self-Attention for NLP</a></li>
<li class="chapter" data-level="10" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>10</b> Transfer Learning for NLP II</a></li>
<li class="chapter" data-level="11" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="12" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Resources and Benchmarks for NLP</a></li>
<li class="chapter" data-level="13" data-path="software-for-nlp-the-huggingface-transformers-module.html"><a href="software-for-nlp-the-huggingface-transformers-module.html"><i class="fa fa-check"></i><b>13</b> Software for NLP: The huggingface transformers module</a></li>
<li class="chapter" data-level="14" data-path="use-bases-for-nlp.html"><a href="use-bases-for-nlp.html"><i class="fa fa-check"></i><b>14</b> Use-Bases for NLP</a></li>
<li class="chapter" data-level="15" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>15</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="15.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#introduction-2"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition"><i class="fa fa-check"></i><b>15.2</b> Definition</a></li>
<li class="chapter" data-level="15.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#domains-of-nlg-systems"><i class="fa fa-check"></i><b>15.3</b> Domains of NLG Systems</a></li>
<li class="chapter" data-level="15.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#commercial-activity"><i class="fa fa-check"></i><b>15.4</b> Commercial Activity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="the-architectures.html"><a href="the-architectures.html"><i class="fa fa-check"></i><b>16</b> The Architectures</a><ul>
<li class="chapter" data-level="16.1" data-path="the-architectures.html"><a href="the-architectures.html#encoder-decoder-architecture-1"><i class="fa fa-check"></i><b>16.1</b> Encoder-Decoder Architecture</a><ul>
<li class="chapter" data-level="16.1.1" data-path="the-architectures.html"><a href="the-architectures.html#encoder"><i class="fa fa-check"></i><b>16.1.1</b> Encoder :</a></li>
<li class="chapter" data-level="16.1.2" data-path="the-architectures.html"><a href="the-architectures.html#decoder"><i class="fa fa-check"></i><b>16.1.2</b> Decoder :</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="the-architectures.html"><a href="the-architectures.html#attention-architecture"><i class="fa fa-check"></i><b>16.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="16.3" data-path="the-architectures.html"><a href="the-architectures.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>16.3</b> Decoding Algorithm at Inference</a><ul>
<li class="chapter" data-level="16.3.1" data-path="the-architectures.html"><a href="the-architectures.html#beam-search"><i class="fa fa-check"></i><b>16.3.1</b> Beam Search</a></li>
<li class="chapter" data-level="16.3.2" data-path="the-architectures.html"><a href="the-architectures.html#pure-sampling-decoder"><i class="fa fa-check"></i><b>16.3.2</b> Pure Sampling Decoder</a></li>
<li class="chapter" data-level="16.3.3" data-path="the-architectures.html"><a href="the-architectures.html#k-sampling-decoder"><i class="fa fa-check"></i><b>16.3.3</b> K-sampling Decoder</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="the-architectures.html"><a href="the-architectures.html#memory-networks"><i class="fa fa-check"></i><b>16.4</b> Memory Networks</a></li>
<li class="chapter" data-level="16.5" data-path="the-architectures.html"><a href="the-architectures.html#language-models"><i class="fa fa-check"></i><b>16.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="question-answer-systems.html"><a href="question-answer-systems.html"><i class="fa fa-check"></i><b>17</b> Question-Answer Systems</a><ul>
<li class="chapter" data-level="17.1" data-path="question-answer-systems.html"><a href="question-answer-systems.html#datasets-1"><i class="fa fa-check"></i><b>17.1</b> Datasets</a></li>
<li class="chapter" data-level="17.2" data-path="question-answer-systems.html"><a href="question-answer-systems.html#types"><i class="fa fa-check"></i><b>17.2</b> Types</a></li>
<li class="chapter" data-level="17.3" data-path="question-answer-systems.html"><a href="question-answer-systems.html#architectures"><i class="fa fa-check"></i><b>17.3</b> Architectures</a></li>
<li class="chapter" data-level="17.4" data-path="question-answer-systems.html"><a href="question-answer-systems.html#evaluation-metrics"><i class="fa fa-check"></i><b>17.4</b> Evaluation Metrics</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="dialog-systems.html"><a href="dialog-systems.html"><i class="fa fa-check"></i><b>18</b> Dialog Systems</a><ul>
<li class="chapter" data-level="18.1" data-path="dialog-systems.html"><a href="dialog-systems.html#types-1"><i class="fa fa-check"></i><b>18.1</b> Types</a></li>
<li class="chapter" data-level="18.2" data-path="dialog-systems.html"><a href="dialog-systems.html#architectures-1"><i class="fa fa-check"></i><b>18.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>19</b> Conclusion</a></li>
<li class="chapter" data-level="20" data-path="use-case-ii.html"><a href="use-case-ii.html"><i class="fa fa-check"></i><b>20</b> Use-Case II</a></li>
<li class="chapter" data-level="21" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>21</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recurrent-neural-networks-and-their-applications-in-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Recurrent neural networks and their applications in NLP</h1>
<p><em>Author: Marianna Plesiak</em></p>
<p><em>Supervisor: Prof. Dr. Christian Heumann</em></p>
<div id="structure-and-training-of-simple-rnns" class="section level2">
<h2><span class="header-section-number">4.1</span> Structure and Training of Simple RNNs</h2>
<p>Recurrent neural networks allow to relax the condition of non-cyclical connections in the classical feedforward neural networks which were described in the previous chapter. This means, while simple multilayer perceptrons can only map from input to output vectors, RNNs allow the entire history of previous inputs to influence the network output. <span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span></p>
<p>The first part of this chapter provides the structure definition of RNNs, presents the principles of their training and explains problems with backpropagation. In the second part, gated units, an improved way to calculate hidden states, are explained. The third part gives an overview of some extended versions of RNNs and their applications in NLP.</p>
<div id="network-structure-and-forwardpropagation" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Network Structure and Forwardpropagation</h3>
<p>The repetitive structure of RNNs can be visualised with help of an unfolded computational graph (see <a href="recurrent-neural-networks-and-their-applications-in-nlp.html#fig:01-02-unfold">4.1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:01-02-unfold"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/02_unfolded_graph.png" alt="Unfolded computatinal graph of a RNN. Source: Own figure." width="100%" />
<p class="caption">
FIGURE 4.1: Unfolded computatinal graph of a RNN. Source: Own figure.
</p>
</div>
<p>Each node is associated with a network layer at a particular time instance. Inputs <span class="math inline">\(x^{(t)}\)</span> must be encoded as numeric vectors, for instance word embeddings or one-hot encoded vectors, see previous chapter. Reccurently connected vectors <span class="math inline">\(h\)</span> are called hidden states and represent the outputs of the hidden layer. At time <span class="math inline">\(t\)</span>, a hidden state <span class="math inline">\(h^{(t)}\)</span> combines information from the previous hidden state <span class="math inline">\(h^{(t-1)}\)</span> as well as the new input <span class="math inline">\(x^{(t)}\)</span> and passes it through to the next hidden state. Obviously, such an architecture requires the initialization of <span class="math inline">\(h^{(0)}\)</span> since there is no memory at the very beginning of the sequence processing. Given the hidden sequences, output vectors <span class="math inline">\(\hat{y}^{(t)}\)</span> are used to build the predictive distribution <span class="math inline">\(Pr(x^{(t+1)}|y^{(t)})\)</span> for the next input <span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span>. Since the predictions are created at each time instance <span class="math inline">\(t\)</span>, the total output has a shape [time_steps, output_features]. However in some cases this is not needed, for example in sentiment analysis the last output of the loop is sufficient because it contains the entire information about the sequence. <span class="citation">(Chollet <a href="#ref-chollet2018deep">2018</a>)</span></p>
<p>The unfolded recurrence can be formalized as following:</p>

<p>After <span class="math inline">\(t\)</span> steps, the function <span class="math inline">\(g^{(t)}\)</span> takes into account the whole sequence <span class="math inline">\((x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)})\)</span> and produces the hidden state <span class="math inline">\(h^{(t)}\)</span>. Because of its cyclical structure, <span class="math inline">\(g^{(t)}\)</span> can be factorized into the repeated application of a same function <span class="math inline">\(f\)</span>. This function can be considered a universal model which is shared across all time steps and is generalized for all sequence lengths. This is called parameter sharing and is illustrated in the unfolded computational graph as a reuse of the same matrices <span class="math inline">\(W_{xh}\)</span>, <span class="math inline">\(W_{hh}\)</span> and <span class="math inline">\(W_{hy}\)</span> through the entire network. <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016deep">2016</a>)</span></p>
<p>Consider a recurrent neural network with one hidden layer that is used to predict words or characters, so the output is discrete and the model maps input sequence to output sequence of the same length. Then the forward propagation is computed by iterating the following equations:</p>

<p>where the parameters and functions denote the following:</p>
<ul>
<li><span class="math inline">\(\mathcal{H}\)</span>: hidden layer function. Usually it is a sigmoid activation function ( <span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-sutskever2014sequence">2014</a>)</span> and <span class="citation">Mikolov et al. (<a href="#ref-mikolov2010recurrent">2010</a>)</span>) or tanh or ReLu</li>
<li><span class="math inline">\(W_{hh}\)</span>: weight matrix connecting recurrent connections between hidden states</li>
<li><span class="math inline">\(W_{xh}\)</span>: weight matrix connecting inputs to hidden layer</li>
<li><span class="math inline">\(W_{hy}\)</span>: weight matrix connecting hidden states to outputs (softmax if we want to predict next word or letter)</li>
<li><span class="math inline">\(\mathcal{Y}\)</span>: output layer function. If the model is used to predict words, softmax function is usually chosen as it returns valid probabilities over the possible outputs <span class="citation">(Mikolov et al. <a href="#ref-mikolov2010recurrent">2010</a>)</span></li>
<li><span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>: input and output bias vectors.</li>
</ul>
<p><span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span></p>
<p>Because inputs <span class="math inline">\(x^{(t)}\)</span> are usually encoded as one-hot-vectors, the dimension of a vector representing one word corresponds to the size of vocabulary. The size of a hidden layer must reflect the size of training data. The model training requires initialization of the initial state <span class="math inline">\(h^{(0)}\)</span> as well as the weight matrices, which are usually set to small random values <span class="citation">(Mikolov et al. <a href="#ref-mikolov2010recurrent">2010</a>)</span>. Since the network is used to compute the predictive distributions <span class="math inline">\(Pr(x^{(t+1)}|y^{(t)})\)</span> at each time instance <span class="math inline">\(t\)</span>, the network distribution is denoted as:</p>

<p>and the total loss used for training is simply the sum of the losses over all time steps denoted as the negative log-likelihood of <span class="math inline">\(Pr(x)\)</span>:</p>

<p><span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span></p>
</div>
<div id="backpropagation" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Backpropagation</h3>
<p>In order to train the model, one must calculate the gradients for the three weight matrices <span class="math inline">\(W_{xh}\)</span>, <span class="math inline">\(W_{hh}\)</span> and <span class="math inline">\(W_{hy}\)</span>. The algorithm differs from a regular backpropagation because a chain rule muste be applied recursively and the gradients are summed up through the network. <span class="citation">(Boden <a href="#ref-boden2002guide">2002</a>)</span></p>
<p>Gradients w.r.t. <span class="math inline">\(W_{hy}\)</span>:</p>
<ul>
<li>for a single time step <span class="math inline">\(t\)</span>:</li>
</ul>

<ul>
<li>for the whole sequence:</li>
</ul>

<p>Gradients w.r.t. <span class="math inline">\(W_{hh}\)</span>:</p>
<ul>
<li>for a single time step <span class="math inline">\(t\)</span>:</li>
</ul>

<p>The last part <span class="math inline">\(h^{(t)}\)</span> also depends on <span class="math inline">\(h^{(t-1)}\)</span> and the gradient can be rewritten as:</p>

<ul>
<li>for the whole sequence:</li>
</ul>

<p>Gradient w.r.t. <span class="math inline">\(W_{xh}\)</span> is similar to <span class="math inline">\(W_{hh}\)</span>:</p>
<ul>
<li>for a single step:</li>
</ul>

<ul>
<li>for the whole sequence:</li>
</ul>

<p><span class="citation">(Chen <a href="#ref-chen2016gentle">2016</a>)</span></p>
</div>
<div id="vanishing-and-exploding-gradients" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Vanishing and Exploding Gradients</h3>
</div>
</div>
<div id="gated-rnns" class="section level2">
<h2><span class="header-section-number">4.2</span> Gated RNNs</h2>
<div id="lstm" class="section level3">
<h3><span class="header-section-number">4.2.1</span> LSTM</h3>
<p>Long Short Term Memory networks were introduced by <span class="citation">Hochreiter and Schmidhuber (<a href="#ref-hochreiter1997long">1997</a>)</span> to deal with problems of long term dependencies. Instead of a simple hidden unit that combines inputs and previous hidden states linearly and outputs the non-linear transformation to the next step, hidden units are now extended by special input, forget and output gates which help to control the flow of information. Such more complex units are called memory cells and the following equations show how a LSTM uses the gating mechanism to calculate the hidden state within a memory cell:</p>
<p><span class="math display" id="eq:lstm-newoutput" id="eq:lstm-newcell" id="eq:lstm-candidates" id="eq:lstm-output" id="eq:lstm-input" id="eq:lstm-forget">\[\begin{align}
f^{(t)} &amp; = sigm(W_{xf}x^{(t)}+W_{hf}h^{(t-1)}+b_{f}) \tag{4.1} \\
i^{(t)} &amp; = sigm(W_{xi}x^{(t)}+W_{hi}h^{(t-1)}+b_{i}) \tag{4.2} \\
o^{(t)} &amp; = sigm(W_{xo}x^{(t)}+W_{ho}h^{(t-1)}+b_{o}) \tag{4.3} \\
g^{(t)} &amp; = tanh(W_{xc}x^{(t)}+W_{hc}h^{(t-1)}+b_{c}) \tag{4.4} \\
c^{(t)} &amp; = f^{(t)}c^{(t-1)}+i^{(t)}g^{(t)} \tag{4.5} \\
h^{(t)} &amp; = o^{(t)}tanh(c^{(t)}) \tag{4.6} \\
\end{align}\]</span>
<span class="citation">(Graves <a href="#ref-graves2013generating">2013</a>)</span></p>
<p>First, forget gate <span class="math inline">\(f^{(t)}\)</span> decides which values of the previous output <span class="math inline">\(h^{(t-1)}\)</span> to forget. The next step is deciding which information will be stored in the internal cell state <span class="math inline">\(c^{(t)}\)</span>. This step consists of two parts: 1) multiplication of the old state <span class="math inline">\(c^{(t-1)}\)</span> by <span class="math inline">\(f^{(t)}\)</span> (forgetting information); 2) adding new candidates calculated in <span class="math inline">\(g^{(t)}\)</span> with help of its multiplication by values from the input gate <span class="math inline">\(i^{(t)}\)</span> (adding new information). The output <span class="math inline">\(h^{(t)}\)</span> is produced with help of the output gate <span class="math inline">\(o^{(t)}\)</span> and applying a <span class="math inline">\(tanh\)</span> function to the cell state in order to only output values which were chosen.(<span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow2016deep">2016</a>)</span>, <span class="citation">Graves (<a href="#ref-graves2013generating">2013</a>)</span>)</p>
</div>
<div id="gru" class="section level3">
<h3><span class="header-section-number">4.2.2</span> GRU</h3>
<p>Invented by <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>.
Is simpler because it includes only two gates: reset and update</p>
<p>The hidden unit is calculated as:</p>

<p>Illustration LSTM vs GRU</p>
<div class="figure" style="text-align: center"><span id="fig:01-02-lstm-gru"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/03_lstm_vs_gru.PNG" alt="Structure of a hidden unit. LSTM on the right and GRU on the left. Source: Own figure inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/" width="100%" />
<p class="caption">
FIGURE 4.2: Structure of a hidden unit. LSTM on the right and GRU on the left. Source: Own figure inspired by <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
</p>
</div>
</div>
</div>
<div id="extentions-of-simple-rnns" class="section level2">
<h2><span class="header-section-number">4.3</span> Extentions of Simple RNNs</h2>
<div id="bidirectional-rnns" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Bidirectional RNNs</h3>
<p>One of the limitations of simple RNNs is that they can process the information only up to a present time instance. In many NLP fields however, prediction may also depend on future inputs. Bidirectional RNNs, introduced by <span class="citation">Schuster and Paliwal (<a href="#ref-schuster1997bidirectional">1997</a>)</span>, handle this issue by training on inputs in original as well as reversed order. The idea is to stack two hidden layers one on another while one of the layers is responsible for the forward information flow and another one for the backward information flow. Such a composition allows using both past and future inputs at any time instance. Since both types of hidden states do not interact, the bidirectional RNNs follow almost the same training rules as a regular RNN with small differences regarding backpropagation.</p>
</div>
<div id="deep-rnns" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Deep RNNs</h3>
<p>The figure <a href="recurrent-neural-networks-and-their-applications-in-nlp.html#fig:01-02-unfold">4.1</a> shows that each layer is associated with one parameter matrix so that the model is considered shallow. This structure can be extended to a deep RNN although the depth of an RNN is not a trivial concept since its units are already expressed as a nonlinear function of multiple previous units. However, a deep RNN is defined as a network with several input, hidden, or output layers stacked on top of each other. Such a structure can be more efficient at representing complex dependencies although it also requires higher computational effort because the distances between two variables at <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span> become longer. <span class="citation">(Pascanu et al. <a href="#ref-pascanu2013construct">2013</a>)</span></p>
<p><span class="citation">Pascanu et al. (<a href="#ref-pascanu2013construct">2013</a>)</span> proposed three ways to make an RNN deeper. Extending input-to-hidden functions helps to transform the raw input, e.g. one-hot encoded vectors, into a more convenient and informative representations, e.g. word embeddings, for higher levels. A deep hidden-to-hidden composition allows for the hidden states to effectively add new information to the accumulated summaries from the previous steps. For example, the model preserves useful information from the past although new inputs vary strongly and rapidly. Finally, a deep hidden-to-output function is used to make hidden states more compact and therefore enables the model to summarize the previous inputs more efficiently.</p>
</div>
<div id="encoder-decoder-architecture" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Encoder-Decoder Architecture</h3>
<p>The problem of mapping variable-length input sequences to variable-length output sequences is known as Sequence-to-Sequence or seq2seq learning in NLP. Although originally applied in machine translation tasks (<span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-sutskever2014sequence">2014</a>)</span>, <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>), the seq2seq approach achieved state-of-the-art results also in speech recognition <span class="citation">(Prabhavalkar et al. <a href="#ref-prabhavalkar2017comparison">2017</a>)</span> and video captioning <span class="citation">(Venugopalan et al. <a href="#ref-venugopalan2015sequence">2015</a>)</span>. According to <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>, the seq2seq model consists of two parts as illustrated below:</p>
<div class="figure" style="text-align: center"><span id="fig:01-02-enc-dec"></span>
<img src="figures/01-02-rnns-and-their-applications-in-nlp/04_encoder_decoder.PNG" alt="Encoder-Decoder architecture. Source: Own figure." width="100%" />
<p class="caption">
FIGURE 4.3: Encoder-Decoder architecture. Source: Own figure.
</p>
</div>
<p>The first part is the encoder, an RNN which is trained on input sequences in order to obtain a large summary vector <span class="math inline">\(c\)</span> with a fixed dimension. The second part is a decoder, another RNN which generates predictions. In contrast to a simple RNN described at the beginning of this chapter, outputs <span class="math inline">\(y^{(t)}\)</span> and decoder hidden states <span class="math inline">\(h^{(t)}\)</span> are both conditioned on the summary vector <span class="math inline">\(c\)</span> from the encoder part as well as on <span class="math inline">\(y^{(t-1)}\)</span> and are computed by:</p>


</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boden2002guide">
<p>Boden, Mikael. 2002. “A Guide to Recurrent Neural Networks and Backpropagation.” <em>The Dallas Project</em>.</p>
</div>
<div id="ref-chen2016gentle">
<p>Chen, Gang. 2016. “A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation.” <em>arXiv Preprint arXiv:1610.02583</em>.</p>
</div>
<div id="ref-cho2014learning">
<p>Cho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine Translation.” <em>arXiv Preprint arXiv:1406.1078</em>.</p>
</div>
<div id="ref-chollet2018deep">
<p>Chollet, Francois. 2018. <em>Deep Learning Mit Python Und Keras: Das Praxis-Handbuch Vom Entwickler Der Keras-Bibliothek</em>. MITP-Verlags GmbH &amp; Co. KG.</p>
</div>
<div id="ref-goodfellow2016deep">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div id="ref-graves2013generating">
<p>Graves, Alex. 2013. “Generating Sequences with Recurrent Neural Networks.” <em>arXiv Preprint arXiv:1308.0850</em>.</p>
</div>
<div id="ref-hochreiter1997long">
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8). MIT Press: 1735–80.</p>
</div>
<div id="ref-mikolov2010recurrent">
<p>Mikolov, Tomáš, Martin Karafiát, Lukáš Burget, Jan Černocky, and Sanjeev Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In <em>Eleventh Annual Conference of the International Speech Communication Association</em>.</p>
</div>
<div id="ref-pascanu2013construct">
<p>Pascanu, Razvan, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2013. “How to Construct Deep Recurrent Neural Networks.” <em>arXiv Preprint arXiv:1312.6026</em>.</p>
</div>
<div id="ref-prabhavalkar2017comparison">
<p>Prabhavalkar, Rohit, Kanishka Rao, Tara N Sainath, Bo Li, Leif Johnson, and Navdeep Jaitly. 2017. “A Comparison of Sequence-to-Sequence Models for Speech Recognition.” In <em>Interspeech</em>, 939–43.</p>
</div>
<div id="ref-schuster1997bidirectional">
<p>Schuster, Mike, and Kuldip K Paliwal. 1997. “Bidirectional Recurrent Neural Networks.” <em>IEEE Transactions on Signal Processing</em> 45 (11). Ieee: 2673–81.</p>
</div>
<div id="ref-sutskever2014sequence">
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3104–12.</p>
</div>
<div id="ref-venugopalan2015sequence">
<p>Venugopalan, Subhashini, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2015. “Sequence to Sequence-Video to Text.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 4534–42.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="foundationsapplications-of-modern-nlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="convolutional-neural-networks-and-their-applications-in-nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/01-02-rnns-and-their-applications-in-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
