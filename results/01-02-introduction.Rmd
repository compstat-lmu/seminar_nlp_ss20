---
header-includes:
- \usepackage[table]{xcolor}
output:
  
  html_document: default
---
Recurrent Neural Networks, or RNNs are a special family of neural networks which were delevoped for modeling sequential data like text. RNNs process a sequence of words or letters $x^{(1)}, ..., x^{(t)}$ by going through its elements one by one and capturing information based on the previous elements. This information is stored in hidden states $h^{(t)}$ as a memory of the network. Core idea is rather simple: we start with a zero vector as a hidden state (because there is no memory yet), process the current state at time $t$ as well as the output from the previous hidden state, and give the result as an input to the next iteration. (Goodfellow 2015)

Basically, a simple RNN is a for-loop that reuses the values which were calculated in the previous iteration. The structure of a classical RNN can be displayed with help of an unfolded computational graph (@ Figure). The gray circle on the left represents a delay of one time step and the arrows on the right express the flow of information in time. (Chollet 2017)

```{r pressure, echo=FALSE, message=FALSE, fig.align="center",fig.cap="Right: Circuit diagram of a simple RNN. Left: Unfolded computational graph of a simple RNN.", out.width = '80%'}

library(here)
knitr::include_graphics(here("/figures/01-02-rnns-and-their-applications-in-nlp/01_intro_folded_and_unfolded_graph.png"))
```

One particular reason why recurrent networks became such a powerful technique in processing sequential data is parameter sharing. Weight matrices remain the same through the loop and are used repeatedly which makes RNNs extremely convenient to work with sequential data because the model size does not grow for longer inputs. Parameter sharing allows application of models to inputs of different length and enables generalization across different positions in time. (Goodfellow 2015)

As each part of the output is a function of the previous parts of the output, backpropagation for the RNNs requires recursive computations of the gradient. The so-called backpropagation in time or BPTT is rather simple in theory and allows for the RNNs to access information from many steps back. In practice though, RNNs in their simple form are subject to two big problems: exploading and vanishing gradients. As we compute gradients recursively, they may become either very small or very large which leads to a complete loss of information about long-term dependencies. To avoid these problems, gated RNNs were developed which accumulate information about specific features over a long duration. The two most popular types of gated RNNs which are widely used in modern NLP are LSTM and GRU. (Goodfellow 2015)

Over last couple of years, various extentions of RNNs were developed which resulted in their wide application in different fields of NLP. Beside classical tasks as document classification and sentiment analysis more complicated challenges such as machine translation, part-of-speech tagging or speech recognition can be solved nowadays with help of advanced versions of RNNs. An overview of these versions and their applications in NLP will be provided in the last part of Chapter 1.2.





