\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Modern Approaches in Natural Language Processing},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Modern Approaches in Natural Language Processing}
\author{}
\date{\vspace{-2.5em}2020-06-05}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{0}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


In the last few years, there have been several breakthroughs concerning the methodologies used in Natural Language Processing (NLP). These breakthroughs originate from both new modeling frameworks as well as from improvements in the availability of computational and lexical resources.

In this seminar, we are planning to review these frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings.

We will further discuss the integration of embeddings into end-to-end trainable approaches, namely convolutional and recurrent neural networks. As Attention-based models and transfer learning approaches are the foundation of most of the recent state-of-the-art models, we will cover these two topics extensively in the second part of our seminar.

We will furthermore talk about software implementations of these methods and benchmark tasks/data sets for evaluating state-of-the-art models.

This book is the outcome of the seminar ``Modern Approaches in Natural Language Processing'' which took place in summer 2020 at the Department of Statistics, LMU Munich.

\begin{figure}
\centering
\includegraphics{figures/by-nc-sa.png}
\caption{Creative Commons License}
\end{figure}

This book is licensed under the \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}.

\mainmatter

\hypertarget{foreword}{%
\chapter*{Foreword}\label{foreword}}


\emph{Author: Christoph Molnar}

This book is the result of an experiment in university teaching.
Each semester, students of the Statistics Master can choose from a selection of seminar topics.
Usually, every student in the seminar chooses a scientific paper, gives a talk about the paper and summarizes it in the form of a seminar paper.
The supervisors help the students, they listen to the talks, read the seminar papers, grade the work and then \ldots{} hide the seminar papers away in (digital) drawers.
This seemed wasteful to us, given the huge amount of effort the students usually invest in seminars.
An idea was born:
Why not create a book with a website as the outcome of the seminar?
Something that will last at least a few years after the end of the semester.
In the summer term 2020, some Statistics Master students signed up for our seminar entitled ``Limitations of Interpretable Machine Learning''.
When they came to the kick-off meeting, they had no idea that they would write a book by the end of the semester.

We were bound by the examination rules for conducting the seminar, but otherwise we could deviate from the traditional format.
We deviated in several ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each student project is part of a book, and not an isolated seminar paper.
\item
  We gave challenges to the students, instead of papers. The challenge was to investigate a specific limitation of interpretable machine learning methods.
\item
  We designed the work to live beyond the seminar.
\item
  We emphasized collaboration. Students wrote some chapters in teams and reviewed each others texts.
\end{enumerate}

\hypertarget{technical-setup}{%
\section*{Technical Setup}\label{technical-setup}}


The book chapters are written in the Markdown language.
The simulations, data examples and visualizations were created with R \citep{rlang}.
To combine R-code and Markdown, we used rmarkdown.
The book was compiled with the bookdown package.
We collaborated using git and github.
For details, head over to the \href{link/to/repo}{book's repository}.

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\emph{Author: }

\emph{Supervisor: }

\hypertarget{intro-about-the-seminar-topic}{%
\section{Intro About the Seminar Topic}\label{intro-about-the-seminar-topic}}

\hypertarget{outline-of-the-booklet}{%
\section{Outline of the Booklet}\label{outline-of-the-booklet}}

\hypertarget{chapter-1}{%
\chapter{Chapter 1}\label{chapter-1}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{lorem-ipsum}{%
\section{Lorem Ipsum}\label{lorem-ipsum}}

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

\citet{rlang}

\hypertarget{using-figures}{%
\section{Using Figures}\label{using-figures}}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/000-test/ch01-figure01} 

}

\caption{This is the caption of the figure!}\label{fig:ch01-figure01}
\end{figure}



Referencing can be done by using the chunk label e.g. \texttt{\textbackslash{}@ref(fig:ch01-figure01)} for \ref{fig:ch01-figure01}.

\textbf{NOTE!!!} Do not use underscores in chunk labels! This will crash the compilation \ldots{}

\hypertarget{using-tex}{%
\section{Using Tex}\label{using-tex}}

HTML rendering uses MathJax while pdf rendering uses LaTeX:

\[
f(x) = x^2
\]

\hypertarget{using-stored-results}{%
\section{Using Stored Results}\label{using-stored-results}}

\begin{tabular}{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 2.1713 & 0.2798 & 7.760 & 0.0000\\
\hline
Sepal.Width & 0.4959 & 0.0861 & 5.761 & 0.0000\\
\hline
Petal.Length & 0.8292 & 0.0685 & 12.101 & 0.0000\\
\hline
Petal.Width & -0.3152 & 0.1512 & -2.084 & 0.0389\\
\hline
Speciesversicolor & -0.7236 & 0.2402 & -3.013 & 0.0031\\
\hline
Speciesvirginica & -1.0235 & 0.3337 & -3.067 & 0.0026\\
\hline
\end{tabular}

\hypertarget{introduction-deep-learning-for-nlp}{%
\chapter{Introduction: Deep Learning for NLP}\label{introduction-deep-learning-for-nlp}}

\emph{Authors: Viktoria Szabo, Marianna Plesiak, Rui Yang}

\emph{Supervisor: Christian Heumann}

\hypertarget{word-embeddings-and-neural-network-language-models}{%
\section{Word Embeddings and Neural Network Language Models}\label{word-embeddings-and-neural-network-language-models}}

In natural language processing computers try to analyze and understand human language for the purpose of performing useful tasks. Therefore, they extract relevant information from words and sentences. But how exactly are they doing this? After the first wave of rationalist approaches with handwritten rules didn't work out too well, neural networks were introduced to find those rules by themselves (see \citet{Bengio.2003}). But neural networks and other machine learning algorithms cannot handle non-numeric input, so we have to find a way to convert the text we want to analyze into numbers.
There are a lot of possibilities to do that. Two simple approaches would be labeling each word with a number (One-Hot Encoding, figure \ref{fig:onehot-bow}) or counting the frequency of words in different text fragments (Bag-of-Words, figure \ref{fig:onehot-bow}). Both methods result in high-dimensional, sparse (mostly zero) data. And there is another major drawback using such kind of data as input. It does not convey any similarities between words. The word ``cat'' would be as similar to the word ``tiger'' as to ``car''. That means the model cannot reuse information it already learned about cats for the much rarer word tiger. This will usually lead to poor model performance and is called a lack of generalization power.

\begin{figure}
\includegraphics[width=0.5\linewidth]{figures/01-00-deep-learning-for-nlp/01-01_one-hot} \includegraphics[width=0.5\linewidth]{figures/01-00-deep-learning-for-nlp/01-01_bow} \caption{One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure.}\label{fig:onehot-bow}
\end{figure}

The solution to this problem is word embedding. Word embeddings use dense vector representations for words. That means they map each word to a continuous vector with n dimensions. The distance in the vector space denotes semantic (dis)similarity. These word embeddings are usually learned by neural networks, either within the final model in an additional layer or in its own model. Once learned they can be reused across different tasks. Practically all NLP projects these days build upon word embeddings, since they have a lot of advantages compared to the aforementioned representations.
The basic idea behind learning word embeddings is the so called ``distributional hypothesis'' (see \citet{Harris.1954}). It states that words that occur in the same contexts tend to have similar meanings. The two best known approaches for calculating word embeddings are Word2vec from \citet{Mikolov.2013c} and GloVE from \citet{Pennington.2014}. The Word2vec models (Continous Bag-Of-Words (CBOW) and Skip-gram) try to predict a target word given his context or context words given a target word using a simple feed-forward neural network. In contrast to these models GloVe not only uses the local context windows, but also incorporates global word co-occurrence counts.
As mentioned, a lot of approaches use neural networks to learn word embeddings. A simple feed-forward network with fully connected layers for learning such embeddings while predicting the next word for a given context is shown in figure \ref{fig:nnlm}. In this example the word embeddings are first learnt in a projection layer and are then used in two hidden layers to model the probability distribution over all words in the vocabulary. With this distribution one can predict the target word. This simple structure can be good enough for some tasks but it also has a lot of limitations. Therefore, recurrent and convolutional networks are used to overcome the limitations of a simple neural network.

\begin{figure}
\includegraphics[width=1\linewidth]{figures/01-00-deep-learning-for-nlp/01-01_nnlm} \caption{Feed-forward Neural Network. Source: Own figure based on Bengio et al. 2013.}\label{fig:nnlm}
\end{figure}

\hypertarget{recurrent-neural-networks}{%
\section{Recurrent Neural Networks}\label{recurrent-neural-networks}}

The main drawback of feedforward neural networks is that they assume a fixed length of input and output vectors which is known in advance. But for many natural language problems such as machine translation and speech recognition it is impossible to define optimal fixed dimensions a-priori. Other models that map a sequence of words to another sequence of words are needed \citep{sutskever2014sequence}. Recurrent neural networks or RNNs are a special family of neural networks which were explicitely developed for modeling sequential data like text. RNNs process a sequence of words or letters \(x^{(1)}, ..., x^{(t)}\) by going through its elements one by one and capturing information based on the previous elements. This information is stored in hidden states \(h^{(t)}\) as the network memory. Core idea is rather simple: we start with a zero vector as a hidden state (because there is no memory yet), process the current state at time \(t\) as well as the output from the previous hidden state, and give the result as an input to the next iteration \citep{goodfellow2016deep}.

Basically, a simple RNN is a for-loop that reuses the values which are calculated in the previous iteration \citep{chollet2018deep}. An unfolded computational graph (figure \ref{fig:unfolded}) can display the structure of a classical RNN. The gray square on the left represents a delay of one time step and the arrows on the right express the flow of information in time \citep{goodfellow2016deep}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/home/travis/build/compstat-lmu/seminar_nlp_ss20/figures/01-00-deep-learning-for-nlp/01_02_unfolded_graph} 

}

\caption{Right: Circuit diagram (left) and unfolded computational graph (right) of a simple RNN. Source: Own figure.}\label{fig:unfolded}
\end{figure}

One particular reason why recurrent networks have become such a powerful technique in processing sequential data is parameter sharing. Weight matrices remain the same through the loop and they are used repeatedly, which makes RNNs extremely convenient to work with sequential data because the model size does not grow for longer inputs. Parameter sharing allows application of models to inputs of different length and enables generalization across different positions in real time \citep{goodfellow2016deep}.

As each part of the output is a function of the previous parts of the output, backpropagation for the RNNs requires recursive computations of the gradient. The so-called backpropagation through time or BPTT is rather simple in theory and allows for the RNNs to access information from many previous steps \citep{boden2002guide}. In practice though, RNNs in their simple form are subject to two big problems: exploding and vanishing gradients. As we compute gradients recursively, they may become either very small or very large, which leads to a complete loss of information about long-term dependencies. To avoid these problems, gated RNNs were developed and accumulation of information about specific features over a long duration became possible. The two most popular types of gated RNNs, which are widely used in modern NLP, are Long Short-Term Memory models (LSTM) and Gated Recurrent Units (GRU) \citep{goodfellow2016deep}.

Over last couple of years, various extentions of RNNs were developed which resulted in their wide application in different fields of NLP. Encoder-Decoder architectures aim to map input sequences to output sequences of different length and therefore are often applied in machine translation and question answering \citep{sutskever2014sequence}. Bidirectional RNNs feed sequences in their original as well as reverse order because the prediction may depend on the future context, too \citep{schuster1997bidirectional}. Besides classical tasks as document classification and sentiment analysis, more complicated challenges such as machine translation, part-of-speech tagging or speech recognition can be solved nowadays with the help of advanced versions of RNNs.

\hypertarget{convolutional-neural-networks}{%
\section{Convolutional Neural Networks}\label{convolutional-neural-networks}}

Throughout machine learning or deep learning algorithms, no one algorithm is only applicable to a certain field. Most algorithms that have achieved significant results in a certain field can still achieve very good results in other fields after slight modification. We know that convolutional neural networks (CNN) are widely used in computer vision. For instance, a remarkable CNN model called AlexNet achieved a top-5 error of 15.3\% in the ImageNet 2012 Challenge on 30 September 2012 (see \citet{Krizhevsky2012ImageNetCW}). Subsequently, a majority of models submitted by ImageNet teams from around 2014 are based on CNN. After the convolutional neural network achieved great results in the field of images, some researchers began to explore convolutional neural networks in the field of natural language processing (NLP). Early research was restricted to sentence classification tasks, CNN-based models have achieved very significant effects as well, which also shows that CNN is applicable to some problems in the field of NLP. Similarly, as mentioned before, one of the most common deep learning models in NLP is the recurrent neural network (RNN), which is a kind of sequence learning model and this model is also widely applied in the field of speech processing. In fact, some researchers have tried to implement RNN models in the field of image processing, such as (\citet{Visin2015ReNetAR}). It can be seen that the application of CNN or RNN is not restricted to a specific field.

As the Word2vec algorithm from \citet{Mikolov.2013c} and the GloVe algorithm from \citet{Pennington.2014} for calculating word embeddings became more and more popular, applying this technique as a model input has become one of the most common text processing methods. Simultaneously, significant effectiveness of CNN in the field of computer vision has been proven. As a result, utilizing CNN to word embedding matrices and automatically extract features to handle NLP tasks appeared inevitable.

The following figure \ref{fig:figintro1} illustrates a basic structure of CNN, which is composed of multiple layers. Many of these layers are described and developed with some technical detail in later chapters of this paper.

\begin{figure}[h]
\includegraphics[width=1.05\linewidth]{figures/01-00-deep-learning-for-nlp/01_03_basic_structure} \caption{Basic structure of CNN. Source: Own figure.}\label{fig:figintro1}
\end{figure}

It is obvious that neural networks consist of a group of multiple neurons (or perceptron) at each layer, which uses to simulate the structure and behavior of biological nervous systems, and each neuron can be considered as logistic regression.

\begin{figure}[h]

{\centering \includegraphics[width=0.5\linewidth]{figures/01-00-deep-learning-for-nlp/01_03_Comparison_Fully_Partial} 

}

\caption{Comparison between the fully-connected and partial connected architecture. Source: Own figure.}\label{fig:figintro2}
\end{figure}

The structure of CNN is different compared with traditional neural networks as illustrated in figure \ref{fig:figintro2}. In traditional neural networks structure, the connections between neurons are fully connected. To be more specific, all of the neurons in the layer \(m-1\) are connected to each neuron in the layer \(m\), but CNN sets up spatially-local correlation by performing a local connectivity pattern between neurons of neighboring layers, which means that the neurons in the layer \(m-1\) are partially connected to the neurons in the layer \(m\). In addition to this, the left picture presents a schematic diagram of fully-connected architecture. It can be seen from the figure that there are many edges between neurons in the previous layer to the next layer, and each edge has parameters. The right side is a local connection, which shows that there are relatively few edges compared with fully-connected architecture and the number of visible parameters has significantly decreased.

\newpage

A detailed description of CNN will be presented in the later chapters and the basic architecture of if will be further explored. Subsection 5.1 gives an overview of CNN model depends upon (\citet{Kim2014ConvolutionalNN}). At its foundation, it is also necessary to explain various connected layers, including the convolutional layer, pooling layer, and so on. In 5.2 and later subsections, some practical applications of CNN in the field of NLP will be further explored, and these applications are based on different CNN architecture at diverse level, for example, exploring the model performance at character-level on text classification research (see \citet{Zhang2015CharacterlevelCN}) and based on multiple data sets to detect the Very Deep Convolutional Networks (VD-CNN) for text classification (see \citet{Schwenk2017VeryDC}).

\hypertarget{foundationsapplications-of-modern-nlp}{%
\chapter{Foundations/Applications of Modern NLP}\label{foundationsapplications-of-modern-nlp}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{recurrent-neural-networks-and-their-applications-in-nlp}{%
\chapter{Recurrent neural networks and their applications in NLP}\label{recurrent-neural-networks-and-their-applications-in-nlp}}

\emph{Author: Marianna Plesiak}

\emph{Supervisor: Christian Heumann}

\hypertarget{rnns}{%
\section{RNNs}\label{rnns}}

\hypertarget{structure}{%
\subsection{Structure}\label{structure}}

Recurrent neural networks allow to relax the condition of non-cyclical connections in the classical feedforward neural networks which were described in the previous chapter. This means, while simple MLP (multilayer perceptrons) can only map from input to output vectors, RNNs allow the entire history of previous inputs to influence the network output. (@ Graves 2012)

The repetitive structure of RNNs can be visualised with help of an \textbf{unfolded} computational graph(see @ figure 1).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{/home/travis/build/compstat-lmu/seminar_nlp_ss20/figures/01-02-rnns-and-their-applications-in-nlp/02_unfolded_graph} 

}

\caption{Unfolded computatinal graph of a RNN.}\label{fig:pressure}
\end{figure}

Each node is associated with a network layer at a particular time instance. Inputs \(x^{(t)}\) must be encoded as numeric vectors, for instance word embeddings or one-hot encoded vectors, see previous chapter. Reccurently connected vectors \(h\) are called hidden states and represent the outputs of the hidden layers. At time \(t\), a hidden state \(h^{(t)}\) combines information from the previous hidden state \(h^{(t-1)}\) as well as the new input \(x^{(t)}\) and passes it through to the next hidden layer. Obviously, such an architecture requires the initialization of \(h^{(0)}\) since there is no memory at the very beginning of the sequence processing. Given the hidden sequences, output vectors \(\hat{y}^{(t)}\) are used to build the predictive distribution \(Pr(x^{(t+1)}|y^{(t)})\) for the next input. Since the predictions are created at each time instance \(t\), the total output has a shape {[}time\_steps, output\_features{]}. However in some cases this is not needed, for example in sentiment analysis the last output of the loop is sufficient because it contains the entire information about the sequence. (@ Chollet) (@ Graves 2012)

The unfolded recurrence can be formalized as following:

\begin{align}
h^{(t)} & = g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)}) \\
& = f(h^{(t-1)},x^{(t)}| \theta)  \label{eq:recurrent}
\end{align}

After \(t\) steps, the function \(g^{(t)}\) takes into account the whole sequence \((x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)})\) and produces the hidden state \(h^{(t)}\). Because of its cyclical structure, \(g^{(t)}\) can be factorized into the repeated application of a same function \(f\). This function can be considered a universal model which is shared across all time steps and is generalized for all sequence lengths. This is called parameter sharing and is illustrated in the unfolded computational graph as a reuse of the same matrices \(U\), \(W\) and \(V\) through the entire network. (@ Goodfellow book)

Update equations:

\hypertarget{backpropagation-and-drawbacks}{%
\subsection{Backpropagation and Drawbacks}\label{backpropagation-and-drawbacks}}

\hypertarget{gated-rnns}{%
\section{Gated RNNs}\label{gated-rnns}}

\hypertarget{lstm}{%
\subsection{LSTM}\label{lstm}}

\hypertarget{gru}{%
\subsection{GRU}\label{gru}}

\hypertarget{versions}{%
\section{Versions}\label{versions}}

\hypertarget{bidirectional-and-deep-rnns}{%
\subsection{Bidirectional and Deep RNNs}\label{bidirectional-and-deep-rnns}}

\hypertarget{applications}{%
\subsection{Applications}\label{applications}}

\hypertarget{convolutional-neural-networks-and-their-applications-in-nlp}{%
\chapter{Convolutional neural networks and their applications in NLP}\label{convolutional-neural-networks-and-their-applications-in-nlp}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

Test test

\hypertarget{introduction-transfer-learning-for-nlp}{%
\chapter{Introduction: Transfer Learning for NLP}\label{introduction-transfer-learning-for-nlp}}

\emph{Authors: Carolin Becker, Joshua Wagner, Bailan He}

\emph{Supervisor: Matthias Aßenmacher}

As discussed in the previous chapters, natural language processing (NLP) is a very powerful tool in the field of processing human language. In recent years, there have been many proceedings and improvements in NLP to the state-of-art models like BERT. A decisive further development in the past was the way to transfer learning, but also self-attention.

In the next three chapters, various NLP models will be presented, which will be taken to a new level with the help of transfer learning in a first and a second step with self-attention and transformer-based model architectures. To understand the models in the next chapters, the idea and advantages of transfer learning are introduced. Additionally, the concept of self-attention and an overview over the most important models will be established

\hypertarget{what-is-transfer-learning}{%
\section{What is Transfer Learning?}\label{what-is-transfer-learning}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-00-transfer-learning-for-nlp/compare-classical-transferlearning-ml} 

}

\caption{Classic Machine Learning and Transfer Learning}\label{fig:ch02-figure01}
\end{figure}



In figure \ref{fig:ch02-figure01} the difference between classical machine learning and transfer learning is shown.

For classical machine learning a model is trained for every special task or domain.
Transfer learning allows us to deal with the learning of a task by using the existing labeled data of some related tasks or domains. Tasks are the objective of the model. e.g.~the sentiment of a sentence, whereas the domain is where data comes from. e.g.~all sentences are selected from Reddit. In the example above, knowledge gained in task A for source domain A is stored and applied to the problem of interest (domain B).

Generally, transfer learning has several advantages over classical machine learning: saving time for model training, mostly better performance, and not a need for a lot of training data in the target domain.

It is an especially important topic in NLP problems, as there is a lot of knowledge about many texts, but normally the training data only contains a small piece of it. A classical NLP model captures and learns a variety of linguistic phenomena, such as long-term dependencies and negation, from a large-scale corpus. This knowledge can be transferred to initialize another model to perform well on a specific NLP task, such as sentiment analysis. \citep{evolutiontransferlearning}

\hypertarget{self-attention}{%
\section{(Self-)attention}\label{self-attention}}

The most common models for language modeling and machine translation were, and still are to some extent, recurrent neural networks with long short-term memory \citep{hochreiter1997long} or gated recurrent units \citep{gru}. These models commonly use an encoder and a decoder archictecture. Advanced models use attention, either based on Bahdanau's attention \citep{bahdanau2014neural} or Loung's attention \citep{luong2015effective}.

\citet{vaswani2017attention} introduced a new form of attention, self-attention, and with it a new class of models, the \textit{Transformers}. A Transformer still consists of the typical encoder-decoder setup but uses a novel new architecture for both. The encoder consists of 6 Layers with 2 sublayers each. The newly developed self-attention in the first sublayer allows a transformer model to process all input words at once and model the relationships between all words in a sentence. This allows transformers to model long-range dependencies in a sentence faster than RNN and CNN based models. The speed improvement and the fact that ``individual attention heads clearly learn to perform different tasks'' \citet{vaswani2017attention} lead to the eventual development of \textbf{B}idirectional \textbf{E}ncoder \textbf{R}epresentations from \textbf{T}ransformers by \citet{bert}. \textbf{BERT} and its successors are, at the time of writing, the state-of-the-art models used for transfer learning in NLP. The concepts attention and self-attention will be further discussed in the \protect\hyperlink{Attention-and-self-Attention-for-nlp}{\textbf{``Chapter 9: Attention and Self-Attention for NLP''}}.

\hypertarget{overview-over-important-nlp-models}{%
\section{Overview over important NLP models}\label{overview-over-important-nlp-models}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-00-transfer-learning-for-nlp/overview-tranferlearning} 

}

\caption{Overview of the most important models for transfer learning}\label{fig:ch02-figure02}
\end{figure}



The models in figure \ref{fig:ch02-figure02} will be presented in the next three chapters.

First, the two model architectures ELMo and ULMFit will be presented, which are mainly based on transfer learning and LSTMs, in \protect\hyperlink{Transfer-Learning-for-NLP-I}{\textbf{Chapter 8: ``Transfer Learning for NLP I''}}:

\begin{itemize}
\item
  \textbf{ELMo} (Embeddings from Language Models) first published in \citet{elmopaper} uses a deep, bi-directional LSTM model to create word representations. This method goes beyond traditional embedding methods, as it analyses the words within the context
\item
  \textbf{ULMFiT} (Universal Language Model Fine-tuning for Text Classification) consists of three steps: first, there is a general pre-training of the LM on a general domain (like WikiText-103 dataset), second, the LM is finetuned on the target task and the last step is the multilabel classifier fine tuning where the model provides a status for every input sentence.
\end{itemize}

In the \protect\hyperlink{Transfer-Learning-for-NLP-II}{\textbf{``Chapter 10: Transfer Learning for NLP II''}} models like BERT, GTP2 and XLNet will be introduced as they include transfer learning in combination with self-attention:

\begin{itemize}
\item
  \textbf{BERT} (Bidirectional Encoder Representations from Transformers \citet{bert}) is published by researchers at Google AI Language group.
  It is regarded as a milestone in the NLP community by proposing a bidirectional Language model based on Transformer. BERT uses the Transformer Encoder as the structure of the pre-train model and addresses the unidirectional constraints by proposing new pre-training objectives: the ``masked language model''(MLM) and a ``next sentence prediction''(NSP) task. BERT advances state-of-the-art performance for eleven NLP tasks and its improved variants \textbf{Albert} \citet{lan2019albert} and \textbf{Roberta} \citet{liu2019roberta} also reach great success.
\item
  \textbf{GPT2} (Generative Pre-Training-2, \citet{radford2019gpt2}) is proposed by researchers at OpenAI. GPT-2 is a tremendous multilayer Transformer Decoder and the largest version includes 1.543 billion parameters. Researchers create a new dataset ``WebText'' to train GPT-2 and it achieves state-of-the-art results on 7 out of 8 tested datasets in a zero-shot setting but still underfits ``WebText''.
\item
  \textbf{XLNet} is proposed by researchers at Google Brain and CMU\citep{yang2019xlnet}. It borrows ideas from autoregressive language modeling (e.g., Transformer-XL \citet{dai2019transformer}) and autoencoding (e.g., BERT) while avoiding their limitations. By using a permutation operation during training, bidirectional contexts can be captured and make it a generalized order-aware autoregressive language model. Empirically, XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.
\end{itemize}

\hypertarget{transfer-learning-for-nlp-i}{%
\chapter{Transfer Learning for NLP I}\label{transfer-learning-for-nlp-i}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{attention-and-self-attention-for-nlp}{%
\chapter{Attention and Self-Attention for NLP}\label{attention-and-self-attention-for-nlp}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{transfer-learning-for-nlp-ii}{%
\chapter{Transfer Learning for NLP II}\label{transfer-learning-for-nlp-ii}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{introduction-resources-for-nlp}{%
\chapter{Introduction: Resources for NLP}\label{introduction-resources-for-nlp}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{resources-and-benchmarks-for-nlp}{%
\chapter{Resources and Benchmarks for NLP}\label{resources-and-benchmarks-for-nlp}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{software-for-nlp-the-huggingface-transformers-module}{%
\chapter{Software for NLP: The huggingface transformers module}\label{software-for-nlp-the-huggingface-transformers-module}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{use-bases-for-nlp}{%
\chapter{Use-Bases for NLP}\label{use-bases-for-nlp}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{use-case-i}{%
\chapter{Use-Case I}\label{use-case-i}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{use-case-ii}{%
\chapter{Use-Case II}\label{use-case-ii}}

\emph{Authors: Author 1, Author 2}

\emph{Supervisor: Supervisor}

\hypertarget{acknowledgements}{%
\chapter{Acknowledgements}\label{acknowledgements}}

The most important contributions are from the students themselves.
The success of such projects highly depends on the students.
And this book is a success, so thanks a lot to all the authors!
The other important role is the supervisor.
Thanks to all the supervisors who participated!
Special thanks to \href{https://www.misoda.statistik.uni-muenchen.de/personen/professoren/heumann/index.html}{Christian Heumann} who enabled us to conduct the seminar in such an experimental way, supported us and gave valuable feedback for the seminar structure.
Thanks a lot as well to the entire \href{https://www.statistik.uni-muenchen.de/}{Department of Statistics} and the \href{http://www.en.uni-muenchen.de/index.html}{LMU Munich} for the infrastructure.

The authors of this work take full responsibilities for its content.

\bibliography{book.bib,packages.bib}

\backmatter
\printindex

\end{document}
