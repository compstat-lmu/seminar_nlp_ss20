\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Modern Approaches in Natural Language Processing},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Modern Approaches in Natural Language Processing}
\author{}
\date{\vspace{-2.5em}2020-09-01}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{0}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


In the last few years, there have been several breakthroughs concerning the methodologies used in Natural Language Processing (NLP). These breakthroughs originate from both new modeling frameworks as well as from improvements in the availability of computational and lexical resources.

In this seminar booklet, we are reviewing these frameworks starting with a methodology that can be seen as the beginning of modern NLP: \emph{Word Embeddings}.

We will further discuss the integration of embeddings into end-to-end trainable approaches, namely convolutional and recurrent neural networks.

The second chapter of this booklet is going to cover the impact of Attention-based models, since they are the foundation of most of the recent state-of-the-art architectures. Consequently, we will also spend a large part of this chapter on the use of transfer learning approaches in modern NLP.

To cap it all of, the last chapter will be abour pre-training resources and benchmark tasks/data sets for evaluating state-of-the-art models followed by an illustrative use case on Natural Language Generation.

This book is the outcome of the seminar ``Modern Approaches in Natural Language Processing'' which took place in the summer term 2020 at the Department of Statistics, LMU Munich.

\begin{figure}
\centering
\includegraphics{figures/by-nc-sa.png}
\caption{Creative Commons License}
\end{figure}

This book is licensed under the \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}.

\mainmatter

\hypertarget{foreword}{%
\chapter*{Foreword}\label{foreword}}


\emph{Author: Matthias Aßenmacher}

This book is the result of an experiment in university teaching. We were inspired by a group of other PhD Students around Christoph Molnar, who conducted another \href{https://compstat-lmu.github.io/iml_methods_limitations/}{seminar on Interpretable Machine Learning} in this format.
Instead of letting every student work on a seminar paper, which more or less isolated from the other students, we wanted to foster collaboration between the students and enable them to produce a tangible outout (that isn't written to spend the rest of its time in (digital) drawers).
In the summer term 2020, some Statistics \& Data Science Master students signed up for our seminar entitled ``Modern Approaches in Natural Language Processing'' and had (before kick-off meeting) no idea what they had signed up for: Having written an entire book by the end of the semester.

We were bound by the examination rules for conducting the seminar, but otherwise we could deviate from the traditional format.
We deviated in several ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each student project is a chapter of this booklet, linked contentwise to other chapers since there's partly a large overlap between the topics.
\item
  We gave challenges to the students, instead of papers. The challenge was to investigate a specific impactful recent model or method from the field of NLP.
\item
  We designed the work to live beyond the seminar.
\item
  We emphasized collaboration. Students wrote the introduction to chapters in teams and reviewed each others individual texts.
\end{enumerate}

\hypertarget{technical-setup}{%
\section*{Technical Setup}\label{technical-setup}}


The book chapters are written in the Markdown language.
The simulations, data examples and visualizations were created with R \citep{rlang}.
To combine R-code and Markdown, we used rmarkdown.
The book was compiled with the bookdown package.
We collaborated using git and github.
For details, head over to the \href{https://github.com/compstat-lmu/seminar_nlp_ss20}{book's repository}.

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\emph{Author: Xiao-Yin To}

\emph{Supervisor: Daniel Schalk, Matthias Aßenmacher}

Over the course of the past decades the importance and utilization of artificial intelligence technology has continuously gained traction. In the present times, it is already inextricably linked with most of the surroundings that constitute the human shaped environment. Consequently, a myriad of sectors such as commerce, research and development, information services, engineering, social services, and medical science have already been irreversibly impacted by the capabilities of artificial intelligence.
There are three major fields of artificial intelligence that comprise the technology: speech recognition, computer vision, and natural language processing (see \citet{yeung2020}).
In this book we will take a closer look at the modern approaches in natural language processing (NLP).

\hypertarget{history-and-development}{%
\section{History and development}\label{history-and-development}}

The history of artificial intelligence and NLP dates back to the 18th century, where well-known philosophers such as Leibniz, Spinoza, Hobbes, Locke, Kant, and Hume as well as scientists such as La Mettrie and Hartley tried to formulate laws of thought (see \citet{mccorduck2004}).

However, the first steps of development started in the 20th century. Alan \citet{turing1937} was the first to propose an abstract Universal Computing Machine and became one of the most defining scientists who shaped the path of the scientific development of artificial intelligence in the following years. He further developed these ideas in his work ``Intelligent machinery'' (see \citet{turing1948}) and ``Computing Machinery And Intelligence'' (see \citet{turing1950}).
In 1949, Warren Weaver proposed that ``given that all humans are the same (inspite of speaking a variety of languages), a document in one language could be viewed as having been written in code. Once this code was broken, it would be possible to output the document in another language. From this point of view, German was English in code'' (see \citet{weaver1949}).

In the post World War II era, numerous German documents needed to be translated into the English language. Due to the sheer number of documents, an automatic decryption system was required in order to ensure time efficiency regarding the herculean task, which further accelerated the research concerning NLP. The first teams were comprised of numerous bilingual programmers. The idea behind this was that the knowledge of multiple languages might facilitate the process of creating programs, which could understand languages and their structures, and would subsequently be able to translate texts. While working on the first NLP programs the main difficulty that crystallized related to the complexity and irregularity of many languages (see \citet{hancox1996}).
Beginning in the 1950s, linguists and machine learning teams congregated and introduced new ideas. During this time period, Georgetown University and International Business Machines Corporation (IBM) published the Georgetown Experiment, the first public demonstration of machine translation, which included the first fully automatic translation, being able to translate more than sixty Russian sentences into English (see \citet{hutchins2005}).
Moreover, Noam Chomsky, one of the most important and influential scientists in linguistics, introduced the idea of Generative Grammar, which describes syntactic structures based on rules (see \citet{chomsky1957}). The most successful NLP systems that were developed back then were the first chatbot ELIZA (see \citet{weizenbaum1966}), STUDENT (see \citet{bobrow1964}), and SHRDLU, a language program that allowed user interaction with a block world (see \citet{winograd1972}). As the resources that were available for computing in the past were extremely undeveloped -- access to computers was restricted, the machines were still really slow, storage was limited, and there were no suitable higher-level programming languages -- the creation of such programs was considerably more difficult. The fact that any progress in this field was attained makes the achievements of those scientists all the more remarkable.

In addition to limited resources, researchers encountered the problem that research regarding the development of NLP software came with high costs: By the mid-1960s, machine translation research expenses amounted to 20 million USD, which were paid by U.S. government funding. Those two obstacles, resource limitations along with high costs, were the main reason for the slow advancement of research in this area. The history of NLP reached its lowest point, when in 1966 the Automatic Language Processing Advisory Committee evaluated the results that were attained through the funding and reported, that ``there had been no machine translation of general scientific text, and none is in immediate prospect'' (\citet{alpac1966}). This report caused U.S. funding to be discontinued, which is the reason why in the following decade the quantity of NLP in scientific literature decreased enormously. Nevertheless, compelling developments such as Augmented Transition Networks, which aid in the analysis of sentence structures, Case Grammar, which facilitates comprehension of linguistic structures by using the link between different components of sentences, and Semantic Representation, which signifies an abstract language in which meanings can be represented, originated in that time (see \citet{hancox1996}).

In the 1980s, the so-called \emph{Statistical Revolution} took place. Prior to that, NLP was a primarily ``grammar-based approach'', which denotes that systems were created by hand-coding rules and parameters. Via the statistical revolution, the empirical ``statistical approach'' was introduced (see \citet{johnson2009}) and consequently ``NLP was characterized by the exploitation of data corpora and of (shallow) machine learning, statistical or otherwise, to make use of such data'' (see \citet{deng2018}). This approach has dominated NLP ever since, as the amount of machine-readable data and computational power has continuously expanded. Since simple Machine Learning techniques are often not sufficient for creating NLP applications that can fulfil the requirements of real-life tasks, nowadays most of the methods are based on Deep Learning designs (see \citet{deng2018}).

\hypertarget{statistical-background}{%
\section{Statistical Background}\label{statistical-background}}

Ever since the Statistical Revolution, many challenging aspects could be tackled using statistical approaches and artificial intelligence. Statistics shaped a substantial part of the path of NLP, conjointly with fundamental knowledge of linguistics. The statistical approaches used in this booklet presuppose mathematical foundations such as elementary probability theory and essential information theory. In order facilitate the comprehension of the approaches explained in the later chapters, now some of the basic schemes that lie the foundations to those modern approaches in NLP will be introduced.

Human language courses usually consist of two elementary parts: vocabulary and grammar. The language skills are often measured by the number of words a person knows, while grammar allows using the words and form sentences correctly. Further, NLP systems basically consist of learning and understanding words as well as recognizing the patterns in which they occur.

The first step is characterized by the recognition and comprehension of words. One difficulty arising when trying to understand words is that many words possess multiple meanings. It might be challenging to ascertain which of the meanings is implied. For word sense, disambiguation methods such as bag of words models or Bayesian classification can be used, which inspect the words around the ambiguous word.
Bag of words models consider the dependence of words in a so-called bag, a vector of words that appear in a sentence, so co-occurrences can be learned without understanding grammar.
By applying the Bayesian decision rule, the meaning of the word will be decided by choosing the meaning with the highest conditional probability while minimizing the probability of error.
Another difficulty is that in many languages words exist, which do not (only) have a meaning themselves, but also possess combined meanings in a collocation, an expression consisting of two or more words. One way for NLP systems to implement this is by using basic statistics such as frequency, mean and variance, and hypothesis testing. If two or more words often occur together in a sentence, it may be concluded that these words together possess a special function in this sentence and cannot be explained by the combination of their respective meanings. Mean and variance can help finding the affiliation between words, which do not always appear in the same structure or with the same distance within a phrase, by calculating the mean distance between words in a sentence and the variance of this distance during training, in order to enable correct classification of these words into a collocation in later applications of the model.

The second step is understanding not only the words themselves, but also their meaning given their contexts. As an example, Markov Models can be used for the classification of texts depending on the surrounding context as well as grammatical structure finding. A Markov Model is a sequence classifier which assigns a sequence of classes to a sequence of observations, enabling the classification of texts depending on the class of the previous texts. Markov Models can be used for designing a part-of-speech tagger. Part-of-speech tagging allows assigning words to their part-of-speech in a sentence, allowing for the comprehension of a sentence without requiring complete understanding (see \citet{manning2008}).

Combining the understanding of words and grammar, many NLP problems can be solved. In this booklet more advanced methods, of which some are based on the described basic methods, will be introduced.

\hypertarget{outline-of-the-booklet}{%
\section{Outline of the Booklet}\label{outline-of-the-booklet}}

This booklet circumstantiate modern approaches used for natural language processing, such as Deep Learning and Transfer Learning. Moreover, the resources that are available for the training of NLP tasks will be investigated and a use-case where NLP will be applied for generation of natural language will be shown.

For the analysis and comprehension of human language, NLP programs need to extract information from words and sentences. As neural networks and other machine learning algorithms require a numeric input for training, word embeddings, using dense vector representations for words, are applied. Those are usually learned by neural networks with multiple hidden layers, deep neural networks. In order to solve easy tasks, simple structured neural networks can be applied. In order to overcome the limitations of those simple structures, recurrent and convolutional neural networks are utilized. Thereby, recurrent neural networks are used for models that can learn sequences without pre-defined optimal fixed dimensions while convolutional neural networks are applied for sentence classification.
Chapter \href{Chapter\%20Introduction:\%20Deep\%20Learning\%20for\%20NLP}{2} of the booklet gives a short introduction to Deep Learning in NLP. The Foundations and Applications of Modern NLP will be described in chapter \href{Foundations/Applications\%20of\%20Modern\%20NLP}{3}. In chapter \href{Recurrent\%20neural\%20networks\%20and\%20their\%20applications\%20in\%20NLP}{4} and \href{Convolutional\%20neural\%20networks\%20and\%20their\%20applications\%20in\%20NLP}{5} recurrent neural networks and convolutional neural networks and their applications in NLP will be explained and discussed.

Transfer learning is an alternative to learning models for every task or domain. Here, existing labeled data of related tasks or domains can be used for training a model and applying it onto the task or domain of interest. The advantage of this approach is that there is no need for a long training in the target domain, and that time for training of the model can be saved, while still resulting in a (mostly) better performance. A concept used in transfer learning is Attention, which enables the decoder to attend to the entire input sequence, or Self-Attention, which allows a transformer model to process all input words at once and model the relationships between all words in a sentence, which renders fast modelling of long-range dependencies in a sentence possible.
The concepts of transfer learning will be briefly introduced in chapter \href{Introduction:\%20Transfer\%20Learning\%20for\%20NLP}{6} of the booklet. Chapter \href{Transfer\%20Learning\%20for\%20NLP\%20I}{7} will describe transfer learning and LSTMs by presenting the models ELMo, ULMFiT, and GPT. Chapter \href{Attention\%20and\%20Self-Attention\%20for\%20NLP}{8} will illustrate the concepts of Attention and Self-Attention for NLP in detail. In chapter \href{Transfer\%20Learning\%20for\%20NLP\%20II}{9}, transfer learning is combined with Self-Attention, introducing the models BERT, GTP2, and XLNet.

For NLP modelling, resources are needed. In order to find the best model for a task, benchmarks can be used. For comparing different models within a benchmark experiment, metrics such as exact match, F\beta score, perplexity, or bilingual evaluation understudy, or accuracy, are required.
Chapter \href{Introduction:\%20Resources\%20for\%20NLP}{10} of the booklet provides a brief introduction to the resources for NLP and the manner in which they are used. Chapter \href{Resources\%20and\%20Benchmarks\%20for\%20NLP}{11} will explain the different metrics, give an insight into the benchmark datasets SQuAD, CoQa, GLUE and SuperGLUE, AQuA-Rat, SNLI, and LAMBADA as well as pre-trained models and databases where resources can be found, such as ``Papers with Code'' and ``The Big Bad NLP Database''.

In the \href{Natural\%20Language\%20Generation}{last} chapter of the booklet, the generative NLP process Natural Language Generation, thus the generation of understandable text in a human language, is presented. Therefore, different algorithms will be described and chatbots as well as image captioning will be shown for illustrating the possibilities of application.

This introduction to the various methods in NLP functions as the foundation for the following deliberations. The individual chapters of the booklet will present modern methods in NLP and provide a more detailled discussion of the potential as well as the limitations along with various examples.

\hypertarget{introduction-deep-learning-for-nlp}{%
\chapter{Introduction: Deep Learning for NLP}\label{introduction-deep-learning-for-nlp}}

\emph{Authors: Viktoria Szabo, Marianna Plesiak, Rui Yang}

\emph{Supervisor: Prof.~Dr.~Christian Heumann}

\hypertarget{word-embeddings-and-neural-network-language-models}{%
\section{Word Embeddings and Neural Network Language Models}\label{word-embeddings-and-neural-network-language-models}}

In natural language processing computers try to analyze and understand human language for the purpose of performing useful tasks. Therefore, they extract relevant information from words and sentences. But how exactly are they doing this? After the first wave of rationalist approaches with handwritten rules didn't work out too well, neural networks were introduced to find those rules by themselves (see \citet{Bengio.2003}). But neural networks and other machine learning algorithms cannot handle non-numeric input, so we have to find a way to convert the text we want to analyze into numbers.
There are a lot of possibilities to do that. Two simple approaches would be labeling each word with a number (One-Hot Encoding, figure \ref{fig:onehot-bow}) or counting the frequency of words in different text fragments (Bag-of-Words, figure \ref{fig:onehot-bow}). Both methods result in high-dimensional, sparse (mostly zero) data. And there is another major drawback using such kind of data as input. It does not convey any similarities between words. The word ``cat'' would be as similar to the word ``tiger'' as to ``car''. That means the model cannot reuse information it already learned about cats for the much rarer word tiger. This will usually lead to poor model performance and is called a lack of generalization power.

\begin{figure}
\includegraphics[width=0.5\linewidth]{figures/01-00-deep-learning-for-nlp/01-01_one-hot} \includegraphics[width=0.5\linewidth]{figures/01-00-deep-learning-for-nlp/01-01_bow} \caption{One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure.}\label{fig:onehot-bow}
\end{figure}

The solution to this problem is word embedding. Word embeddings use dense vector representations for words. That means they map each word to a continuous vector with n dimensions. The distance in the vector space denotes semantic (dis)similarity. These word embeddings are usually learned by neural networks, either within the final model in an additional layer or in its own model. Once learned they can be reused across different tasks. Practically all NLP projects these days build upon word embeddings, since they have a lot of advantages compared to the aforementioned representations.
The basic idea behind learning word embeddings is the so called ``distributional hypothesis'' (see \citet{Harris.1954}). It states that words that occur in the same contexts tend to have similar meanings. The two best known approaches for calculating word embeddings are Word2vec from \citet{Mikolov.2013c} and GloVE from \citet{Pennington.2014}. The Word2vec models (Continous Bag-Of-Words (CBOW) and Skip-gram) try to predict a target word given his context or context words given a target word using a simple feed-forward neural network. In contrast to these models GloVe not only uses the local context windows, but also incorporates global word co-occurrence counts.
As mentioned, a lot of approaches use neural networks to learn word embeddings. A simple feed-forward network with fully connected layers for learning such embeddings while predicting the next word for a given context is shown in figure \ref{fig:nnlm}. In this example the word embeddings are first learnt in a projection layer and are then used in two hidden layers to model the probability distribution over all words in the vocabulary. With this distribution one can predict the target word. This simple structure can be good enough for some tasks but it also has a lot of limitations. Therefore, recurrent and convolutional networks are used to overcome the limitations of a simple neural network.

\begin{figure}
\includegraphics[width=1\linewidth]{figures/01-00-deep-learning-for-nlp/01-01_nnlm} \caption{Feed-forward Neural Network. Source: Own figure based on Bengio et al. 2013.}\label{fig:nnlm}
\end{figure}

\hypertarget{recurrent-neural-networks}{%
\section{Recurrent Neural Networks}\label{recurrent-neural-networks}}

The main drawback of feedforward neural networks is that they assume a fixed length of input and output vectors which is known in advance. But for many natural language problems such as machine translation and speech recognition it is impossible to define optimal fixed dimensions a-priori. Other models that map a sequence of words to another sequence of words are needed \citep{sutskever2014sequence}. Recurrent neural networks or RNNs are a special family of neural networks which were explicitely developed for modeling sequential data like text. RNNs process a sequence of words or letters \(x^{(1)}, ..., x^{(t)}\) by going through its elements one by one and capturing information based on the previous elements. This information is stored in hidden states \(h^{(t)}\) as the network memory. Core idea is rather simple: we start with a zero vector as a hidden state (because there is no memory yet), process the current state at time \(t\) as well as the output from the previous hidden state, and give the result as an input to the next iteration \citep{goodfellow2016deep}.

Basically, a simple RNN is a for-loop that reuses the values which are calculated in the previous iteration \citep{chollet2018deep}. An unfolded computational graph (figure \ref{fig:01-00-unfolded}) can display the structure of a classical RNN. The gray square on the left represents a delay of one time step and the arrows on the right express the flow of information in time \citep{goodfellow2016deep}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/01-00-deep-learning-for-nlp/01_02_unfolded_graph} 

}

\caption{Right: Circuit diagram (left) and unfolded computational graph (right) of a simple RNN. Source: Own figure.}\label{fig:01-00-unfolded}
\end{figure}

One particular reason why recurrent networks have become such a powerful technique in processing sequential data is parameter sharing. Weight matrices remain the same through the loop and they are used repeatedly, which makes RNNs extremely convenient to work with sequential data because the model size does not grow for longer inputs. Parameter sharing allows application of models to inputs of different length and enables generalization across different positions in real time \citep{goodfellow2016deep}.

As each part of the output is a function of the previous parts of the output, backpropagation for the RNNs requires recursive computations of the gradient. The so-called backpropagation through time or BPTT is rather simple in theory and allows for the RNNs to access information from many previous steps \citep{boden2002guide}. In practice though, RNNs in their simple form are subject to two big problems: exploding and vanishing gradients. As the gradients are computed recursively, they may become either very small or very large, which leads to a complete loss of information about long-term dependencies. To avoid these problems, gated RNNs were developed and accumulation of information about specific features over a long duration became possible. The two most popular types of gated RNNs, which are widely used in modern NLP, are Long Short-Term Memory models (LSTMs, presented by \citet{hochreiter1997long}) and Gated Recurrent Units (GRUs, presented by \citet{cho2014learning}).

Over last couple of years, various extentions of RNNs were developed which resulted in their wide application in different fields of NLP. Encoder-Decoder architectures aim to map input sequences to output sequences of different length and therefore are often applied in machine translation and question answering \citep{sutskever2014sequence}. Bidirectional RNNs feed sequences in their original as well as reverse order because the prediction may depend on the future context, too \citep{schuster1997bidirectional}. Besides classical tasks as document classification and sentiment analysis, more complicated challenges such as machine translation, part-of-speech tagging or speech recognition can be solved nowadays with the help of advanced versions of RNNs.

\hypertarget{convolutional-neural-networks}{%
\section{Convolutional Neural Networks}\label{convolutional-neural-networks}}

Throughout machine learning or deep learning algorithms, no one algorithm is only applicable to a certain field. Most algorithms that have achieved significant results in a certain field can still achieve very good results in other fields after slight modification. We know that convolutional neural networks (CNN) are widely used in computer vision. For instance, a remarkable CNN model called AlexNet achieved a top-5 error of 15.3\% in the ImageNet 2012 Challenge on 30 September 2012 (see \citet{Krizhevsky2012ImageNetCW}). Subsequently, a majority of models submitted by ImageNet teams from around 2014 are based on CNN. After the convolutional neural network achieved great results in the field of images, some researchers began to explore convolutional neural networks in the field of natural language processing (NLP). Early research was restricted to sentence classification tasks, CNN-based models have achieved very significant effects as well, which also shows that CNN is applicable to some problems in the field of NLP. Similarly, as mentioned before, one of the most common deep learning models in NLP is the recurrent neural network (RNN), which is a kind of sequence learning model and this model is also widely applied in the field of speech processing. In fact, some researchers have tried to implement RNN models in the field of image processing, such as (\citet{Visin2015ReNetAR}). It can be seen that the application of CNN or RNN is not restricted to a specific field.

As the Word2vec algorithm from \citet{Mikolov.2013c} and the GloVe algorithm from \citet{Pennington.2014} for calculating word embeddings became more and more popular, applying this technique as a model input has become one of the most common text processing methods. Simultaneously, significant effectiveness of CNN in the field of computer vision has been proven. As a result, utilizing CNN to word embedding matrices and automatically extract features to handle NLP tasks appeared inevitable.

The following figure \ref{fig:figintro1} illustrates a basic structure of CNN, which is composed of multiple layers. Many of these layers are described and developed with some technical detail in later chapters of this paper.

\begin{figure}[h]

{\centering \includegraphics[width=1.05\linewidth]{figures/01-00-deep-learning-for-nlp/01_03_basic_structure} 

}

\caption{Basic structure of CNN. Source: Own figure.}\label{fig:figintro1}
\end{figure}

It is obvious that neural networks consist of a group of multiple neurons (or perceptron) at each layer, which uses to simulate the structure and behavior of biological nervous systems, and each neuron can be considered as logistic regression.

\begin{figure}[h]

{\centering \includegraphics[width=0.5\linewidth]{figures/01-00-deep-learning-for-nlp/01_03_Comparison_Fully_Partial} 

}

\caption{Comparison between the fully-connected and partial connected architecture. Source: Own figure.}\label{fig:figintro2}
\end{figure}

The structure of CNN is different compared with traditional neural networks as illustrated in figure \ref{fig:figintro2}. In traditional neural networks structure, the connections between neurons are fully connected. To be more specific, all of the neurons in the layer \(m-1\) are connected to each neuron in the layer \(m\), but CNN sets up spatially-local correlation by performing a local connectivity pattern between neurons of neighboring layers, which means that the neurons in the layer \(m-1\) are partially connected to the neurons in the layer \(m\). In addition to this, the left picture presents a schematic diagram of fully-connected architecture. It can be seen from the figure that there are many edges between neurons in the previous layer to the next layer, and each edge has parameters. The right side is a local connection, which shows that there are relatively few edges compared with fully-connected architecture and the number of visible parameters has significantly decreased.

\newpage

A detailed description of CNN will be presented in the later chapters and the basic architecture of if will be further explored. Subsection 5.1 gives an overview of CNN model depends upon (\citet{Kim2014ConvolutionalNN}). At its foundation, it is also necessary to explain various connected layers, including the convolutional layer, pooling layer, and so on. In 5.2 and later subsections, some practical applications of CNN in the field of NLP will be further explored, and these applications are based on different CNN architecture at diverse level, for example, exploring the model performance at character-level on text classification research (see \citet{Zhang2015CharacterlevelCN}) and based on multiple data sets to detect the Very Deep Convolutional Networks (VD-CNN) for text classification (see \citet{Schwenk2017VeryDC}).

\hypertarget{foundationsapplications-of-modern-nlp}{%
\chapter{Foundations/Applications of Modern NLP}\label{foundationsapplications-of-modern-nlp}}

\emph{Authors: Viktoria Szabo}

\emph{Supervisor: Christian Heumann}

Word embeddings can be seen as the beginning of modern natural language processing. They are widely used in every kind of NLP task. One of the advantages is that one can download and use pretrained word embeddings. With this, it is possible to save a lot of time for training the final model. But if the task is not a standard one it is usually better to train own embeddings to get a better model performance for the specific task. In the following the evolution from sparse representations of words to dense word embeddings will be outlined in the first part. After that the calculation methods for word embeddings within a neural network language model and with word2vec and GloVe will be described. The third part shows how to improve the model performance regardless of the chosen model class based on hyperparameter tuning and system design choices and explains some model expansion to tackle problems of the aforementioned methods. The evaluation of word embeddings on different tasks and datasets is another topic which will be covered in the fourth part of this chapter. Finally some resources to download pretrained word embeddings will be presented.

\hypertarget{the-evolution-of-word-embeddings}{%
\section{The Evolution of Word Embeddings}\label{the-evolution-of-word-embeddings}}

Since computers work with numeric representations, converting the text and sentences to be analyzed into numbers is unavoidable. One-Hot Encoding and Bag-of-Words (BOW) are two simple approaches to how this could be accomplished. These methods are usually used as input for calculating more elaborate word representations called word embeddings.\\
The \textbf{One-Hot Encoding} labels each word in the vocabulary with an index. Let \(n\) be size of the vocabulary, then each word is represented by a vector with dimension \(n\). Every vector entry is zero except for the one corresponding to its index, which is set to \(1\). A sentence is represented as a matrix of shape (\(n\times n\)) where \(n\) is the number of unique words in the sentence or a document. In figure \ref{fig:onehot-bow-01-01} an example for a one-hot encoded word is shown on the left side.\\
A more elaborate approach compared to the first one is called \textbf{Bag-of-Words (BOW)} and belongs to the count-based approaches. This approach counts the occurrences and co-occurrences of all distinct words in a document or a text chunk. Each text chunk is then represented by a row in a matrix, where the columns are the words. That means that, compared to the One-Hot Encoding, this approach already incorporates some context information in sentences and text chunks. An example for this kind of representation can be seen on the right side in figure \ref{fig:onehot-bow-01-01}.

\begin{figure}
\includegraphics[width=0.5\linewidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_one-hot} \includegraphics[width=0.5\linewidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_bow} \caption{One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure.}\label{fig:onehot-bow-01-01}
\end{figure}

These approaches definitely have some \textbf{positive points} about them. They are very simple to construct, robust to changes, and it was observed that simple models trained on large amounts of data outperform complex systems trained on less data. Bag-of-Words is especially useful if the number of distinct words is small and the sequence of the words doesn't play a key role, like in sentiment analysis. Without calculating word embeddings on top of them, these approaches should only be used if there is a small number of distinct words in the document, the words are not meaningfully correlated and there is a lot of data to learn from.\\
Nevertheless, the \textbf{problems} that arise from these approaches usually outweigh the positive points. The most obvious one is that these approaches lead to very sparse input vectors, that means large vectors with relatively few non-zero values. Many machine learning models won't work well with high dimensional and sparse features (\citet{goldberg2016primer}). Neural networks in particular struggle with this type of data. And with growing vocabulary the feature size vectors also increases by the same length. So, the dimensionality of these approaches is the same as the number of different words in your text. That means estimating more parameters and therefore using exponentially more data is required to build a reasonably generalizable model. This is known as the curse of dimensionality. But these problems can be solved with dimensionality reduction methods such as Principal Component Analysis or feature selection models where less informative context words, such as \emph{the} and \emph{a} are dropped.\\
The major drawback of these methods is that there is no notion of similarity between words. That means words like \emph{cat} and \emph{tiger} are represented as similar as \emph{cat} and \emph{car}. If the words \emph{cat} and \emph{tiger} would be represented as similar words one could use the information won from the more frequent word ``cat'' for sentences in which the less frequent word \emph{tiger} appears. If the word embedding for \emph{tiger} is similar to that of \emph{cat} the network model can take a similar path instead of having to learn how to handle it completely anew.\\

To overcome these problems \textbf{word embeddings} were introduced. Word embeddings use continuous vectors to represent each word in a vocabulary. These vectors have \(n\) dimensions, usually between 100 and 500, which represent different aspects of the word. With this approach, semantic similarity can be maintained in the representation and generalization may be achieved. Through these vectors, the words are mapped to a continuous vector space, called a semantic space, where semantically similar words occur close to each other, while more dissimilar words are far from each other. Figure \ref{fig:word-embedding1} shows a simple example to convey the idea behind this approach. In this fictional example the words are represented by a two-dimensional vector, which represents the cuteness and scariness of the creatures.

\begin{figure}
\includegraphics[width=\textwidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_word_embeddings_1} \caption{Example for word embeddings with two dimensions. Source: Own figure}\label{fig:word-embedding1}
\end{figure}

If the goal is to represent higher dimensional word vectors one could use dimension reduction methods such as principal component analysis (PCA) to break down the number of dimensions into two or three and then plot the words. There is an example of this for selected country names and their capitals in figure \ref{fig:word-embedding2}.The country names all have negative values on the x-axis and the capitals all have positive values on the x-axis. Furthermore, the countries have y-axis values similar to their corresponding capitals.

\begin{figure}
\includegraphics[width=\textwidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_word_embeddings_2} \caption{Two-dimensional PCA projection of 1000-dimensional word vectors of countries and their capital cities. Source: @Mikolov.2013c}\label{fig:word-embedding2}
\end{figure}

With such word vectors even algebraic computations become possible as shown in \citet{mikolov2013linguistic}. For example, \(vector(King)-vector(Man) + vector(Woman)\) results in a vector that is closest to the vector representation of the word \emph{Queen}. Another possibility to use word embeddings vectors is translation between languages. \citet{mikolov2013exploiting} showed that they can find word translations by comparing vectors generated from different languages. By searching for a translation one can use the word vector from the source language and search for the closest vector in the target language vector space, this word can then be used as a translation. The reason this works is that if a word vector from one language is similar to the word vector of the other language, this word is used in a similar context. This method can be used to infer missing dictionary entries. An example for this method depicted in figure \ref{fig:word-embedding3}. In figure \ref{fig:word-embedding3} the vectors for numbers and animals are depicted on the left side and the same words are depicted on the right side. It can be seen that the vectors for the correct translation align in similar geometric spaces. Again, two-dimensional representation was achieved by using dimension reduction methods.

\begin{figure}
\includegraphics[width=\textwidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_language} \caption{Distributed word vector representations of numbers and animals in English (left) and Spanish (right). Source: @mikolov2013exploiting}\label{fig:word-embedding3}
\end{figure}

\hypertarget{methods-to-obtain-word-embeddings}{%
\section{Methods to Obtain Word Embeddings}\label{methods-to-obtain-word-embeddings}}

The basic idea behind learning word embeddings is the so called \emph{distributional hypothesis} (\citet{Harris.1954}). It states that words that occur in the same contexts tend to have similar meanings. For instance, the words \emph{car} and \emph{truck} tend to have similar semantics as they appear in similar contexts, e.g., with words such as \emph{road}, \emph{traffic}, \emph{transportation}, \emph{engine}, and \emph{wheel}. Hence machine learning and deep learning algorithms can find representations by themselves by evaluating the context in which a word occurs. Words that are used in similar contexts will be given similar representations. This is usually done as an unsupervised or self-supervised procedure, which is a big advantage. That means word embeddings can be thought of as unsupervised feature extractors for words. However, the methods to find such similarities in the context of words vary. Finding word representations started out with more traditional count-based techniques, which collected word statistics like occurrence and co-occurrence frequencies as seen above with BOW. But these representations often require some sort of dimensionality reduction. Later, when neural networks were introduced into NLP, the so-called predictive techniques, mainly popularized after 2013 with the introduction of word2vec, supplanted the traditional count-based word representations.These models learn what is called \emph{dense representations} of words, since they directly learn low-dimensional word representations, without needing the additional dimensionality reduction step. In the following an introduction to the best-known predictive approaches to model word embeddings will be given. First, neural network language models, where word embeddings are learnt as a part of the final language model will be discussed. The description of the two popular algorithms word2vec and GloVe, which learn word embeddings in a pre-step before the actual statistical language model, follow afterwards.

\hypertarget{feedforward-neural-network-language-model-nnlm}{%
\subsection{Feedforward Neural Network Language Model (NNLM)}\label{feedforward-neural-network-language-model-nnlm}}

\citet{Bengio.2003} were the first to propose learning word embeddings within a statistical \textbf{neural network language model (NNLM)}. The goal of the NNLM model of \citet{Bengio.2003} is to predict the next word based on a sequence of preceding words. Using a simple feedforward neural network, the model first learns the word embeddings and in a second step the probability function for word sequences. This way, one obtains not only the model itself, but also the learned word representations, which can be used as input for other, potentially unrelated, tasks.\\
The proposed neural network architecture has an input layer with one-hot encoded word inputs, a linear projection layer for the word embeddings, and a hidden layer with a hyperbolic tangent function, where most of the computation is done, followed by a softmax classifier output layer. The output of the model is a vector of the probability distribution over all words given a specific context. That means a vector with probability scores for each word of the vocabulary. The \emph{i}-th element of the output vector is the probability estimation \(P(w_t = i|context)\). The softmax classifier is used to guarantee positive probabilities summing to one. It computes the following function:
\[\widehat{P}(w_t|w_{t-1},...,w_{t-n+1}) = \frac{ e^{Y_{w_t}} }{ \sum_{i} {e^{y_i}} }\]
The \(y_i\) are the unnormalized log-probabilities for each output word \(i\), which were computed in the previous layer. The model architecture proposed in \citet{Bengio.2003} is depicted in figure \ref{fig:bengio-nnlm}.\\
When training a neural network, one has to define a loss function \(L(\widehat{y}, y)\) stating the loss of predicting \(\widehat{y}\) when the true output is \(y\). In the NNLM literature, a cross-entropy loss is very common (see \citet{goldberg2016primer}). The \(\widehat{y}\) is the network output vector, which was transformed by the softmax classifier and represents the conditional distribution. The \(y\) is usually either a one-hot vector for the correct output word or a vector representing the true multinomial probability distribution over the vocabulary given the specific context. Then the parameter \(\phi\) of the neural network (for example the weights for the embedding vectors) is iteratively changed in order to minimize the loss \(L\) over the training examples.\\
This is usually done with the \textbf{stochastic gradient descent (SGD)} optimizer where the gradient is obtained via \textbf{backpropagation}. The gradient descent optimizer tries to find the direction of the strongest descent via partial derivatives and updates the parameter \(\phi\) accordingly. The learning rate \(\varepsilon\) defines the size of the step in this direction (see \citet{goldberg2016primer}).\\
In \citet{Bengio.2003} a gradient ascent optimizer is used, which performs the following iterative update after presenting the t-th word of the training corpus:
\[\theta \leftarrow  \theta + \varepsilon\frac{\partial log\widehat{P}(w_t|w_{t-1},...,w_{t-n+1})}{\partial \theta }\]
The method by which parameter adjustments are made during training so they can be optimized is called backpropagation. Backpropagation essentially consists of six steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialization of the parameter of the network
\item
  Calculation of \(y(x_i)\) for the inputs \(x_i\)
\item
  Determining the cost of the inputs \(x_i\)
\item
  Calculation of the partial derivatives of the loss for each parameter
\item
  Update the parameter in the network using the partial derivatives calculated in step 4
\item
  Return to step 2 and continue the procedure until the partial derivatives of the loss approach zero
\end{enumerate}

\begin{figure}
\includegraphics[width=\textwidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_bengio_nnlm} \caption{Architecture for NNLM proposed by @Bengio.2003 .  Source: @Bengio.2003}\label{fig:bengio-nnlm}
\end{figure}

\hypertarget{word2vec}{%
\subsection{Word2Vec}\label{word2vec}}

In 2013 \citet{mikolov2013efficient} proposed the two word2vec algorithms which led to a wave in NLP that popularized word embeddings. In contrast to the NNLM model above, the word2vec algorithms are not used for a statistical language modeling goal, but rather to learn the word embeddings themselves. The two word2vec algorithms named Continuous Bag-of-Words (CBOW) and Continuous Skip-Gram use shallow neural networks with an input layer, a projection layer, and an output layer. This means compared to the previously explained feedforward NNLM, the non-linear hidden layer is removed.\\
The general idea behind CBOW is to predict the focus word based on a window of context words. The order of context words does not influence the prediction, thus the name Bag-of-Words. In contrast, Skip-Gram tries to predict the context words given a source word. This is done while adjusting the initial weights during training so that a loss function is reduced.\\
In the \textbf{CBOW} architecture the \(N\) input (context) words are each one-hot encoded vectors of size \(V\), where \(V\) is the size of the vocabulary. Compared to the NNLM model CBOW uses both previous and following words as context instead of only the previous words. The projection layer is a standard fully connected (dense) layer which has the dimensionality \(1 \times D\), where \(D\) is the size of the dimensions for the word embeddings. The projection layer is shared for all words. That means all words get projected into the same position in a linear manner, where the vectors are averaged. The output layer outputs probabilities for the target words from the vocabulary and has a dimensionality of \(V\). That means the output is a probability distribution over all words of the vocabulary as in the NNLM model, where the prediction is the word with the highest probability. But instead of using a standard softmax classifier as in the NNLM model the authors propose to use a log-linear hierarchical softmax classifier for the calculation of the probabilities. The model architecture is shown in figure \ref{fig:word2vec}.
The \textbf{continuous Skip-gram} architecture also uses a log-linear hierarchical softmax classifier with a continuous projection layer, but the input is only one source word, and the output layer consists of as many probability vectors over all words as the chosen number of context words. Also, since the more distant words are usually less related to the source word, the skip-gram model weighs nearby context words more heavily than more distant context words by sampling less from those words in the training examples. The model architecture for skip-gram can be found on the right side of figure \ref{fig:word2vec}.

\begin{figure}
\includegraphics[width=\textwidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_word2vec} \caption{Learning word embeddings with the model architecture of CBOW and Skip-Gram.  Source: @mikolov2013efficient}\label{fig:word2vec}
\end{figure}

As said before the word2vec models use hierarchical softmax, where the vocabulary is represented as a Huffman binary tree, instead of the standard softmax classifier explained in the section before. With hierarchical softmax the size of the output vector can be reduced from the vocabulary size \(V\) to the logarithm to base 2 of \(V\), which is a dramatic change in computational complexity and number of operations needed for the algorithm. Further explanations for this method can be found in \citet{morin2005hierarchical}. For both models \citet{mikolov2013efficient} use gradient descent optimization and backpropagation as described in the previous chapter.\\
\citet{mikolov2013efficient} show that their word2vec algorithms outperform a lot of other standard NNLM models. CBOW is faster while skip-gram does a better job for infrequent words. Skip-gram works well with small amounts of training data and has good representations for words that are considered rare, whereas CBOW trains several times faster and has slightly better accuracy for frequent words.

\hypertarget{glove}{%
\subsection{GloVe}\label{glove}}

GloVe stands for \textbf{Glo}bal \textbf{Ve}ctor word representation, which emphasizes the global character of this model. Unlike the previously described algorithms like word2vec, GloVe not only relies on local context information but also incorporates global co-occurrence statistics. Instead of extracting the embeddings from a neural network that is designed to perform a task like predicting neighboring words (CBOW) or predicting the focus word (Skip-Gram), the embeddings are optimized directly, so that the dot product of two word vectors is equal to the log of the number of times the two words will occur near each other. The model builds on the possibility to derive semantic relationships between words from the co-occurrence matrix and that the ratio of co-occurrence probabilities of two words with a third word is more indicative of semantic association than a direct co-occurrence probability (see \citet{Pennington.2014}).\\
Let \(P_{ij} = P(j|i) = X_{ij}/X_i\) be the probability that word \(j\) appears in the context of word \(i\). Figure \ref{fig:glove} shows an example with the words \emph{ice} and \emph{steam}. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with other words \(k\).\\
For words like \emph{solid} which are related to \emph{ice} but not \emph{steam} the ratio is large (\textgreater{}1), while for words like \emph{gas}, which is related to \emph{steam} but not to \emph{ice}, the ratio is small (\textless{}1). For words like \emph{water} or \emph{fashion}, which are either related to both of the words or to none the ratios are close to 1. It is evident that the comparison of co-occurrence probabilities with a third word (\(P_{ik}/P_{jk}\)) is more indicative of the semantic meanings of words \(i\) and \(j\) and is better at identifying words (\emph{solid} and \emph{gas}) that distinguish \(i\) and \(j\) and words that do not (\emph{water} and \emph{fashion}) than the raw probabilities.

\begin{figure}
\includegraphics[width=\textwidth]{figures/01-01-foundations-applications-of-modern-NLP/01-01_glove_ratios} \caption{Co-occurrence probabilities for target words ice and steam with selected context words.  Source: @Pennington.2014}\label{fig:glove}
\end{figure}

\citet{Pennington.2014} tried to incorporate the ratio \(P_{ik}/P_{jk}\) into computing word embeddings. They propose an optimization problem which aims at fulfilling the following objective:
\[w_i^Tw_k + b_i + b_k = log(X_{ik})\]
Where \(b_i\) and \(b_k\) are bias terms for word \(w_i\) and probe word \(w_k\) and \(X_{ik}\) is the number of times \(w_i\) co-occurs with \(w_k\). Fulfilling this objective minimizes the difference between the dot product of \(w_i\) and \(w_k\) and the logarithm of their number of co-occurrences. In other words, the optimization results in the construction of vectors \(w_i\) and \(w_k\) whose dot product gives a good estimate of their transformed co-occurrence counts. To solve this optimization problem, they reformulate the equation as a least squares problem and introduce a weighting function, since rare co-occurrences add both noise to the model and less information than more frequent co-occurrences.

\hypertarget{hyperparameter-tuning-and-system-design-choices}{%
\section{Hyperparameter Tuning and System Design Choices}\label{hyperparameter-tuning-and-system-design-choices}}

Once adapted across methods, hyperparameter tuning significantly improves performance in every task. \citet{levy2015improving} showed that in a lot of cases, changing the setting of a single hyperparameter could yield a greater increase in performance than switching to a better algorithm or training on a larger corpus. They conducted a series of experiments where they assessed the contributions of diverse hyperparameters. They also show that when all methods are allowed to tune a similar set of hyperparameters, their performance is largely comparable. However they also found that choosing the wrong hyperparameter settings can actually degrade performance of a model. That is why tuning the hyperparameter fitting the specific context is very important. Depending on the model one chooses there are a lot of hyperparameters available for tuning. These are parameters like the number of epochs, batch-size, learning rate, embedding size, window size, corpus size et cetera. In the following the focus will lie on hyperparameters which are frequently discussed. Furthermore, some system design and setup choices will be described which will tackle some of the problems posed by the algorithms mentioned above.

\textbf{Word embedding size}\\
The question of how many embedding dimensions should be used is mostly answered empirically. It depends on the task, computing capacity and vocabulary. The trade-off is between accuracy and computational concerns. More dimensions could potentially increase the accuracy of the representations; since the vectors can capture more aspects of the word. But more dimensions also mean higher computing time and effort. In practice word embedding vectors with dimensions around 50 to 300 are usually used as a rule of thumb (see \citet{goldberg2016primer}).\\
In contrast, \citet{patel2017towards} found that the dimension size should be chosen based on some text corpus statistics. One can calculate a lower bound from the number of pairwise equidistant words of the corpus vocabulary. Choosing a dimension size below this bound results in a loss of quality of learned word embeddings. This result was tested empirically for the skip-gram algorithm (see \citet{patel2017towards}).\\
\citet{Pennington.2014} compare performance of the GloVe model for embedding sizes from 1 to 600 for different evaluation tasks (semantic, syntactic and overall). They found that after around 200 dimensions the performance increase begins to stagnate.

\textbf{Context Window}\\
In traditional approaches the context window around the focus word is constant-sized and unweighted. For example, if the symmetrical context size is 5 then the 5 words before the focus word and 5 words after the focus words are the context window. It is also possible to use an asymmetric context window, which for example only uses words that appear before the focus word. A window size of 5 is commonly used to capture broad topic/domain information like what other words are used in related discussions (i.e. \emph{dog}, \emph{bark} and \emph{leash} will be grouped together, as well as \emph{walked}, \emph{run} and \emph{walking}), whereas smaller windows contain more specific information about the focus word and produce more functional and syntactic similarities (i.e. \emph{Poodle}, \emph{Pitbull}, \emph{Rottweiler}, or \emph{walking}, \emph{running}, \emph{approaching}) (see \citet{goldberg2014word2vec}, \citet{goldberg2016primer}).\\
Since words which appear closer to the focus word are usually more indicative of its meaning it is possible to give the context words weights according to their distance from the focus word. Both word2vec and GloVe use such weighting schemes. GloVe's implementation weights contexts using the harmonic function, e.g.~a context word three tokens away will have 1/3 as a weight. Word2vec's uses weightings of the distance from the focus word divided by the window size. For example, a size-5 window will weigh its contexts by \(\frac {5}{5}\), \(\frac {4}{5}\), \(\frac {3}{5}\), \(\frac {2}{5}\), \(\frac {1}{5}\) (Levy et al.~2015).\\
A performance comparison for GloVe using different window sizes for different evaluation tasks (semantic, syntactic and overall) can be found in \citet{Pennington.2014}. Which shows that, depending on the task, larger performance gains can be expected from a larger window size. But for all three tasks the performance gains decrease after a window size of 4.

\textbf{Document Context}\\
Instead of using a few words as the context window one could consider all the other words that appear with the focus word in the same sentence, paragraph, or document. One can either consider this as using very large window sizes or, as in the doc2vec algorithm from \citet{le2014distributed}, add another embedding vector for a whole paragraph to the other context word vectors. These approaches will result in word vectors that capture topical similarity (words from the same topic, i.e.~words that one would expect to appear in the same document, are likely to receive similar vectors). (\citet{goldberg2016primer}; \citet{le2014distributed})

\textbf{Subsampling of Frequent Words}\\
\citet{Mikolov.2013c} proposed for their word2vec algorithms to use a subsample of the most frequent words. Very frequent words are often so-called stop-words, like \emph{the} or \emph{a}, which do not provide much information. Using fewer of these frequent words leads to a significant speedup and improves accuracy of the representations of less frequent words (see \citet{Mikolov.2013c}). The method randomly removes words \(w\) with a probability \(p\) that occur more often than a certain threshold \(t\). In \citet{Mikolov.2013c} the probability \(p\) is defined as follows:
\[P(w_i) = 1-\sqrt{\frac{t}{f(w_i)}}\]
where \(f\) marks the word's frequency in the text corpus. In \citet{Mikolov.2013c} the threshold \(t\) is set to \(10^{-5}\), but generally this parameter is open for tuning. In \citet{Mikolov.2013c} this subsampling is done before processing the text corpus. This leads to an artificial enlargement of the context window size. One could perform the subsampling step without affecting the context window size, but \citet{levy2015improving} found that it does not affect performance too much.

\textbf{Negative Sampling}\\
In their first paper \citet{mikolov2013efficient} proposed using hierarchical softmax instead of the standard softmax function to speed up the calculation in the neural network. But later they published a new method called negative sampling, which is even more efficient in the calculation of word embeddings. The negative sampling approach is based on the skip-gram algorithm, but it optimizes a different objective. It maximizes a function of the product of word and context pairs \((w, c)\) that occur in the training data, and minimizes it for negative examples of word and context pairs \((w, c_n)\) that do not occur in the training corpus. The negative examples are created by drawing \(k\) negative examples for each observed \((w, c)\) pair. \(k\) can be tuned as a hyperparameter. \citet{Mikolov.2013c} found that \(k\) in the range 5-20 is useful for small training datasets; while for large datasets \(k\) can be as small as 2-5 (see \citet{Mikolov.2013c}; \citet{goldberg2014word2vec}).

\textbf{Subword Information}\\
An individual word can convey a lot of information besides the general meaning of the word. Ignoring the internal structure of words can lead to a large loss of information, especially in morphologically rich languages like Finnish or Turkish. Furthermore, coping with completely unseen words not included in the training data, so-called \emph{out-of-vocabulary} (OOV) words, is not possible if every word gets assigned a distinct new vector representation like in the previously presented models. One way to deal with these problems is to train character-based models instead of word-based models. But as \citet{goldberg2016primer} states: ``working on the character level is very challenging, as the relationship between form (characters) and function (syntax, semantics) in language is quite loose. Restricting oneself to stay on the character level may be an unnecessarily hard constraint.'' A more promising approach to solve the problem of OOV words is to use subword information. In this context \textbf{fastText} was introduced in two papers 2016 and 2017 (see \citet{joulin2016bag} and \citet{bojanowski2017enriching}). fastText builds upon the previously described continuous skip-gram model. But instead of learning vectors for words directly as done by skip-gram, fastText represents each word as an n-gram of characters. So, for example, take the word, \emph{planning} with n=3, the fastText representation of this word is \textless{}pl, pla, lan, ann, nni, nin, ing, ng\textgreater{}, where the angular brackets indicate the beginning and end of the word. This helps capture the meaning of shorter words inside longer words and allows the embeddings to understand suffixes and prefixes. In addition to these n-grams fastText learns embeddings to the whole word. \citet{bojanowski2017enriching} extracted all the n-grams for n greater or equal to 3 and smaller or equal to 6. They also state that ``the optimal choice of length ranges depends on the considered task and language and should be tuned appropriately''. In the end one word will be represented by the sum of the vector representations of its n-grams and the word itself. In the case of an unseen word (OOV), the corresponding embedding is induced by averaging the vector representations of its constituent character n-grams. Hence fastText performs well when having data with a large number of rare words.

\textbf{Phrase representation}\\
The models described in the previous section all focus on individual words as input. But there are many words which will only be meaningful in combination with other words, or which change meaning completely when paired up with another word. Therefore, there are phrases with meanings that are more than a simple composition of the meanings of their individual words. Often these are names like ``New York'' for a city or ``Toronto Raptors'' for a basketball team. Since the meaning is changed completely when evaluating the combination of these words one embedding must be learned for the whole phrase instead of using the embeddings of the individual words.\\
\citet{Mikolov.2013c} proposed an approach to deal with such phrases. First, they use the following score to find them in the text corpus:
\[score(w_i,w_j) = \frac{count(w_iw_j)-\delta }{count(w_i)\times count(w_j)}\]
With this score they try to find words that appear frequently together, and infrequently in other contexts. The score compares the appearance of two words together to their appearance alone in the text corpus. The \(\delta\) is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed. When the score of a word pair is above a chosen threshold value it will be used as a phrase. They repeat this calculation 2 to 4 times with decreasing threshold value, allowing longer phrases that consist of several words to be formed.

\hypertarget{evaluation-methods}{%
\section{Evaluation Methods}\label{evaluation-methods}}

The use of different algorithms, hyperparameter or system choices need to be evaluated and compared in their performance to make a reasonable choice. Therefore, there are several tasks and datasets which are used to evaluate word embeddings. In this chapter the two most common tasks and five corresponding datasets for each task will be presented. The dataset collection was done by \citet{bakarov2018survey}, also refer to this paper for a more thorough examination of the evaluation tasks.

\textbf{Word Similarity Task}\\
The word similarity task tries to evaluate the distances between embedding vectors, which should represent their similarity, by comparing them to similarity scores given by humans. The more similar the embedding distance to the human score the better the embedding. The word vectors are evaluated by ranking the pairs according to their cosine similarities, and measuring the correlation (Spearman's ρ) with the human ratings. These are five datasets used to evaluate word similarity ranked by their size:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  SimVerb-3500, 3 500 pairs of verbs assessed by semantic similarity with a scale from 0 to 4 (\citet{gerz2016simverb}).
\item
  MEN, 3 000 pairs assessed by semantic relatedness with a discrete scale from 0 to 50 (\citet{bruni2014multimodal}).
\item
  RW (acronym for Rare Word), 2 034 pairs of words with low occurrences (rare words) assessed by semantic similarity with a scale from 0 to 10 (\citet{luong2013better}).
\item
  SimLex-999, 999 pairs assessed with a strong respect to semantic similarity with a scale from 0 to 10 (\citet{hill2015simlex}).
\end{enumerate}

\textbf{Word analogy task}\\
In the word analogy task word relations are predicted in the form \emph{``a is to a∗ as b is to b∗''}, where b∗ is hidden, and must be guessed from the entire vocabulary. A popular example of an analogy is that \emph{king} relates to \emph{queen} as \emph{man} relates to \emph{woman}. These are five datasets used to evaluate word analogy ranked by their size:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  WordRep, 118 292 623 analogy questions (4-word tuples) divided into 26 semantic classes (\citet{gao2014wordrep}).
\item
  BATS (acronym for Bigger Analogy Test Set), 99 200 questions divided into 4 classes (inflectional morphology, derivational morphology, lexicographic semantics and encyclopedic semantics) and 10 smaller subclasses (\citet{gladkova2016intrinsic}).
\item
  Google Analogy (also called Semantic-Syntactic Word Relationship Dataset), 19 544 questions divided into 2 classes (morphological relations and semantic relations) and 10 smaller subclasses (8 869 semantic questions and 10 675 morphological questions) (\citet{mikolov2013efficient}).
\item
  SemEval-2012, 10 014 questions divided into 10 semantic classes and 79 subclasses prepared for the SemEval-2017 Task 2 (Measuring Degrees of Relational Similarity) (\citet{jurgens2012semeval}).
\item
  MSR (acronym for Microsoft Research Syntactic Analogies), 8 000 questions divided into 16 morphological classes (\citet{mikolov2013linguistic}).
\end{enumerate}

\hypertarget{outlook-and-resources}{%
\section{Outlook and Resources}\label{outlook-and-resources}}

The use of word embeddings furthered much development in the field of natural language processing. Still, there are problems word embeddings are often not suited to resolve. When calculating word embeddings, the word order is not taken into account. For some NLP tasks like sentiment analysis, this does not pose a problem. But for other tasks like translation, word order can not be ignored. Recurrent neural networks, which will be presented in the following chapter, are one of the tools to face this difficulty.\\
Furthermore, there are a lot of words with two or more different meanings. \emph{Mouse} for example can be understood as an animal or as an operator for a computer. Humans naturally take the context into account, in which the word was used, to infer the meaning. The above described word embeddings are not able to do this. But ELMO, which will be discussed in chapter \href{./transfer-learning-for-nlp-i.html}{Chapter 7}, uses contextualized embeddings to solve this problem.\\
Still there are two additional problems, which will not be addressed in this book. First of the word embeddings are mostly learned from text corpora from the internet, therefore they learn a lot of stereotypes that reflect everyday human culture. Another problem is that there are some domains and languages for which only little training data exists on the internet. The algorithms described above all use large amounts of training data to learn exact word embeddings. Solutions to these problems are still work in progress.\\
Last but not least some resources for downloading pre-calculated word embeddings will be presented. As stated before, if the task is rather common and the words used are from a general vocabulary, one could use pre-calculated word embeddings for the training of the language model. The first two links lead to websites, where word embeddings learned with GloVe and fastText can be downloaded. These were trained on different training data sources like \emph{Wikipedia}, \emph{Twitter} or \emph{Common Crawl} text. GloVe embeddings can only be downloaded for english words, whereas fastText also offers word embeddings for 157 different languages. The last link leads to a website, which is maintained by the Language Technology Group at the University of Oslo and offers word embeddings for many different languages and models.

\textbf{Glove}:\\
\url{https://nlp.stanford.edu/projects/glove/}

\textbf{fastText}:\\
\url{https://fasttext.cc/docs/en/english-vectors.html}

\textbf{Different Models and different languages}:\\
\url{http://vectors.nlpl.eu/repository/}

\hypertarget{recurrent-neural-networks-and-their-applications-in-nlp}{%
\chapter{Recurrent neural networks and their applications in NLP}\label{recurrent-neural-networks-and-their-applications-in-nlp}}

\emph{Author: Marianna Plesiak}

\emph{Supervisor: Prof.~Dr.~Christian Heumann}

\hypertarget{structure-and-training-of-simple-rnns}{%
\section{Structure and Training of Simple RNNs}\label{structure-and-training-of-simple-rnns}}

\textbf{R}ecurrent \textbf{n}eural \textbf{n}etworks (\textbf{RNNs}) enable to relax the condition of non-cyclical connections in the classical feedforward neural networks which were described in the previous chapter. This means, while simple multilayer perceptrons can only map from input to output vectors, RNNs allow the entire history of previous inputs to influence the network output. \citep{graves2013generating}

The first part of this chapter provides the structure definition of RNNs, presents the principles of their training and explains problems with backpropagation. The second part covers gated units, an improved way to calculate hidden states. The third part gives an overview of some extended versions of RNNs and their applications in NLP.

\hypertarget{network-structure-and-forwardpropagation}{%
\subsection{Network Structure and Forwardpropagation}\label{network-structure-and-forwardpropagation}}

An \textbf{unfolded computational graph} can visualize the repetitive structure of RNNs:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/01-02-rnns-and-their-applications-in-nlp/1_unfolded_graph} 

}

\caption{Unfolded computatinal graph of an RNN. Source: Own figure.}\label{fig:01-02-unfold}
\end{figure}

Each node is associated with a network layer at a particular time instance. Inputs \(x^{(t)}\) must be encoded as numeric vectors, for instance word embeddings or one-hot encoded vectors, see the previous chapter. Recurrently connected vectors \(h\) are called \textbf{hidden states} and represent the outputs of the hidden layer. At time \(t\), a hidden state \(h^{(t)}\) combines information from the previous hidden state \(h^{(t-1)}\) as well as the new input \(x^{(t)}\) and transmits it to the next hidden state. Obviously, such an architecture requires the initialization of \(h^{(0)}\) since there is no memory at the very beginning of the sequence processing. Given the hidden sequences, output vectors \(\hat{y}^{(t)}\) are used to build the predictive distribution \(Pr(x^{(t+1)}|y^{(t)})\) for the next input \citep{graves2013generating}. Since the predictions are created at each time instance \(t\), the total output has a shape \([\#\ time\_steps, \#\ output\_features]\). However, in some cases the whole history of output features is not necessary. For example, in sentiment analysis the last output of the loop is sufficient because it contains the entire information about the sequence. \citep{chollet2018deep}

The unfolded recurrence can be formalized as following:

\begin{align}
h^{(t)} & = g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)}) \\
& = f(h^{(t-1)},x^{(t)}| \theta)  \label{eq:recurrent}
\end{align}

After \(t\) steps, the function \(g^{(t)}\) takes into account the whole sequence \((x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)})\) and produces the hidden state \(h^{(t)}\). Because of its cyclical structure, \(g^{(t)}\) can be factorized into the repeated application of the same function \(f\). This function can be considered to be a universal model with parameters \(\theta\) which are shared across all time steps and generalized for all sequence lengths. This concept is called parameter sharing and is illustrated in the unfolded computational graph as a reuse of the same matrices \(W_{xh}\), \(W_{hh}\) and \(W_{hy}\) through the entire network. \citep{goodfellow2016deep}

Considering a recurrent neural network with one hidden layer that is used to predict words or characters, the output should be discrete and the model maps input sequence to output sequence of the same length. Then, the forward propagation is computed by iterating the following equations:

\begin{align}
h^{(t)} & = \mathcal{f}(a+W_{hh}h^{(t-1)}+W_{xh}x^{(t)}) \label{eq:input-to-hidden} \\
y^{(t)} & = \mathcal{s}(b+W_{hy}h^{(t)}) \label{eq:hidden-to-output} 
\end{align}

where, according to \citet{graves2013generating}, the parameters and functions denote the following:

\begin{itemize}
\tightlist
\item
  \(x^{(t)}\), \(h^{(t)}\) and \(y^{(t)}\): input, hidden state and output at time step \(t\) respectively;
\item
  \(\mathcal{f}\): activation function of the hidden layer. Usually it is a saturating nonlinear function such as a sigmoid activation function ( \citet{sutskever2014sequence} and \citet{mikolov2010recurrent});\\
\item
  \(W_{hh}\): weight matrix connecting recurrent connections between hidden states;\\
\item
  \(W_{xh}\): weight matrix connecting inputs to a hidden layer;\\
\item
  \(W_{hy}\): weight matrix connecting hidden states to outputs;\\
\item
  \(\mathcal{s}\): output layer function. If the model is used to predict words, the softmax function is usually chosen as it returns valid probabilities over the possible outputs \citep{mikolov2010recurrent};\\
\item
  \(a\), \(b\): input and output bias vectors.
\end{itemize}

Since inputs \(x^{(t)}\) are usually encoded as one-hot-vectors, the dimension of a vector representing one word corresponds to the size of vocabulary. The size of a hidden layer must reflect the size of training data. The model training requires initialization of the initial state \(h^{(0)}\) as well as the weight matrices, which are usually set to small random values \citep{mikolov2010recurrent}. Since the model is used to compute the predictive distributions \(Pr(x^{(t+1)}|y^{(t)})\) at each time instance \(t\), the network distribution is denoted as following:

\begin{align}
Pr(x) & =\prod_{t} Pr(x^{(t+1)}|y^{(t)}) \label{eq:rnn-probability} 
\end{align}

and the total loss \(\mathcal{L(x)}\), which must be minimized during the training, is simply the sum of losses over all time steps denoted as the negative log-likelihood of \(Pr(x)\):

\begin{align}
\mathcal{L(x)} & =-\sum_{t} \log{Pr(x^{(t+1)}|y^{(t)})} \label{eq:rnn-log-loss}   
\end{align}

\citep{graves2013generating}

\hypertarget{backpropagation}{%
\subsection{Backpropagation}\label{backpropagation}}

To train the model, one must calculate the gradients for the three weight matrices \(W_{xh}\), \(W_{hh}\) and \(W_{hy}\). The algorithm differs from a regular backpropagation because a chain rule must be applied recursively, and the gradients are summed up through the network \citep{boden2002guide}. Using the notation \(\mathcal{L^{(t)}}\) as the output at time \(t\), one must first estimate single losses at each time step and then sum them up in order to obtain total loss \(\mathcal{L(x)}\).

\citet{chen2016gentle} provides a nice guide to backpropagation for a simple RNN in the meaning of equations \eqref{eq:input-to-hidden} and \eqref{eq:hidden-to-output}. According to him, the gradient w.r.t. \(W_{hy}\) for a single time step \(t\) is calculated as follows:

\begin{align}
\frac{\partial \mathcal{L^{(t)}}}{\partial W_{hy}} & = \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial W_{hy}}\label{eq:rnn-back-hy-one} \\
\end{align}

Since \(W_{hy}\) is shared across all time sequence, the total loss w.r.t. the weight matrix connecting hidden states to outputs is simply a sum of single losses:

\begin{align}
\frac{\partial \mathcal{L}}{\partial W_{hy}} & = \sum_{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}} {\partial W_{hy}}  \label{eq:rnn-back-hy-all} \\    
\end{align}

Similarly, derivation of the gradient w.r.t. \(W_{hh}\) for a single time step is obtained as follows:

\begin{align}
\frac{\partial \mathcal{L^{(t)}}}{\partial W_{hh}} & = \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial W_{hh}} \label{eq:rnn-back-hh-one}
\end{align}

However, the last part \(h^{(t)}\) also depends on \(h^{(t-1)}\) and the gradient can be rewritten according to the \textbf{B}ack\textbf{p}ropagation \textbf{T}hrough \textbf{Time} algorithm (\textbf{BPTT}) starting from \(t\) and going back to the initial hidden state at time step \(k=0\):

\begin{align}
\frac{\partial \mathcal{L^{(t)}}}{\partial W_{hh}}
& = \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(t-1)}} \frac{\partial h^{(t-1)}}{\partial W_{hh}} \\
& = \sum_{k=0}^{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W_{hh}} \label{eq:rnn-back-hh-one-one}
\end{align}

Single gradients are again aggregated to yield the overall loss w.r.t \(W_{hh}\):

\begin{align}
\frac{\partial \mathcal{L}}{\partial W_{hh}}
& = \sum_{t} \sum_{k=0}^{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W_{hh}} \label{eq:rnn-back-hh-all}
\end{align}

The derivation of gradients w.r.t. \(W_{xh}\) is similar to those w.r.t \(W_{hh}\) because both \(h^{(t-1)}\) and \(x^{(t)}\) contribute to \(h^{(t)}\) at time step \(t\). The derivative w.r.t. \(W_{xh}\) for the whole sequence is then obtained by summing up all contributions from \(t\) to \(0\) via backpropagation for all time steps:

\begin{align}
\frac{\partial \mathcal{L}}{\partial W_{xh}}
& = \sum_{t} \sum_{k=0}^{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W_{xh}} \label{eq:rnn-back-xh-all}
\end{align}

The derivatives w.r.t. the bias vectors \(a\) and \(b\) are calculated based on the same principles and thus are not shown here explicitly.

\hypertarget{vanishing-and-exploding-gradients}{%
\subsection{Vanishing and Exploding Gradients}\label{vanishing-and-exploding-gradients}}

In order to better understand the mathematical challenges of BPTT, one should consider equation \eqref{eq:rnn-back-hh-one-one}, and in particular the factor \(\frac{\partial h^{(t)}}{\partial h^{(k)}}\). \citet{pascanu2013difficulty} go into detail and show that it is a chain rule in itself and can be rewritten as a product \(\frac{\partial h^{(t)}}{\partial h^{(t-1)}} \frac{\partial h^{(t-1)}}{\partial h^{(t-2)}} \ldots \frac{\partial h^{(2)}}{\partial h^{(1)}}\). Since one computes the derivative of a vector w.r.t another vector, the result will be a product of \(t-k\) Jacobian matrices whose elements are the pointwise derivatives:

\begin{align}
\frac{\partial h^{(t)}}{\partial h^{(k)}}
& = \prod_{t\geq\ i>k}^{} \frac{\partial h^{(i)}}{\partial h^{(i-1)}}
= \prod_{t\geq\ i>k}^{} W_{hh}^{T}diag(\mathcal{f}^{'}(h^{(i-1)}))  \label{eq:rnn-back-vanishing} 
\end{align}

Thus, with small values in \(W_{hh}\) and many matrix multiplications the norm of the gradient shrinks to zero exponentially fast with \(t-k\) which results in a vanishing gradient problem and the loss of long term contributions. Exploding gradients refer to the opposite behaviour when the norm of the gradient increases largely and leads to a crash of the model. \citet{pascanu2013difficulty} give an overview of techniques for dealing with the exploding and vanishing gradients. Among other solutions they mention proper initialisation of weight matrices, sampling \(W_{hh}\) and \(W_{xh}\) instead of learning them, rescaling or clipping the gradient's components (putting a maximum limit on it) or using L1 or L2 penalties on the recurrent weights. Even more popular models are gated RNNs which explicitly address the vanishing gradients problem and will be explained in the next subchapter.

\hypertarget{gated-rnns}{%
\section{Gated RNNs}\label{gated-rnns}}

Main feature of gated RNNs is the ability to store long-term memory for a long time and at the same time to account for new inputs as effectively as possible. In modern NLP, two types of gated RNNs are used widely: Long Short-Term Memory networks and Gated Recurrent Units.

\hypertarget{lstm}{%
\subsection{LSTM}\label{lstm}}

\textbf{L}ong \textbf{S}hort-\textbf{T}erm \textbf{Memory} (\textbf{LSTM}) networks were introduced by \citet{hochreiter1997long} with the purpose of dealing with problems of long term dependencies. Figure \ref{fig:01-02-lstm-gru} illustrates the complex architecture of LSTM hidden state. Instead of a simple hidden unit that combines inputs and previous hidden states linearly and outputs their nonlinear transformation to the next step, hidden units are now extended by special input, forget and output gates that help to control the flow of information. Such more complex units are called memory cells, and the following equations show how a LSTM uses the gating mechanism to calculate the hidden state within a memory cell:

\begin{align}
f^{(t)} & = sigm(W_{xf}x^{(t)}+W_{hf}h^{(t-1)}+b_{f}) \label{eq:lstm-forget} \\
i^{(t)} & = sigm(W_{xi}x^{(t)}+W_{hi}h^{(t-1)}+b_{i}) \label{eq:lstm-input} \\
o^{(t)} & = sigm(W_{xo}x^{(t)}+W_{ho}h^{(t-1)}+b_{o}) \label{eq:lstm-output} \\
g^{(t)} & = tanh(W_{xc}x^{(t)}+W_{hc}h^{(t-1)}+b_{c}) \label{eq:lstm-candidates} \\
c^{(t)} & = f^{(t)}c^{(t-1)}+i^{(t)}g^{(t)} \label{eq:lstm-newcell} \\
h^{(t)} & = o^{(t)}tanh(c^{(t)}) \label{eq:lstm-newoutput} \\
\end{align}

First, the forget gate \(f^{(t)}\) decides which values of the previous output \(h^{(t-1)}\) to forget. The next step involves deciding which information will be stored in the internal cell state \(c^{(t)}\). This step consists of the following two parts: 1) the old cell state \(c^{(t-1)}\) is multiplied by \(f^{(t)}\) in order to forget information; 2) new candidates are calculated in \(g^{(t)}\) and multiplied by the input gate \(i^{(t)}\) in order to add new information. Finally, the output \(h^{(t)}\) is produced with help of the output gate \(o^{(t)}\) and applying a \(tanh\) function to the cell state in order to only output selected values. (\citet{goodfellow2016deep}, \citet{graves2013generating})

\hypertarget{gru}{%
\subsection{GRU}\label{gru}}

In 2014, \citet{cho2014learning} introduced \textbf{G}ated \textbf{R}ecurrent \textbf{U}nits (\textbf{GRU}) whose structure is simpler than that of LSTM because they have only two gates, namely reset and update gate.
The hidden state is thus calculated as follows:

\begin{align}
r^{(t)} & = sigm(W_{xr}x^{(t)}+W_{hr}h^{(t-1)}) \label{eq:gru-reset} \\
\tilde{h}^{(t)} & = tanh(W_{xh}x^{(t)}+W_{hh}(r^{(t)} \odot h^{(t-1)})) \label{eq:gru-hidden} \\
z^{(t)} & = sigm(W_{xz}x^{(t)}+W_{hz}h^{(t-1)}) \label{eq:gru-update} \\
h^{(t)} & = z^{(t)}\odot h^{(t-1)}+(1-z^{(t)})\odot \tilde{h}^{(t)} \label{eq:gru-hidden-output} \\
\end{align}

First, the reset gate \(r^{(t)}\) decides how much the previous hidden state \(h^{(t-1)}\) is ignored, so if \(r^{(t)}\) is close to \(0\), the hidden state is forced to drop past irrelevant information and reset with current input \(x^{(t)}\). A candidate hidden state \(\tilde{h}^{(t)}\) is then obtained in the following three steps: 1) weight matrix \(W_{xh}\) is multiplied by current input \(x^{(t)}\); 2) weight matrix \(W_{hh}\) is multiplied by the element-wise product (denoted as \(\odot\)) of reset gate \(r^{(t)}\) and previous hidden state \(h^{(t)}\); 3) both products are added and a \(tanh\) function is applied in order to output the candidate values for a hidden state.
On the other hand, the update gate \(z^{(t)}\) decides how much of the previous information stored in \(h^{(t-1)}\) will carry over to \(h^{(t)}\) and how much the new hidden state must be updated with the candidate \(\tilde{h}^{(t)}\). Thus, the update gate combines the functionality of LSTM forget and input gates and helps to capture long-term memory. \citet{cho2014learning} used the GRU in the machine translation task and showed that the novel hidden units can achieve good results in term of BLEU score (see chapter \href{./resources-and-benchmarks-for-nlp.html}{11} for definition) while being computationally cheaper than LSTM.

Figure \ref{fig:01-02-lstm-gru} visualizes the architecture of hidden states of gated RNNs mentioned above and illustrates their differences:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/01-02-rnns-and-their-applications-in-nlp/2_lstm_vs_gru} 

}

\caption{Structure of a hidden unit. LSTM on the right and GRU on the left. Source: Own figure inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/}\label{fig:01-02-lstm-gru}
\end{figure}

\hypertarget{extensions-of-simple-rnns}{%
\section{Extensions of Simple RNNs}\label{extensions-of-simple-rnns}}

In most challenging NLP tasks, such as machine translation or text generation, one-layer RNNs even with gated hidden states cannot often deliver successful results. Therefore more complicated architectures are necessary. Deep RNNs with several hidden layers may improve model performance significantly. On the other hand, completely new designs such as Encoder-Decoder architectures are most effective when it comes to mapping sequences of an arbitrary length to a sequence of another arbitrary length.

\hypertarget{deep-rnns}{%
\subsection{Deep RNNs}\label{deep-rnns}}

The figure \ref{fig:01-02-unfold} shows that each layer is associated with one parameter matrix so that the model is considered shallow. This structure can be extended to a deep RNN, although the depth of an RNN is not a trivial concept since its units are already expressed as a nonlinear function of multiple previous units.

\hypertarget{stacked-rnns}{%
\subsubsection{Stacked RNNs}\label{stacked-rnns}}

In their paper \citet{graves2013speech} use a deep RNN for speech recognition where the depth is defined as stacking \(N\) hidden layers on top of each other with \(N>1\). In this case the hidden vectors are computed iteratively for all hidden layers \(n=1\) to \(N\) and for all time instances \(t=1\) to \(T\):

\begin{align}
h_{n}^{(t)} & = \mathcal{f}(a_{n}+W_{h_{n}h_{n}}h_{n}^{(t-1)}+W_{h_{n-1}h_{n}}h_{n-1}^{(t)}) \label{eq:deep-hidden}
\end{align}

As a hidden layer function \citet{graves2013speech} choose \textbf{bidirectional LSTM}. Compared to regular LSTM, BiLSTM can train on inputs in their original as well as reversed order. The idea is to stack two separate hidden layers one on another while one of the layers is responsible for the forward information flow and another one for the backward information flow. Especially in speech recognition one must take in consideration future context, too, because pronouncement depends both on previous and next phonemes. Thus, BiLSTMs are able to access long-time dependencies in both input directions. Finally, \citet{graves2013speech} compare different deep BiLSTM models (from 1 to 5 hidden layers) with unidirectional LSTMs and a pre-trained RNN transducer (BiLSTM with 3 hidden layers pretrained to predict each phoneme given the previous ones) and show clear advantage of deep networks over shallow designs.

\hypertarget{deep-transition-rnns}{%
\subsubsection{Deep Transition RNNs}\label{deep-transition-rnns}}

\citet{pascanu2013construct} proposed another way to make an RNN deeper by introducing \textbf{transitions}, one or more intermediate nonlinear layers between input to hidden, hidden to output or two consecutive hidden states. They argue that extending input-to-hidden functions helps to better capture temporal structure between successive inputs. A deeper hidden-to-output function, DO-RNN, can make hidden states more compact and therefore enables the model to summarize the previous inputs more efficiently. A deep hidden-to-hidden composition, DT-RNN, allows for the hidden states to effectively add new information to the accumulated summaries from the previous steps. They note though, because of including deep transitions, the distances between two variables at \(t\) and \(t+1\) become longer and the problem of loosing long-time dependencies may occur. One can add shortcut connections to provide shorter paths for gradients, such networks are referred to as DT(S)-RNNs. If deep transitions with shortcuts are implemented both in hidden and output layers, the resulting model is called DOT(S)-RNNs. \citet{pascanu2013construct} evaluate these designs on the tasks of polyphonic music prediction and character- or word-level language modelling. Their results reveal that deep transition RNNs clearly outperform shallow RNNs in terms of perplexity (see chapter \href{./resources-and-benchmarks-for-nlp.html}{11} for definition) and negative log-likelihood.

\hypertarget{encoder-decoder-architecture}{%
\subsection{Encoder-Decoder Architecture}\label{encoder-decoder-architecture}}

\hypertarget{design-and-training}{%
\subsubsection{Design and Training}\label{design-and-training}}

The problem of mapping variable-length input sequences to variable-length output sequences is known as Sequence-to-Sequence or \textbf{seq2seq} learning in NLP. Although originally applied in machine translation tasks (\citet{sutskever2014sequence}, \citet{cho2014learning}), the seq2seq approach achieved state-of-the-art results also in speech recognition \citep{prabhavalkar2017comparison} and video captioning \citep{venugopalan2015sequence}. According to \citet{cho2014learning}, the seq2seq model is composed of two parts as illustrated below:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/01-02-rnns-and-their-applications-in-nlp/3_encoder_decoder} 

}

\caption{Encoder-Decoder architecture. Source: Own figure based on @cho2014learning.}\label{fig:01-02-enc-dec}
\end{figure}

The first part is an \textbf{encoder}, an RNN which is trained on input sequences in order to obtain a large summary vector \(c\) with a fixed dimension. This vector is called context and is usually a simple function of the last hidden state. \citet{sutskever2014sequence} used the final encoder hidden state as context such that \(c=h_{e}^{(T)}\). The second part of the model is a \textbf{decoder}, another RNN which generates predictions given the context \(c\) and all the previous outputs. In contrast to a simple RNN described at the beginning of this chapter, decoder hidden states \(h_{d}^{(t)}\) are now conditioned on the previous outputs \(y^{(t)}\), previous hidden states \(h_{d}^{(t)}\) and the summary vector \(c\) from the encoder part. Therefore, the conditional distribution of the one-step prediction is obtained by:

\begin{align}
h_{d}^{(t)} & = f(h_{d}^{(t-1)},y^{(t-1)},c) \label{eq:decoder-hidden} \\
P(y^{(t)}|y^{(t-1)},...y^{(1)},c) & = \mathcal{s}(h_{d}^{(t)},y^{(t-1)},c) \label{eq:decoder-output} \\
\end{align}

Both parts are trained simultaneously to maximize the conditional log-likelihood \(\frac{1}{N}\sum_{n=1}^{N}\log{p_{\theta}(y_{n}|x_{n})}\), where \(\theta\) denotes the set of model parameters and \((y_{n},x_{n})\) is an (input sequence, output sequence) pair from the training set with size \(N\) (\citet{cho2014learning}).

\hypertarget{multi-task-seq2seq-learning}{%
\subsubsection{Multi-task seq2seq Learning}\label{multi-task-seq2seq-learning}}

\citet{luong2015multi} extended the idea of encoder-decoder architecture even further by allowing \textbf{multi-task learning} (\textbf{MLT}) for seq2seq models. MLT aims to improve performance of one task using other related tasks such that one task complements another. In their paper, they investigate the following three settings: a) \emph{one-to-many} - where the encoder is shared between different tasks such as translation and syntactic parsing, b) \emph{many-to-one} - where the encoders learn different tasks such as translation and image captioning and the decoder is shared, c) \emph{many-to-many} - where the model consists of multiple encoders and decoders which is the case of autoencoders, an unsupervised task used to learn a representation of monolingual data.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/01-02-rnns-and-their-applications-in-nlp/4_multitask_seq2seq} 

}

\caption{Multi-task settings. Source: @luong2015multi.}\label{fig:01-02-multi-task-seq2seq}
\end{figure}

\citet{luong2015multi} consider German-to-English and English-to-German translations as the primary task and try to determine whether other tasks can improve their performance and vice versa. After training deep LSTM models with four layers for different task combinations, they conclude that MLT can improve the performance of seq2seq models substantially. For instance, the translation quality improves after adding a small number of parsing minibatches (one-to-many setting) or after the model have been trained to generate image captions (many-to-one setting). In turn, translation task helps to parse large data corpus much better (one-to-many setting). In contrast to these achievements, autoencoder task does not show significant improvements in translation after two unsupervised learning tasks on English and German language data.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

RNNs are powerful models for sequential data. Even in their simple form they can show valid results in different NLP tasks but their shortcomings led to introduction of more flexible networks such as gated units and encoder-decoder designs described in this chapter. As NLP has become a highly discussed topic in the recent years, even more advanced concepts such as Transfer Learning and Attention have been introduced, which still base on an RNN or its extension. However, RNNs are not the only type of neural networks used in NLP. Convolutional neural networks also find their application in modern NLP, and the next chapter will describe them.

\hypertarget{convolutional-neural-networks-and-their-applications-in-nlp}{%
\chapter{Convolutional neural networks and their applications in NLP}\label{convolutional-neural-networks-and-their-applications-in-nlp}}

\emph{Authors: Rui Yang}

\emph{Supervisor: Prof.~Dr.~Christian Heumann}

\hypertarget{introduction-to-basic-architecture-of-cnn}{%
\section{Introduction to Basic Architecture of CNN}\label{introduction-to-basic-architecture-of-cnn}}

This section presents a brief introduction of the Convolutional neural network (CNN) and its main elements, based on which it would be more effective for further exploration of the applications of a Convolutional neural network in the field of Natural language processing (NLP).

\begin{figure}[ht]

{\centering \includegraphics[width=1.05\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/basic_structure} 

}

\caption{Basic structure of CNN}\label{fig:figs}
\end{figure}

As illustrated in Figure \ref{fig:figs}, a convolutional neural network includes successively an input layer, multiple hidden layers, and an output layer, the input layer will be dissimilar according to various applications. The hidden layers, which are the core block of a CNN architecture, consist of a series of \textbf{convolutional layers}, \textbf{pooling layers}, and finally export the output through the \textbf{fully-connected layer}. In the following sub-chapters, descriptions of the critical layers of CNN and their corresponding intuitive examples will be provided in detail.

\hypertarget{convolutional-layer}{%
\subsection{Convolutional Layer}\label{convolutional-layer}}

The convolutional layer is the core building block of a CNN. In short, the input with a specific shape will be abstracted to a \textbf{feature map} after passing the convolutional layer. a set of learnable \textbf{filters (or kernels)} plays an important role throughout this process. The following Figure \ref{fig:figs-2} provides a more intuitive explanation of the convolutional layer.

\begin{figure}[ht]

{\centering \includegraphics[width=0.65\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/Matrix} 

}

\caption{Basic operational structure of the convolutional layer}\label{fig:figs-2}
\end{figure}

The input of the Neural Networks might be assumed as a \(6\times6\) matrix and each element of which can be presented as the integer `0', `1'. As mentioned before, there is a set of learnable filters in the convolutional layer and each filter can be considered as a matrix, which is similar to a neuron in a fully-connected layer. In this instance, filters of size \(3 \times 3\) slide over with a specific stride across the entire input image, and each element of the matrix or filter serves as a parameter (weight and bias) of Neural Networks. Traditionally, these parameters are not based on the initial setting but are trained through the training data.

An activated filter of size \(3 \times 3\) has an ability to detect a pattern of the same size at some spatial position in the input. The algebraic operation explicates the transformation process from the input to the feature map.

\[ X  : = \begin{pmatrix}
X_{11} & X_{12} & \cdots & X_{16}\\ \\
X_{21} & X_{22} & \cdots &X_{26} \\ \\
\vdots & \vdots & \ddots & \vdots \\ \\
X_{61} & X_{62} & \cdots & X_{66}
\end{pmatrix}\]
where \(X\) is the input matrix of size \(6 \times 6\) as mentioned before;

\[ F  : = \begin{pmatrix}
w_{11} & w_{12}  & w_{13}\\ \\
w_{21} & w_{22}  & w_{23} \\ \\
w_{31} & w_{32}  & w_{33}
\end{pmatrix}\]
where \(F\) denotes one filter of size \(3 \times 3\);
\[ \beta  : = \begin{pmatrix}
w_{11} & w_{12} & \cdots & w_{16}\\
\end{pmatrix}\]
where \(\beta\) is a unrolled matrix or filter;
\[ A_{11} = (F \times X)_{11}  : =
\beta \cdot \begin{pmatrix}
w_{11} & w_{12} & w_{13} & w_{21} & w_{22} & w_{23} & w_{31} & w_{32} & w_{33}\\
\end{pmatrix}^{T}\\\]
Therefore, the first element of the feature map \(A_{11}\) can be calculated through the dot product operation shown above. Sequentially, the second element of the feature map is determined by the sliding dot product of filter and the succeeding input matrix with the same size after setting a specific value of stride, which can be considered as a moving distance. After the whole process, a feature map of size \(4 \times 4\) has been generated. Generally, there is more than one filter in the convolutional layer and each filter generates a feature map with the same size. The result of this convolutional layer is multiple feature maps (also referred to as activation map) and these feature maps corresponding to different filters are stacked together along the depth dimension.

Another improved convolutional layer was proposed by (\citet{Kalchbrenner2016NeuralMT}) and this kind of convolution is named Dilated convolution, in order to solve the problem that the pooling operation in the pooling layer will lose a lot of information. The critical contribution of this convolution is that the receptive field of the network will not be reduced by removing the pooling operation. In other words, the units of feature maps in the deeper hidden layer can still map a larger region of the original input. As illustrated in Figure \ref{fig:figs-dilated} provided by (\citet{Oord2016WaveNetAG}), although there is no pooling layer, the original input information is still increased as the layers are deeper.

\begin{figure}[ht]

{\centering \includegraphics[width=0.65\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/Temporal} 

}

\caption{Visualization of dilated causal convolutional layers}\label{fig:figs-dilated}
\end{figure}

\hypertarget{relu-layer}{%
\subsection{ReLU layer}\label{relu-layer}}

A non-linear layer (or activation layer) will be the subsequent process after each convolutional layer and the purpose of which is to introduce non-linearity to the neural networks because the operations during the convolutional layer are still linear (element-wise multiplications and summations). Generally, the major reason for introducing non-linearity is that there is a certain non-linear relationship between separate neurons. However, a convolutional layer is to perform basically a linear operation, and therefore, consecutive convolution layers are essentially equivalent to a single convolution layer, which is only used to reduce the representational power of the networks. As a result, the property of non-linearity between neurons has not been reflected and it is necessary to establish an activation function between the convolutional layer to avoid such an issue.

\textbf{Activation function}, which performs a non-linear transformation, plays a critical role in CNN to decide whether a neuron should be activated or ignored. Several activation functions are available after the convolutional layer, such as hyperbolic function and sigmoid function, etc., among of which ReLU is the most commonly used activation function in neural networks, especially in CNNs\citep{Krizhevsky2012ImageNetCW} because of its two properties:

\begin{itemize}
\tightlist
\item
  Non-linearity: ReLU is the abbreviation of Rectified Linear Unit and defined mathematically as below:
  \[ R(z)=z^{+}= max(0,z)\]
\end{itemize}

Where z denotes the output element of the previous convolutional layer. All negative values of feature maps from the previous will be replaced by setting them to zero.

\begin{itemize}
\tightlist
\item
  Non-Saturation: Saturation arithmetic is a kind of arithmetic in which all operations are limited to a fixed range between a minimum and maximum value.

  \begin{itemize}
  \tightlist
  \item
    \(f\) is non-saturating iff \((|\displaystyle{\lim_{z \to -\infty}f(z)}|=+\infty) \cup (|\displaystyle{\lim_{z \to +\infty}f(z)}|=+\infty)\)
  \item
    \(f\) is saturating iff \(f\) is not non-saturating
  \end{itemize}
\end{itemize}

As illustrated in Figure \ref{fig:figs-3}, compared with saturating activation sigmoid function that saturate at large values of the input, ReLU activation function does not saturate\citep{Krizhevsky2012ImageNetCW} and the gradient of it is 0 on the negative x-axis and 1 on the positive side, which is a benefit of using this activation function because the updates to the weights of the neural networks at each iteration are consistent with the gradient of the activation function. To be more specific, neuron`s weights will stop updating if its gradient is close to zero. It is obviously problematic if such a scenario appears too early in the training process.

\begin{figure}[ht]

{\centering \includegraphics[width=0.6\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/ReLU_sigmoid} 

}

\caption{Comparison between saturating and non-saturating activation function }\label{fig:figs-3}
\end{figure}

The following Figure \ref{fig:figs-4} based on Figure \ref{fig:figs-2} indicates a simplified version of the ReLU layer. Every single element of multiple feature maps, which is determined from the previous convolutional layer, will be further calculated by the ReLU activation function in this layer. Specifically, all positive values remain the same, and negative values are replaced by setting them to zero. The output after the ReLU layer, which has an identical network structure with the feature map from the previous convolutional layer, will be used as an input for the subsequent convolutional layer.

\begin{figure}[ht]

{\centering \includegraphics[width=0.5\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/ReLU} 

}

\caption{Basic operational structure of the ReLU layer }\label{fig:figs-4}
\end{figure}

\hypertarget{pooling-layer}{%
\subsection{Pooling layer}\label{pooling-layer}}

The pooling layer is a concept that can be intuitively understood. The purpose of the pooling layer is to reduce progressively the spatial size of the feature map, which is generated from the previous convolutional layer, and identify important features.

There are multiple pooling operations, such as average pooling, \(l_{2}-norm\) pooling, and \textbf{max pooling}, Among which max pooling is the most commonly used function(\citet{Scherer2010EvaluationOP}), and the idea of max pooling is that the exact location of a feature is less important than its rough location relative to other features (\citet{Yamaguchi1990ANN}). Simultaneously, this process helps to control overfitting to a certain extent. The following Figure \ref{fig:figs-5} illustrates an example that constructs a basic operational structure of the max pooling.

\begin{figure}[ht]

{\centering \includegraphics[width=0.8\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/Max_Pooling_finish} 

}

\caption{Basic operational structure of the max pooling layer }\label{fig:figs-5}
\end{figure}

The example mentioned above shows that two feature maps are generated according to two different filters. In this case, these feature maps of size \(4\times4\) are separated into four non-overlapping sub-regions of size \(2\times2\), and every single sub-region is named as depth slice. The Maximum value of each sub-region will be stored in the output of the pooling layer. As a result, the input dimensions are further reduced from \(4\times4\) to \(2\times2\).

Some of the most critical reasons why adding max pooling layer to neural networks include the following:

\begin{itemize}
\tightlist
\item
  \textbf{Reducing computation complexity}: Since max pooling is reducing the dimension of the given output of a convolutional layer, the networks will be able to detect larger areas of the output. This process reduces the number of parameters in the neural networks and consequently reduces computational load.
\item
  \textbf{Controlling overfitting}: Overfitting appears when the model is too complex or fits the training data too well. It may lose the true structure and then becomes difficult to generalize to new cases that are in the test data. With max-pooling operation, not all features but the primary features from each sub-region are extracted. Therefore, max-pooling operation reduces the probability of overfitting to a great extent.
\end{itemize}

Except for this most commonly applied operation in NLP, several pooling operations for different intention include the following:

\begin{itemize}
\item
  \textbf{Average pooling} is usually used for topic models. If a sentence has different topics and the researchers assume that max pooling extracts insufficient information, average pooling can be considered as an alternative.
\item
  \textbf{Dynamic pooling} proposed by (\citet{Kalchbrenner2014ACN}) has an ability to dynamically adjust the number of features according to the network structure. More specifically, by combining the adjacent word information at the bottom and passing it gradually, new semantic information is recombined at the upper layer, so that words far away in the sentence also have interactive behavior (or some kind of semantic connection). Eventually, the most important semantic information in the sentence is extracted through the pooling layer.
\end{itemize}

\hypertarget{fully-connected-layer}{%
\subsection{Fully-connected layer}\label{fully-connected-layer}}

As we mentioned in the previous section, one or more fully-connected layers are connected after multiple convolutional layers and pooling layers, each neuron in the fully connected layer is fully connected with all the neurons from the penultimate layer. The fully-connected layer, shown in Figure \ref{fig:figs-6}, can integrate local information with class distinction in the convolutional layer or pooling layer. In order to improve the CNN network performance, the excitation function of each neuron in the fully connected layer generally uses the ReLU function.

\begin{figure}[ht]

{\centering \includegraphics[width=0.6\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/Fully_Connected} 

}

\caption{Basic operational structure of the fully connected layer}\label{fig:figs-6}
\end{figure}

\hypertarget{cnn-for-sentence-classification}{%
\section{CNN for sentence classification}\label{cnn-for-sentence-classification}}

The explanation of CNN's basic architecture provided in the first sub-chapters is based on a general example. Many researchers constructed their own specific CNN models based on this basic architecture in recent years and achieved outstanding results in the field of NLP. Therefore, this section explores four superior CNN architecture with some technical detail and their performance comparison will be provided in later sub-chapters of this report.

\hypertarget{cnn-randcnn-staticcnn-non-staticcnn-multichannel}{%
\subsection{CNN-rand/CNN-static/CNN-non-static/CNN-multichannel}\label{cnn-randcnn-staticcnn-non-staticcnn-multichannel}}

The first model to explore is published by (\citet{Kim2014ConvolutionalNN}), one of the highlights of this model is that the architecture is conceptually simple and efficient when dealing with the tasks of sentiment analysis and question classification. As illustrated in Figure \ref{fig:figs-7} provided by (\citet{Kim2014ConvolutionalNN}), a simple CNN architecture of (\citet{Collobert2011NaturalLP}) with a single convolutional layer is utilized and the general architecture includes the following sub-structure:

\begin{figure}[ht]

{\centering \includegraphics[width=0.7\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/CNN_Sentence_Classification} 

}

\caption{Model architecture of CNN for sentence classification}\label{fig:figs-7}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Representation of sentence}: Assume that there are \(n\) words in a sentence, and each word is denoted as \(x_{i};\{i \in \mathbb{N} \mid 1 \leq i \leq n \}\) and \(x_{i} \in \mathbb{R}^{k}\) to the \(k\)-dimensional word vector. Therefore, a sentence can be represented as:
  \[ X_{1:n}=X_{1} \oplus X_{2} \oplus ... \oplus X_{n} \]
  Where \(\oplus\) is the concatenation operator.
\item
  \textbf{Convolutional layer}: Let a filter denote as \(w \in \mathbb{R}^{hk}\), which is used to a window of \(h\) words. A feature map \(c=[c_1,c_2,…,c_{n-h+1}]\) can be generated by:
  \[ c_i=f(w×x_{i:i+h-1}+b) \]
  where \(b \in \mathbb{R}\) is a bias term.
\item
  \textbf{Max-over-time pooling}: Pooling operation has been applied for the respective filter to select the most important feature from each feature map \(\hat{c} =max(\boldsymbol{c})\), notice that one feature \(\hat{c}\) is generated by one filter, and these features will be passed to the last layer.
\item
  \textbf{Fully connected layer}: The selected features \(\boldsymbol{Z}=[\hat{c}_1,\hat{c}_2,…,\hat{c}_j]\) from the previous layer have been flattened into a single vector, in order to aggregate each of them and therefore a specific class can be assigned to it based on the entire input.
\end{enumerate}

CNN is a feed-forward model without cyclic connection. To be more specific, the direction of information flow in a forward model is in one direction (i.e from inputs to outputs). However, the models are trained or learned by the use of backward propagation (i.e from outputs to inputs), where the gradients are recalculated at each epoch to avoid co-adaptation.

In a forward propagation of this CNN, the output unit y based on the selected features \(\boldsymbol{Z}\) is determined by using
\[y=w \cdot z+b\]
In a backward propagation, a dropout mechanism is applied as follows.
\[y=w \cdot (z \circ r)+b\]
Where \(\circ\) is the element-wise multiplication operator and \(r \in \mathbb{R}^{m}\) denotes a `masking' vector of Bernoulli random variables with probability \(p\) of being 1. As a result, gradients are backpropagated with probability \(p\) and weights \(\hat{w}\) are trained by using
\[\hat{w}=pw\]

Based on the model architecture described above, four derivative CNN models are introduced by (\citet{Kim2014ConvolutionalNN}) and the major difference among them are listed below:

\begin{itemize}
\tightlist
\item
  \textbf{CNN-rand}: All words are randomly initialized and then modified during training.
\item
  \textbf{CNN-static}: A model with pre-trained word vectors by using word2vec and keep them static.
\item
  \textbf{CNN-non-static}: A model with pre-trained word vectors by using word2vec and these word vectors are fine-tuned for each task.
\item
  \textbf{CNN-Multichannel}: A model with two channels generated by two sets of words vectors and each filter is employed to both channels.
\end{itemize}

\hypertarget{character-level-convnets}{%
\subsection{Character-level ConvNets}\label{character-level-convnets}}

The second model is published by (\citet{Zhang2015CharacterlevelCN}), the two major differences of which compared with the previous model from (\citet{Kim2014ConvolutionalNN}) include the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The model architecture with 6 convolutional layers and 3 fully-connected layers (9 layers deep) is relatively more complex.
\item
  Different from the previous word-based Convolutional neural networks (ConvNets), this model is at character-level by using character quantization.
\end{enumerate}

\begin{figure}[ht]

{\centering \includegraphics[width=0.7\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/Character_level_CNN} 

}

\caption{\label{fig:fig_8} Model architecture of character-level CNN}\label{fig:figs-8}
\end{figure}

The Figure \ref{fig:figs-8} above provided by (\citet{Zhang2015CharacterlevelCN}) shows the basic architecture of Character-level ConvNets, the corresponding explanation of the main components will be provided below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Character quantization}: The input characters will be transformed into an encoding matrix of size \(m \times l_0\) by using 1-of-\(m\) encoding (or ``one-hot'' encoding). To be noticed that, The length of the character exceeds the determined value \(l_0\) and will be ignored and the black characters as well as the characters that are not in the alphabet will be quantized as zero vectors.
\item
  \textbf{Temporal convolutional module}: A sequence of encoded characters followed by a temporal convolutional module, which is a variation over Convolutional Neural Networks works for sequence modelling tasks. To be more specific, when sentiment analysis is performed by using ConvNets, a fixed-size input will be a precondition, we can adjust the initial input length by truncating or padding the actual input to satisfy this criterion without affecting the sentiment and sequentially generate fixed-size outputs. Conceptually, this kind of 1-D convolution is called temporal convolution and the convolutional function defined as follow:
  \[h(y)=\sum_{x=1}^{k}f(x)\cdot g(y \cdot d -x+c)\]
  where \(h_{j}(y)\) denotes outputs of the convolutional layer; a discrete kernel functions \(f_{i,j} \in [1,k] \to \mathbb{R}\) (\((i=1,...,m\) and \(j=1,...,n)\)) is also called weights; \(d\) is denoted as stride; \(c= k-d+1\) is used as an offset constant.
\item
  \textbf{Temporal max-pooling}: Based on the research of (\citet{Boureau2010ATA}), a 1-D version of the max-pooling \(h(y)\) is employed in this ConvNets, which is defined as
\end{enumerate}

\[h(y)=max (g(y \cdot d -x+c))\]
With the help of this pooling function, it is possible to train ConvNets deeper than 6 layers.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{ReLU layer}: the activation function used in this model is similar to ReLU \(h(x)=max\{0,x\}\). More specifically, the algorithm is stochastic gradient descent (SGD). However, SGD is influenced by the strong curvature of the optimization function and moves slowly towards the minimum. Therefore, based on the research of \citet{Sutskever2013OnTI}, a momentum of 0.9 and an initial step size 0.01 are established to reach the minimum more quickly.
\end{enumerate}

\hypertarget{very-deep-cnn}{%
\subsection{Very Deep CNN}\label{very-deep-cnn}}

The third character-level model to explore is from \citet{Schwenk2017VeryDC}. Inspired by (\citet{Simonyan2015VeryDC}), a CNN model with deep architectures of many convolutional layers is developed, the significant difference of which from the previous model architecture is that this model applies much deeper architectures (i.e.~using up to 29 convolutional layers), in order to learn hierarchical representations of whole sentences. The overall architecture will be explored based on Figure \ref{fig:figs-9} and \ref{fig:figs-10} also provided by (\citet{Schwenk2017VeryDC}).

\begin{figure}[ht]

{\centering \includegraphics[width=1\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/Convolutional_Block} 

}

\caption{\label{fig:fig_9} Architecture of convolutional block}\label{fig:figs-9}
\end{figure}

The above Figure \ref{fig:figs-9} shows an architecture of a convolutional block with 256 feature maps and the kernel size of all the convolutions is 3. The convolutional block is composed of two consecutive convolutional layers, and each one followed by a temporal BatchNorm layer (\citet{Ioffe2015BatchNA}) and a ReLU activation.

\textbf{Batch normalization}, as the name suggests, is a normalized operation commonly used to improve the speed and stability of neural networks, especially for the deep neural network. The following shows the algorithm of Batch Normalizing Transform.

For a layer with \(d\)-dimensional input \(x = (x^{(1)},...,x^{(d)})\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mu_B = \frac 1 m \sum_{i=1}^m x_i\) //mini-batch mean
\item
  \(\sigma_B^2 = \frac 1 m \sum_{i=1}^m (x_i-\mu_B)^2\) // mini-batch variance
\item
  \(\hat{x}_{i}^{(k)} = \frac {x_i^{(k)}-\mu_B^{(k)}}{\sqrt{\sigma_B^{(k)^2}+\epsilon}}\) // normalize
\end{enumerate}

where \(k \in [1,d]\) and \(i \in [1,m]\); \(\mu_B^{(k)}\)and \(\sigma_B^{(k)^2}\) are the per-dimension mean and variance; \(\epsilon\) is an arbitrarily small constant.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \(y_i^{(k)} = \gamma^{(k)} \hat{x}_{i}^{(k)} +\beta^{(k)}\) // scale and shift
\end{enumerate}

where the parameters \(\gamma^{(k)}\)and \(\beta^{(k)}\) are subsequently learned in the optimization process.

To be more specific, in the SGD training process, use mini-batch to normalize the corresponding activation so that the mean value of the results (all dimensions of the output signal) is \(0\) and the variance is \(1\). Subsequently, two independent learnable parameters \(\beta\) and \(\gamma\) will be introduced in the final operation ``scale and shift''.

\begin{figure}[ht]

{\centering \includegraphics[width=0.65\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/VDCNN} 

}

\caption{\label{fig:fig_10} Very Deep Convolutional Networks architecture}\label{fig:figs-10}
\end{figure}

The overall architecture of VDCNN is shown in Figure \ref{fig:figs-10} above, The first layer contains 64 convolutions of size 3 after importing the initial text, a series of convolutional blocks are connected after the convolutional layer, and the number of feature maps is determined by two rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If the temporal resolution of output remains the same，the layers have the same number of feature maps.
\item
  If the temporal resolution is halved, the number of feature maps is doubled.
\end{enumerate}

Based on the above rules, the size of resolution will be halved after each pooling operation, so the number of feature maps will be corresponding to double from 128 to 512.

\textbf{k-max pooling}: The result of k-max pooling is not to return a single maximum value, but to return k sets of maximum values, which are a subsequence of the original input. The parameter \(k\) in the pooling can be a dynamic function, and this specific value depends on the input or other parameters of the network. the specific dynamic function is as follows.

\[k_{l}=max(k_{top},\lceil \frac{L-l}{L}s \rceil)\]
In this formula, \(s\) denotes the length of the sentence, \(L\) represents the total number of convolutional layers, and \(l\) represents the number of convolutional layers that are currently in. so it can be seen that \(k\) varies with the length of the sentence and the depth of the network change. The advantage of K-max pooling is that it not only extracts more than one important information in the sentence but also retains their order.

The output of k-max pooling is transformed into a single vector which will be used as input to a three fully-connected layer with ReLU activation function and softmax outputs.

\hypertarget{deep-pyramid-cnn}{%
\subsection{Deep Pyramid CNN}\label{deep-pyramid-cnn}}

The last model we will discuss in this chapter is Deep Pyramid Convolutional Neural Networks (DPCNN), which is published by (\citet{Johnson2017DeepPC}). The major difference from the previous deep CNN is that this model is constructed of a low-complexity word-level deep CNN architecture. The motivation for using such an architecture based on the point of view of (\citet{Johnson2017DeepPC}) includes the following:

\textbf{low-complexity}: As the research of (\citet{Johnson2016ConvolutionalNN}) shows that very shallow 1-layer word-level CNNs such as (\citet{Kim2014ConvolutionalNN}) performs more accurate and much faster than the deep character-level CNNs of (\citet{Schwenk2017VeryDC}).

\textbf{word-level}: word-level CNNs are more accurate and much faster than the state-of-the-art very deep networks such as character-level CNNs even in the setting of large training data.

The description of key features of DPCNN will be provided based on the Figure \ref{fig:figs-11} from (\citet{Johnson2017DeepPC}) as follows.

\begin{figure}[ht]

{\centering \includegraphics[width=0.35\linewidth]{figures/01-03-cnns-and-their-applications-in-nlp/DPCNN} 

}

\caption{\label{fig:fig_11} Architecture of Deep Pyramid CNN}\label{fig:figs-11}
\end{figure}

\textbf{Text region embedding with unsupervised embeddings}: This technique is based on the basic region embedding. More specifically, the embedding generated after performing a set of convolution operations on different sizes of text area/segment (such as bi-grams or tri-grams) and these convolution operations are achieved by using multi-size learnable convolutional filters. There are two options when performing convolution operation on a text area, For example, by using tri-grams, one is to preserve the word order, which means that setting a set of 2D convolution kernels with size= \(3 \times D\) to convolve the 3 words (where D is the word embedding dimension), another is not to preserve the word order (i.e.~using the Bag-of-words model). Preliminary experiments indicated that considering word order does not significantly help to improve accuracy and increase computational complexity. Therefore, DPCNN adopts a method similar to the Bag-of-words model. In addition to this, to avoid the overfitting caused by high representation power of n-gram, a technique called tv-embedding (two-views embedding) will be introduced in DPCNN, which means that a region of text as view-1 and its adjacent regions as view-2 will be pre-defined, and view-1 will be trained to predict view-2 in one hidden layer and it will be considered as input, this process is called unsupervised embedding.

\textbf{Shortcut connections with pre-activation} : From experience, the depth of the network is critical to the performance of the model，Because by giving the model more parameters, the model can fit the train data at least as well as before and detect more complex feature patterns. However, the experimental results show that the training accuracy will decrease as the depth of the network increases, the reason is Degradation problem or so-called vanishing/exploding gradients. More specifically, Assume we consider a shallow architecture and add more layers linked by identity mapping (i.e. \(f(z) = z\)). The value of the output after multiple nonlinear layers should also be z, but the result will be biased due to the existence of this Degradation problem. Therefore, the shortcut connections were proposed by (\citet{He2016DeepRL}). Formally, denoting the desired underlying mapping as \(H(z)\), \(f(z) := H(z)−z\) is denoted as a residual function, so the original mapping is recast into \(f(z)+z\). This technique is used to solve the problem of depth model accuracy.

\textbf{Downsampling with the number of feature maps fixed}: Similar to the VDCNN described in the previous sub-chapter is that multiple convolutional blocks are also introduced to the architecture of DPCNN. However, the difference is that the number of feature maps is fixed instead of increasing like VDCNN, because (\citet{Johnson2017DeepPC}) thinks increasing the number of feature maps will lead to increased computation time substantially without accuracy improvement, Therefore, the computation time and sample size for each convolution layer are halved (Downsampling).

\hypertarget{datasets-and-experimental-evaluation}{%
\section{Datasets and Experimental Evaluation}\label{datasets-and-experimental-evaluation}}

A brief description of the eight data sets will be provided in this sub-chapter, based on which the performance comparison of all four models mentioned above will also be listed.

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

The comparison results of all CNN models explored in this chapter are based on the eight datasets complied by (\citet{Zhang2015CharacterlevelCN}), summarized in the following table.

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Data & AG & Sogou & Dbpedia & Yelp.p & Yelp.f & Yahoo & Ama.f & Ama.p\tabularnewline
\midrule
\endhead
\# of training documents & 120K & 450K & 560K & 560K & 650K & 1.4M & 3M & 3.6M\tabularnewline
\# of test documents & 7.6K & 60K & 70K & 38K & 50K & 60K & 650K & 400K\tabularnewline
\# of classes & 4 & 5 & 14 & 2 & 5 & 10 & 5 & 2\tabularnewline
Average \#words & 45 & 578 & 55 & 153 & 155 & 112 & 93 & 91\tabularnewline
\bottomrule
\end{longtable}

Based on the description of the datasets of (\citet{Johnson2017DeepPC}), \textbf{AG} and \textbf{Sogou} are news. \textbf{Dbpedia} is an ontology. \textbf{Yahoo} consists of questions and answers from the `Yahoo! Answers' website. \textbf{Yelp} and \textbf{Amazon (`Ama')} are reviews where \texttt{.p} (polarity) in the names indicates that labels are binary (positive/negative), and \texttt{.f} (full) indicates that labels are the number of stars. \textbf{Sogou} is in Romanized Chinese, and the others are in English. Classes are balanced on all the datasets. Furthermore, These eight datasets can be subdivided into two groups, the first four datasets are relatively smaller datasets and the rest of them can be classified as relatively larger data sets.

\hypertarget{experimental-evaluation}{%
\subsection{Experimental Evaluation}\label{experimental-evaluation}}

The following table provided by (\citet{Johnson2017DeepPC}) shows the error rates in percentage on datasets described in the previous sub-chapter in comparison with all models. To be more specific, a lower error rate represents better model performance and the corresponding best results are marked in bold. \texttt{tv} stands for tv-embeddings. \texttt{w2v} stands for word2vec. \texttt{(w2v)} indicates that the best results among those with and without word2vec pretraining are shown. Note that \texttt{best} next to the model name is denoted as the model by choosing the best test error rate among several variations presented in the respective papers.

\begin{longtable}[]{@{}lcrcrrrrrrr@{}}
\toprule
Models & Deep & Unsup. embed. & AG & Sogou & Dbpedia & Yelp.p & Yelp.f & Yahoo & Ama.f & Ama.p\tabularnewline
\midrule
\endhead
DPCNN + unsupervised embed.(\citet{Johnson2017DeepPC}) & \(\surd\) & tv & 6.87 & \textbf{1.84} & 0.88 & \textbf{2.64} & \textbf{30.58} & \textbf{23.90} & \textbf{34.81} & \textbf{3.32}\tabularnewline
ShallowCNN + unsup. embed.(\citet{Kim2014ConvolutionalNN}) & & tv & \textbf{6.57} & 1.89 & \textbf{0.84} & 2.90 & 32.39 & 24.85 & 36.24 & 3.79\tabularnewline
Very Deep char-level CNN: best (\citet{Schwenk2017VeryDC}) & \(\surd\) & & 8.67 & 3.18 & 1.29 & 4.28 & 35.28 & 26.57 & 37.00 & 4.28\tabularnewline
fastText bigrams (\citet{Joulin2017BagOT}) & & & 7.5 & 3.2 & 1.4 & 4.3 & 36.1 & 27.7 & 39.8 & 5.4\tabularnewline
{[}ZZL15{]}'s char-level CNN: best(\citet{Zhang2015CharacterlevelCN}) & \(\surd\) & & 9.51 & 4.88 & 1.55 & 4.88 & 37.95 & 28.80 & 40.43 & 4.93\tabularnewline
{[}ZZL15{]}'s word-level CNN : best(\citet{Zhang2015CharacterlevelCN}) & \(\surd\) & (w2v) & 8.55 & 4.39 & 1.37 & 4.60 & 39.58 & 28.84 & 42.39 & 5.51\tabularnewline
{[}ZZL15{]}'s linear model: best(\citet{Zhang2015CharacterlevelCN}) & & & 7.64 & 2.81 & 1.31 & 4.36 & 40.14 & 28.96 & 44.74 & 7.98\tabularnewline
\bottomrule
\end{longtable}

Based on the comparison results shown above, some of the significant results include the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  DPCNN achieves an outstanding performance, which shows the lowest error rate in six of the eight datasets.
\item
  Shallow CNN, which possesses the simplest model architecture with only one convolutional layer, performs even better than other deep models.
\item
  By mere comparison of character-level CNN (\citet{Zhang2015CharacterlevelCN}) and word-level CNN (\citet{Zhang2015CharacterlevelCN}), the result will be that words-level CNN performs better in the smaller datasets and character-level CNN performs better in the bigger datasets.
\end{enumerate}

\hypertarget{conclusion-and-discussion}{%
\section{Conclusion and Discussion}\label{conclusion-and-discussion}}

In summary, this chapter introduces the architecture of multiple CNN models and compares the performance of these models in the application of NLP especially for text categorization in different scale datasets. Subsequently, we will discuss the results obtained from the previous table in two ways including a comparison between the character-level approach and the word-level approach and depth of the model.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Character-level and word-level} Extending from the experimental studies and the corresponding comparison results shown in the previous sub-chapter, word-level CNN possesses higher accuracy presented in lower error rate in comparison of character-level CNNs in general, although character-level CNN holds an advantage in not having to deal with millions of distinct words, word-level approach displays higher accuracy than character-level because the advantage of word-level is that Word can represent the meaning.
\item
  \textbf{Deep and shallow model architecture} DPCNN, which shows the best test error rate among all CNN models discussed in this chapter, could be considered as a kind of deeper Shallow CNN. Consequently, increasing the depth of the model can lead to an increase in parameters and increased capacity to handle complex feature patterns. Better performance of DPCNN compared with Shallow CNN evidence that increasing depth can improve accuracy to a certain extent. However, it comes to the opposite conclusion by comparing with Very Deep CNN since the probability of potential issues such as vanishing/exploding gradients will increase as the number of layers increases. Therefore, it is essential to improve the accuracy of complex and deep models by introducing appropriate components (e.g.~shortcut connections).
\end{enumerate}

\hypertarget{introduction-transfer-learning-for-nlp}{%
\chapter{Introduction: Transfer Learning for NLP}\label{introduction-transfer-learning-for-nlp}}

\emph{Authors: Carolin Becker, Joshua Wagner, Bailan He}

\emph{Supervisor: Matthias Aßenmacher}

As discussed in the previous chapters, natural language processing (NLP) is a very powerful tool in the field of processing human language. In recent years, there have been many proceedings and improvements in NLP to the state-of-art models like BERT. A decisive further development in the past was the way to transfer learning, but also self-attention.

In the next three chapters, various NLP models will be presented, which will be taken to a new level with the help of transfer learning in a first and a second step with self-attention and transformer-based model architectures. To understand the models in the next chapters, the idea and advantages of transfer learning are introduced. Additionally, the concept of self-attention and an overview over the most important models will be established

\hypertarget{what-is-transfer-learning}{%
\section{What is Transfer Learning?}\label{what-is-transfer-learning}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-00-transfer-learning-for-nlp/compare-classical-transferlearning-ml} 

}

\caption{Classic Machine Learning and Transfer Learning}\label{fig:ch02-figure01}
\end{figure}



In figure \ref{fig:ch02-figure01} the difference between classical machine learning and transfer learning is shown.

For classical machine learning a model is trained for every special task or domain.
Transfer learning allows us to deal with the learning of a task by using the existing labeled data of some related tasks or domains. Tasks are the objective of the model. e.g.~the sentiment of a sentence, whereas the domain is where data comes from. e.g.~all sentences are selected from Reddit. In the example above, knowledge gained in task A for source domain A is stored and applied to the problem of interest (domain B).

Generally, transfer learning has several advantages over classical machine learning: saving time for model training, mostly better performance, and not a need for a lot of training data in the target domain.

It is an especially important topic in NLP problems, as there is a lot of knowledge about many texts, but normally the training data only contains a small piece of it. A classical NLP model captures and learns a variety of linguistic phenomena, such as long-term dependencies and negation, from a large-scale corpus. This knowledge can be transferred to initialize another model to perform well on a specific NLP task, such as sentiment analysis. \citep{evolutiontransferlearning}

\hypertarget{self-attention}{%
\section{(Self-)attention}\label{self-attention}}

The most common models for language modeling and machine translation were, and still are to some extent, recurrent neural networks with long short-term memory \citep{hochreiter1997long} or gated recurrent units \citep{gru}. These models commonly use an encoder and a decoder archictecture. Advanced models use attention, either based on Bahdanau's attention \citep{bahdanau2014neural} or Loung's attention \citep{luong2015effective}.

\citet{vaswani2017attention} introduced a new form of attention, self-attention, and with it a new class of models, the \textit{Transformers}. A Transformer still consists of the typical encoder-decoder setup but uses a novel new architecture for both. The encoder consists of 6 Layers with 2 sublayers each. The newly developed self-attention in the first sublayer allows a transformer model to process all input words at once and model the relationships between all words in a sentence. This allows transformers to model long-range dependencies in a sentence faster than RNN and CNN based models. The speed improvement and the fact that ``individual attention heads clearly learn to perform different tasks'' \citet{vaswani2017attention} lead to the eventual development of \textbf{B}idirectional \textbf{E}ncoder \textbf{R}epresentations from \textbf{T}ransformers by \citet{bert}. \textbf{BERT} and its successors are, at the time of writing, the state-of-the-art models used for transfer learning in NLP. The concepts attention and self-attention will be further discussed in the \protect\hyperlink{Attention-and-self-Attention-for-nlp}{\textbf{``Chapter 9: Attention and Self-Attention for NLP''}}.

\hypertarget{overview-over-important-nlp-models}{%
\section{Overview over important NLP models}\label{overview-over-important-nlp-models}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-00-transfer-learning-for-nlp/overview-tranferlearning} 

}

\caption{Overview of the most important models for transfer learning}\label{fig:ch02-figure02}
\end{figure}



The models in figure \ref{fig:ch02-figure02} will be presented in the next three chapters.

First, the two model architectures ELMo and ULMFit will be presented, which are mainly based on transfer learning and LSTMs, in \protect\hyperlink{Transfer-Learning-for-NLP-I}{\textbf{Chapter 8: ``Transfer Learning for NLP I''}}:

\begin{itemize}
\item
  \textbf{ELMo} (Embeddings from Language Models) first published in \citet{elmopaper} uses a deep, bi-directional LSTM model to create word representations. This method goes beyond traditional embedding methods, as it analyses the words within the context
\item
  \textbf{ULMFiT} (Universal Language Model Fine-tuning for Text Classification) consists of three steps: first, there is a general pre-training of the LM on a general domain (like WikiText-103 dataset), second, the LM is finetuned on the target task and the last step is the multilabel classifier fine tuning where the model provides a status for every input sentence.
\end{itemize}

In the \protect\hyperlink{Transfer-Learning-for-NLP-II}{\textbf{``Chapter 10: Transfer Learning for NLP II''}} models like BERT, GTP2 and XLNet will be introduced as they include transfer learning in combination with self-attention:

\begin{itemize}
\item
  \textbf{BERT} (Bidirectional Encoder Representations from Transformers \citet{bert}) is published by researchers at Google AI Language group.
  It is regarded as a milestone in the NLP community by proposing a bidirectional Language model based on Transformer. BERT uses the Transformer Encoder as the structure of the pre-train model and addresses the unidirectional constraints by proposing new pre-training objectives: the ``masked language model''(MLM) and a ``next sentence prediction''(NSP) task. BERT advances state-of-the-art performance for eleven NLP tasks and its improved variants \textbf{Albert} \citet{lan2019albert} and \textbf{Roberta} \citet{liu2019roberta} also reach great success.
\item
  \textbf{GPT2} (Generative Pre-Training-2, \citet{radford2019gpt2}) is proposed by researchers at OpenAI. GPT-2 is a tremendous multilayer Transformer Decoder and the largest version includes 1.543 billion parameters. Researchers create a new dataset ``WebText'' to train GPT-2 and it achieves state-of-the-art results on 7 out of 8 tested datasets in a zero-shot setting but still underfits ``WebText''.
\item
  \textbf{XLNet} is proposed by researchers at Google Brain and CMU\citep{yang2019xlnet}. It borrows ideas from autoregressive language modeling (e.g., Transformer-XL \citet{dai2019transformer}) and autoencoding (e.g., BERT) while avoiding their limitations. By using a permutation operation during training, bidirectional contexts can be captured and make it a generalized order-aware autoregressive language model. Empirically, XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.
\end{itemize}

\hypertarget{transfer-learning-for-nlp-i}{%
\chapter{Transfer Learning for NLP I}\label{transfer-learning-for-nlp-i}}

\emph{Author: Carolin Becker}

\emph{Supervisor: Matthias Aßenmacher}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

As mentioned in the introduction, the field of natural language processing (NLP) has seen rapid advancements due to the growing usage of transfer learning.
A first enhancement began in 2018, where the transfer learning improved several fields in NLP, as the model layers have not to be trained from scratch. Word2Vec, which was explained earlier ({[}chapter??{]}), is one example of transfer learning.

Scientific papers and research introduced many ideas like contextual word embeddings or fine-tuning. In the language modelling these changes led to a better performance in terms of errors and time.

\hypertarget{transfer-learning-in-nlp}{%
\section{Transfer Learning in NLP}\label{transfer-learning-in-nlp}}

{[}Different types of transfer learning{]}

{[}Mostly, we deal with sequential inductive transfer learning.{]}

\hypertarget{steps-in-sequential-inductive-transfer-learning}{%
\section{Steps in sequential inductive transfer learning}\label{steps-in-sequential-inductive-transfer-learning}}

{[}Pan and Young (2010){]} divided \textbf{transfer learning} in different subfields: if the tasks are the same (\emph{transductive learning}) or if the target and source task shares the same domain (\emph{inductive transfer learning}). In the following, the focus will be on \emph{sequential transfer learning} where tasks are learned sequentially, whereas, in multi-task learning, the tasks are learned simultaneously.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/02-01-transfer-learning-for-nlp-1/sequential-transfer-learning} 

}

\caption{(ref:ch21-figure1)}\label{fig:ch21-figure01}
\end{figure}

In the first step, all models are \textbf{pre-trained} on an extensive source data set, which is, in the best case, very close to the target task ({[}Peter et al., 2019{]}). The pre-trained language models in this chapter are uni-directional models that predict the next word.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/02-01-transfer-learning-for-nlp-1/pretrained-lm} 

}

\caption{(ref:ch21-figure7)}\label{fig:ch21-figure07}
\end{figure}

In a second step, follows the \textbf{adoption} on the target task. Here, the main distinction is, if the pre-trained model architecture is kept (\textbf{embedding} or \textbf{feature extraction}) or the pre-trained model is adjusted to the target task (\textbf{fine-tuning}). {[}SOURCE??{]}

In \textbf{Embeddings}, single parts, which can be sentences or characters, are extracted to a fixed-length matrix with the dimensions \(\mathbb{R}^{n} \times k\) where \(k\) is the fixed-length. This matrix represents the context of every word given of every other word. So in the adoption phase, the weights in the LM do not change, and just the top layer of the model is used. The adopted model learns a linear combination of the top layer.

On the other side, \textbf{fine-tuning} trains the weights of the pre-trained model on a specific task, which makes it much more flexible and needs no specific adjustment. The disadvantage of this method is that the general knowledge and relationship between words can get lost in the adjustment phase. The term for that is the ``catastrophic forgetting'' (McCloskey \& Cohen, 1989; French, 1999). Techniques for preventing this are freezing, learning rates, and regularization.

{[}explaining freezing learning rates and regularization xxxxxxx{]}

\hypertarget{most-popular-models}{%
\section{Most popular models}\label{most-popular-models}}

In the following sections, the models \textbf{ELMo}, \textbf{ULMFiT}, and \textbf{GPT} are presented, which shaped the ``first wave'' of transfer learning before transformers like BERT developed and became popular.

\hypertarget{elmo---the-new-age-of-embeddings}{%
\subsection{ELMo - The ``new age'' of embeddings}\label{elmo---the-new-age-of-embeddings}}

In 2018, {[}Peter et al.~2018{]} from AllenNLP introduced \textbf{Embeddings from Language Models} (ELMo), which most significant advance compared to previous models like word2vec and Glove (chapter??) is that ELMo can handle the different meanings of a word in a context (polysemy). For instance, is the meaning of the word ``mouse'' in the context of computers, a device with which you can control the cursor of your PC. Instead, in the context of animals, it means this small animal living in gardens. Until ELMo, this could not be captured by an NLP model, but with ELMo, the different meaning of the words is taken. For this purpose, ELMo can model the semantical and the synthetical characteristics of a word (like word2vec) but also the varying meanings in different contexts.

In contrast to previous word embeddings word, the representations of ELMo are functions of the whole input sentence. These representations are calculated on top of a biLMs (bidirectional language model) with two-layer character convolutions (1) and are a linear combination of the internal network states (2). {[}+ clearer why trnasfer learning{]}

\hypertarget{bidirectional-language-model-bilm}{%
\subsubsection{Bidirectional language model (biLM)}\label{bidirectional-language-model-bilm}}

ELMo is based on the shallow concatenation of independently trained left-to-right and right-to-left multi-layer LSTMs. Bidirectional is, in this case, misleading, as the two steps occur independently from each other.
A forward language model calculates the probability of a sequential token (word or character) \(t_{k}\) at the position \(k\) with the provided history \(t_{1}, \ldots, t_{k-1}\) with:

\[
\begin{aligned}
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)
\end{aligned}
\]

The backward LM can be defined accordingly to the forward LM:

\[
\begin{aligned}
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)
\end{aligned}
\]

In both directions, a context-independent token representation \(\mathbf{x}_{k}^{L M}\) by either token embedding or a CNN over characters is computed and passed through a forward/backward LSTM. Additionally, at each position \(k\) every LSTM layer \(j\) outputs a context-dependent representation \(\overrightarrow{\mathbf{h}}_{k, j}^{L M}\) (or \(\overleftarrow{\mathbf{h}}_{k, j}^{L M}\) in the backward direction).

In the forward direction, a next token \(t_{k+1}\) can predict the top layer \(\overrightarrow{\mathbf{h}}_{k, L}^{L M}\) with a Softmax layer. In the biLM the direction are combined and optimized with a log likelihood:

\[\begin{array}{l}
\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)\right. \\
\quad+\log p\left(t_{k} | t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)
\end{array}\]

where \(\Theta_{s}\) are the parameters for the token representations and
\(\Theta_{x}\) are the parameters of the Softmax-layer.

{[} Add some more explanation to the formulas (maybe in combination with the graph) because it's really hard to understand this part.{]}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/02-01-transfer-learning-for-nlp-1/elmo} 

}

\caption{ELMo}\label{fig:ch21-figure08}
\end{figure}

\hypertarget{elmo-representation}{%
\subsubsection{ELMo representation}\label{elmo-representation}}

The ELMo specific task is formulated by

\[\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M},\]

where \(\gamma\) is the optimization parameter which allows to scale the model, \(s_{j}^{t a s k}\) are ``softmax-normalized weights'' and \(R_{k}\) is the representation of the tokens \(k\):

\[\begin{aligned}
R_{k} &=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} | j=1, \ldots, L\right\} \\
&=\left\{\mathbf{h}_{k, j}^{L M} | j=0, \ldots, L\right\}
\end{aligned}.\]

For every task (question answering, sentiment analysis, etc.) the ELMo representation needs a task-specific calculation {[}Further explanation\ldots{}{]}

\hypertarget{ulmfit---cutting-edge-model-using-lstms}{%
\subsection{ULMFiT - cutting-edge model using LSTMs}\label{ulmfit---cutting-edge-model-using-lstms}}

Also, 2018, {[}Howard and Ruder (2018){]} proposed \textbf{Universal language model fine-tuning (ULMFiT)}, which exceeded many of the cutting-edge models in text classification, as it decreased the error by 18-24\% in most of the datasets.
ULMFiT is based on an AWD-LSTM (ASGD Weight-Dropped LSTM) combined with novel techniques like ``discriminative fine-tuning,'' ``slanted triangular learning rates,'' and ``gradual unfreezing of layers.'' Hence, it can fine-tune a generalized language model to a specific language model for multiple tasks.

\hypertarget{awd-lstm}{%
\subsubsection{AWD-LSTM}\label{awd-lstm}}

As language models with many parameters tend to overfit, {[}Merity (){]} introduced the \textbf{AWD-LSTM}, a highly effective version of the Long Short Term Memory (LSTM, {[}chapter ???{]}). The Dropconnect Algorithm and the Non-monotonically Triggered ASGD (NT-ASGD) are two main improvements of this model architecture.

As in figure {[}??{]} The \textbf{Dropconnect Algorithm} (Wan 2013) regularizes the LSTM and prevents overfitting by setting the activation of units randomly to zero with a predetermined probability of \(p\). So, only a subset of the units from the previous layer is passed to every unit. However, by using this method also long-term dependencies go lost. That is why the algorithm drops weights and not the activations in the end with the probability of \(1-p\). As the weights are set to zero, the drop connect algorithm reduces the information loss, while it reduces overfitting. {[}correct confunfusion with dropout/dropconnect{]}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/02-01-transfer-learning-for-nlp-1/dropconnect_algorithm} 

}

\caption{Dropconnect Algorithm}\label{fig:ch21-figure02}
\end{figure}

To improve the optimization in the AWD-LSTM further, {[}Merity (){]} introduced the \textbf{Non-monotonically Triggered Average SGD} (or NT-ASGD), which is a new variant of Average Stochastic Gradient Descent (ASDG). The Average Stochastic gradient descent takes a gradient descent step, as the gradient descent algorithm. However, it also takes the weight of the previous iterations into account and returns the average. On the contrary, the NT-ASGD only takes the averaged previous iterations into account, if the validation metric does not improve for several steps. Subsequently, The SGD turns into an ASGD if there is no improvement for n steps.

{[}formula for ASGD{]}

In addition to the enhancements above the AWD-LSTM, the authors of the paper propose several other regularization and data efficiency methods: Variable Length Backpropaation Sequences (BPTT), Variational Dropout, Embedding Dropout, Reduction in Embedding Size, Activation Regularization, Temporal Activation Regularization. For further information, the paper is a great way to start. {[}Mehr hier???{]}

\hypertarget{the-three-steps-of-ulmfit}{%
\subsubsection{The three steps of ULMFiT}\label{the-three-steps-of-ulmfit}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/02-01-transfer-learning-for-nlp-1/ulmfit-overview} 

}

\caption{Three steps of ULMFiT}\label{fig:ch21-figure03}
\end{figure}

ULMFiT follows three steps to achieve these notable transfer learning results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{LM pre-training}: The AWD-LSTM (language model) is trained on general-domain data like the Wikipedia data set.
\item
  \textbf{LM fine-tuning}: The model is fine-tuned on the tasks' dataset. For this purposed {[}Howard and Ruder(2018){]} proposed two training techniques to stabilize the fine-tuning process:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Discriminative fine-tuning}
    Considering distinctive layers of LM capture distinct types of information, ULMFiT proposed to tune each layer with different learning rates.
  \item
    \textbf{Slanted triangular learning rates} (STLR)
    STLR is a particular learning rate scheduling that first linearly increases the learning rate, and then gradually declines after a cut. That leads to an abrupt increase and a more extensive decay like in figure ??:
  \end{itemize}
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{figures/02-01-transfer-learning-for-nlp-1/ulmfit-stlr} 

}

\caption{Slanted triangular learning rates}\label{fig:ch21-figure06}
\end{figure}

The learning rates \(\eta_{t}\) are calculated by the number of iterations \(T\): \newline

\[\begin{aligned}
c u t &=\left\lfloor T \cdot c u t_{-} f r a c\right\rfloor \\
p &=\left\{\begin{array}{ll}
t / c u t, & \text { if } t<c u t \\
1-\frac{t-c u t}{c u t \cdot\left(1 / c u t_{-} f r a c-1\right)}, & \text { otherwise }
\end{array}\right.\\
\eta_{t} &=\eta_{\max } \cdot \frac{1+p \cdot(r a t i o-1)}{\text {ratio}}, 
\end{aligned}\]

where \(c u t_{-} f r a c\) is the increasing learning rate factor, \(c u t\) the iteration where the decreasing is started, \(p\) the fraction of the number of iterations that are increased or decreased, \(ratio\) is the ratio the difference between the lowest and highest learning rate

{[}add aim of it!{]}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Classifier fine-tuning}: In the last step, the LM is expanded with two common feed-forward layers and a softmax normalization at the end to predict a target label distribution. Again, two new techniques are submitted:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Concat pooling}
    Concat pooling elicits ``max-pooling and mean-pooling over the history of hidden states and concatenates them with the final hidden state.'':{[}formula for concat pooling{]}
  \item
    \textbf{Gradual unfreezing}
    A common problem of retraining the model is losing information about the general data (Wikipedia), which is called ``catastrophic forgetting.'' Hence, gradual unfreezing the model will be trained step by step, starting from the last layer. So first, all layers are ``frozen'' except the last layer (not tuned). In every step, one additional layer is ``unfrozen.''
  \end{itemize}
\end{enumerate}

\hypertarget{gpt---first-step-towards-transformers}{%
\subsection{GPT - First step towards transformers}\label{gpt---first-step-towards-transformers}}

{[} Radford-et-al(2018){]} from Open AI published \textbf{Generative Pre-Training} (GPT). GPT's idea is very similar to ELMo, as it expands the unsupervised LM to a much larger scale, as it uses pre-trained models and transfer learning.

Despite the similarity, GPT has three significant differences to ELMo:

\begin{verbatim}
* ELMo is based on word embeddings, whereas GPT is based on fine-tuning like ULMFiT. 

* GPT uses a different model architecture. Instead of the multi-layer LSTM, GPT is a multi-layer decoder. A model architecture will be explained in the upcoming chapters, as it is a significant step towards the state-of-the-art NLP models. 

* In contrast to ELMo, that works character-wise, GPT uses tokens (subwords) from the words.
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/02-01-transfer-learning-for-nlp-1/gpt} 

}

\caption{GPT}\label{fig:ch21-figuregpt}
\end{figure}

As shown in figure \ref{fig:ch21-figuregpt} GPT is a uni-directional transformer-decoder with 12 layers which have two sublayers connected by a feed-forward network:
First, the model is pre-trained by thousand books from Google books. Then, the parameters of the model are adopted on the specific task.

As the basic idea of transformers is discussed in the following chapters, further explanations of the functionality of the transformer model architectures will follow.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

In 2018, a new generation of NLP models had been published, as transfer learning mainly pushed further enhancements from computer vision.
The \textbf{main advances} of these models are

\begin{itemize}
\tightlist
\item
  due to the use of \textbf{transfer learning} the training for the target task needs less time and less target specific data,
\item
  \textbf{ELMo} adds the contextualization to word embeddings,
\item
  \textbf{ULMFiT} introduces many ideas like fine-tuning, which undoubtedly lowered the error rate notable, and
\item
  \textbf{GPT} uses first the transformer model architecture, which cutting-edge NLP models use.
\end{itemize}

Besides, many features of these models show high \textbf{potential for improvements}:

\begin{itemize}
\tightlist
\item
  All models are \textbf{not genuinely bi-directional}, as ULMFiT and GPT are one-directional, and ELMo is a concatenation of a right-to-left and left-to-right LSTM. Bidirectional models can even have more precise word representations, as the human language understanding is bidirectional.
\item
  ELMo uses character-based \textbf{model input}, and \textbf{ULMFit} uses word-based \textbf{model input}. \textbf{GPT} and following transformer-based models use \textbf{tokenized} words (subwords), which is take advantage of both other model inputs.
\item
  ULMFiT and ELMo are \textbf{based on LSTMs}, whereas the transformer-based model architecture of GPT has many advantages like parallelization and subsequent performance improvements.
\end{itemize}

In the next chapter, the main idea behind transformers, self-attention, is explained. More popular state-of-art models based on the idea like BERT are presented in chapter 10 {[}??{]}.

\hypertarget{attention-and-self-attention-for-nlp}{%
\chapter{Attention and Self-Attention for NLP}\label{attention-and-self-attention-for-nlp}}

\emph{Authors: Joshua Wagner}

\emph{Supervisor: Matthias Aßenmacher}

Both attention and self-attention were important for the advances made in NLP.
The first part is an overview of attention as it is a building block for self-attention.
The second part focuses on self-attention which enabled the commonly used models
for transfer learning that are seen today.

\hypertarget{attention}{%
\section{Attention}\label{attention}}

In this part of the chapter, we revisit the Encoder-Decoder architecture that was introduced
in chapter \href{01-02-rnns-and-their-applications-in-nlp}{3}. We focus on the improvements
that were made with the development of attention mechanisms on the example of neural machine translation (nmt).

As seen in chapter \href{01-02-rnns-and-their-applications-in-nlp}{3}, traditional early
encoder-decoder architecture passes the last hidden state of the encoder to the decoder.
This leads to the problem that information is lost in long input sequences.
Especially information found early in the sequence tends to be ``forgotten'' after
the entire sequence is processed. The addition of bi-directional layers remedies
this by processing the input in reversed order. The problem still persists for mid
sections of very long input sequences. The development of attention enables the decoder to
attend to the entire input sequence.

\hypertarget{bahdanau-attention}{%
\subsection{Bahdanau-Attention}\label{bahdanau-attention}}

In 2015, \citet{bahdanau2014neural} proposed attention to fix the information problem that
the before seen encoder-decoder architecture faced. Early decoders are trained to predict \(y_{t'}\)
given a context vector \(c\) and all earlier predicted words \(\{y_t, \dots, y_{t'-1}\}\).
\[c=q(\{h_1,\dots,h_T\})\] where \(h_1,\dots,h_T\) are the the hidden states of the encoder for the input sequence
\(x_1,\dots, x_T\) and \(q\) is a non-linear function. \citet{sutskever2014sequence} for example used
\(q(\{h_1,\dots,h_T\}) = h_T\) as their non-linear transformation which remains a popular
choice for architecture without attention.
Attention changes the context vector \(c\) that a decoder uses for translation from a fixed
length vector \(c\) of a sequence of hidden states \(h_1, \dots, h_T\) to a sequence
of context vectors \(c_i\). The hidden state \(h_i\) has a strong focus on the \emph{i}-th
word in the input sequence and its surroundings.
Each \(h_i\) is computed by a concatenation of the forward
\(\overrightarrow{h_i}\) and backward \(\overleftarrow{h_i}\) hidden states of the
bi-directional encoder.

\[
h_i = [\overrightarrow{h_i}; \overleftarrow{h_i}], i = 1,\dots,n
\]
The hidden states of the decoder \(s_t\) at time-point \(t\) is computed as \(s_t = f(s_{t-1},y_{t-1},c_t)\).
The context vector \(c_t\) is computed as a weighted sum of the hidden
states \(h_1,\dots, h_{T_x}\):

\[
c_t = \sum^{T_x}_{i=1}\alpha_{t,i}h_i.
\]
The weight \(\alpha_{t,i}\) of each hidden state \(h_i\) is also called the alignment score.
These alignment scores are computed as:

\[
\alpha_{t,i} = align(y_t, x_i) =\frac{exp(score(s_{t-1},h_i))}{\sum^{n}_{i'=1}exp(score(s_{t-1},h_{i'}))}
\]
with \(s_{t-1}\) being the hidden state of the decoder at time-step \(t-1\).
The alignment score \(\alpha_{t,i}\) models how well input \(x_i\) and output \(y_t\) match
and assigns the weight to \(h_i\). \citet{bahdanau2014neural} parametrize their alignment
score with a single-hidden-layer feed-forward neural network which is jointly
trained with the other parts of the architecture. The score function used by Bahdanau et
al.~is given as

\[
score(s_t,h_i) = v_\alpha^Ttanh(\mathbf{W}_\alpha[s_t;h_i])
\]
were tanh is used as a non-linear activation function and \(v_\alpha\) and \(W_\alpha\)
are the weight matrices to be learned by the alignment model. The alignment score function
is called ``concat'' in \citet{luong2015effective} and ``additive attention'' in \citet{vaswani2017attention}
because of the concatenation seen above. A nice by-product of attention is a matrix of alignment scores
which can be visualised to show the correlation between source and target words.

\begin{figure}
\centering
\includegraphics{./figures/02-02-attention-and-self-attention-for-nlp/bahdanau-fig3.png}
\caption{Alignment Matrix visualised for a French to English translation. Image source: Fig 3 in \citet{bahdanau2014neural}}
\end{figure}

The attention model proposed by Bahdanau et al.~is also called a soft/global attention model as it attends
to every input in the sequence.

\hypertarget{luong-attention}{%
\subsection{Luong-Attention}\label{luong-attention}}

\begin{itemize}
\item
  intro loung
\item
  different proposed score functions
\item
  differences to bahdanau
\item
  explanation for local/hard attention and differences to global/soft attention
\end{itemize}

\hypertarget{attention-models}{%
\subsection{Attention Models}\label{attention-models}}

Overview over models that use attention and different attentions used, segway to self-attention

\begin{itemize}
\tightlist
\item
  just a short overview over different attention models/score functions
\end{itemize}

\hypertarget{self-attention}{%
\section{Self-Attention}\label{self-attention}}

general intro to self-attention

\begin{itemize}
\tightlist
\item
  intro self attention, explanation is done in the transformers part
\end{itemize}

\hypertarget{transformers}{%
\subsection{Transformers}\label{transformers}}

explain transformer architecture, multi-head attention, dot-prod. attention, explain the differences in computational cost to rnns and conv. models

\hypertarget{transfer-learning-for-nlp-ii}{%
\chapter{Transfer Learning for NLP II}\label{transfer-learning-for-nlp-ii}}

\emph{Authors: Bailan He}

\emph{Supervisor: M. Aßenmacher}

Unsupervised representation learning has been highly successful in NLP. Typically, these methods first pre-train neural networks on large-scale unlabeled text corpora and then fine-tune the models on downstream tasks. Here we introduce the three remarkable models, BERT, GPT-2 and XLNet. \href{\%22https://github.com/huggingface/transformers\%22}{Transformers} is an excellent github repository, where readers can find their implementations.

\hypertarget{bidirectional-encoder-representations-from-transformers-bert}{%
\section{Bidirectional Encoder Representations from Transformers (BERT)}\label{bidirectional-encoder-representations-from-transformers-bert}}

\hypertarget{autoencoding}{%
\subsection{Autoencoding}\label{autoencoding}}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/02-03-transfer-learning-for-nlp/autoencoding} 

}

\caption{Autoencoding}\label{fig:ch02-03-figure01}
\end{figure}

Autoencoding(AE) have been most successful pre-training objectives and figure \ref{fig:ch02-03-figure01} shows the modeling of it. AE based model does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. Specifically, given a text sequence \(X = (x_1,...,x_T)\), AE factorizes the log-likelihood into a partial sum \[log p(\bar{X}|\hat{X}) = \sum^T_{t=1} mask_t p(x_t|\hat{X})\], where \(mask_t\) is an indicator to show if a token is masked,\citet{yang2019xlnet}.

The training objective is to reconstruct the masked tokens \(\bar{X}\) given the sequence \(\hat{X}\). AE tries to find the best model \(P\) to predict the masked tokens.

\hypertarget{introduction-of-bert}{%
\subsection{Introduction of BERT}\label{introduction-of-bert}}

BERT is published by researchers at Google AI in 2018. It is regarded as a milestone in the NLP community by proposing a bidirectional Language model based on Transformer.

BERT is the notable example of AE, 15\% of tokens are replaced by a special symbol {[}MASK{]}, and the model is trained to reconstruct the original sequence from the masked tokens. By contrast with previous efforts that looked at a text sequence either from left to right(RNN) or combined left-to-right and right-to-left training (ELMO), the Transformer Encoder utilizes bidirectional contexts simultaneously. Therefore BERT uses the Transformer Encoder as the structure of the pre-train model and addresses the unidirectional constraints by proposing a new pre-training objective: the ``masked language model''(MLM).

In conclusion, BERT uses masked language models to enable pre-trained deep bidirectional representations and after fine-tuning based the representation, BERT advances state-of-the-art performance for eleven NLP tasks.

\hypertarget{input-representation-of-bert}{%
\subsection{Input Representation of BERT}\label{input-representation-of-bert}}

The input representation is able to unambiguously represent both a single text sentence or a pair of text sentences in one token sequence. For a given token, its input representation is constructed by summing the corresponding token, segment and position embeddings, \citet{bert}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/02-03-transfer-learning-for-nlp/bert_input_representation} 

}

\caption{BERT input representation}\label{fig:ch02-03-figure02}
\end{figure}

Figure \ref{fig:ch02-03-figure02} is the visual representation of input representation. The specifics are:

\begin{itemize}
\item
  Use WordPiece embeddings \citet{wu2016google} with a 30,000 token vocabulary and split word pieces denoted with \#\#. e.g.,{[}playing = play and \#\#ing {]}
\item
  The first token of every sequence is always the special classification embedding {[}CLS{]}. For non-classification tasks, this vector is ignored.
\item
  Sentence pairs are packed together into a single sequence and are separated with a special token {[}SEP{]}. Second, different learned sentence embedding{[}e.g.,A and B{]} will be added to every token of each sentence.For single-sentence inputs we only use the sentence A embeddings.
\end{itemize}

\hypertarget{masked-language-model}{%
\subsection{Masked Language Model}\label{masked-language-model}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-03-transfer-learning-for-nlp/bert_masked_task} 

}

\caption{BERT Masked Language Model  
 Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-bert/}\label{fig:ch02-03-figure03}
\end{figure}

Figure \ref{fig:ch02-03-figure03} is the visual representation of Masked Language Model.
As shown in the above figure, when word embeddings are fed into BERT, 15\% of the words in each sequence will be replaced by a special token {[}MASK{]}. The task of MLM is to predict the original value of masked tokens. The output of MLM is the word embeddings of corresponding tokens, then feed the word embedding of {[}MASK{]} token into a simple softmax classifier and get the final prediction of {[}MASK{]} token. And the most important part is the ``yellow'' block, it's basically a multi-layer bidirectional Transformer encoder based on implementation described in \citet{kaiser2017one}.

\begin{itemize}
\tightlist
\item
  What is the Masked Language Model?

  \begin{itemize}
  \tightlist
  \item
    15\% of all WordPiece tokens in each sequence will be randomly masked.
  \item
    Input: token embedding(one sentence, begin with {[}CLS{]})
  \item
    Output: BERT token embedding.
  \item
    Using softmax(a simple linear classifier) to predict the masked token. (using token {[}CLS{]} to do classification task.)
    (words match each other may have the same BERT embedding)
  \end{itemize}
\item
  How to mask?

  \begin{itemize}
  \tightlist
  \item
    80\% of the time: Replace the word with the {[}MASK{]} token, e.g., my dog is hairy \(\Rightarrow\) my dog is {[}MASK{]}
  \item
    10\% of the time: Replace the word with a random word,e.g., my dog is hairy \(\Rightarrow\) my dog is apple.
  \item
    10\% of the time: Keep the word unchanged, e.g., my dog is hairy \(\Rightarrow\) my dog is hairy.
  \end{itemize}
\item
  Why there are two other methods to replace the word?

  \begin{itemize}
  \tightlist
  \item
    Why keep 10\% of masked tokens unchanged?\\
    A: In some downstream task like POSTagging, all the tokens are known, if BERT only trained the Masked sequences, then the model only use the information of context, exclude the information of the masked words. It will lose a part of the information, then weakens the performance of the model.
  \item
    Why replace 10\% of masked tokens with random words?\\
    A: Since we keep 10\% of masked token unchanged, if we do not add random noise, the model will be ``lazy'' in our training, the model will plagiarize current tokens, rather than learning.
  \end{itemize}
\end{itemize}

\hypertarget{next-sentence-tasks}{%
\subsection{Next-sentence Tasks}\label{next-sentence-tasks}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-03-transfer-learning-for-nlp/bert_nsp} 

}

\caption{BERT Next-sentence Tasks  
 Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-bert/}\label{fig:ch02-03-figure04}
\end{figure}

Figure \ref{fig:ch02-03-figure04} is the visual representation of Next-sentence Tasks(NST).
Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. So BERT proposes the NST by using the special token {[}CLS{]} as the first token of every sequence.

\begin{itemize}
\tightlist
\item
  What is Next-sentence Tasks?

  \begin{itemize}
  \tightlist
  \item
    Input: token embedding (two sentence, begin with {[}CLS{]}, each sentence ends with {[}SEP{]})
  \item
    Output: BERT token embedding.
  \item
    Using softmax (a simple linear classifier) to explain the relationship between two sentences.
  \item
    Using {[}CLS{]} token to pre-train a binary classification tasks.
  \item
    Sentences can be trivially generated from monolingual corpus.
  \item
    Choose sentences A and B for each example, 50\% of B is actual next sentence that follows A, and 50\% of B is a random sentence from the corpus.
  \end{itemize}
\item
  For example:

  \begin{itemize}
  \tightlist
  \item
    Input = {[}CLS{]} the man went to {[}MASK{]} store {[}SEP{]} he bought a gallon {[}MASK{]} milk{[}SEP{]}\\
    Label = IsNext.
  \item
    Input = {[}CLS{]} the man {[}MASK{]} to the store {[}SEP{]} penguin {[}MASK{]} are flight \#\#less birds {[}SEP{]}.\\
    Label = NotNext
  \end{itemize}
\end{itemize}

\hypertarget{pre-training-procedure-of-bert}{%
\subsection{Pre-training Procedure of BERT}\label{pre-training-procedure-of-bert}}

For the pre-training corpus, BERT uses the concatenation of BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words) to create two version of BERT (L the number of layers, H the hidden size, A the number of self-attention heads):

\begin{itemize}
\tightlist
\item
  BERT-Base: L = 12, H = 768, A = 12, Total parameters = 110M
\item
  BERT-Large: L = 24, H = 1024, A = 16, Total parameters = 340M
\end{itemize}

\hypertarget{fine-tuning-procedure-of-bert}{%
\subsection{Fine-tuning Procedure of BERT}\label{fine-tuning-procedure-of-bert}}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{figures/02-03-transfer-learning-for-nlp/bert_based_model} 

}

\caption{BERT Task Specific Models}\label{fig:ch02-03-figure05}
\end{figure}

As shown in Figure \ref{fig:ch02-03-figure05}, different types of tasks require different modifications to the model, and the modification of the model before fine-tuning is quite simple. For example, for the sequence-level classification problem (such as sentiment analysis, Task (a) in the figure), take the output representation of the first token{[}CLS{]} and feed it to a softmax to get the classification result. As for token-level classification (e.g.~NER, Task (d) in the figure), take the output of the last layer transformer of all tokens and feed it to the softmax layer for classification.

\hypertarget{feature-extraction}{%
\subsection{Feature Extraction}\label{feature-extraction}}

Like other Language model, the pre-trained BERT can create contextualized word embeddings. Then the word embeddings can be used as features in other models. Readers can try out BERT through \href{https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb}{BERT FineTuning with Cloud TPUs}.

\hypertarget{bert-like-models}{%
\subsection{BERT-like models}\label{bert-like-models}}

The state-of-the-art performance of BERT reveals the deep bidirectional language model can significantly improve the model performance in NLP tasks, and BERT chart a new course that how a real bidirectional model should be. However, BERT has also the following weaknesses: first of all, the input to BERT contains artificial symbols like {[}MASK{]} that never occur in downstream tasks, which creates a pre-train-fine-tuning discrepancy problem. Secondly, BERT assumes the predicted tokens are independent of others given the unmasked tokens, which is oversimplified for natural language. Several models are inspired by BERT to solve these problems.

Roberta \citet{liu2019roberta} shows hyperparameter choices have a significant impact on the final results. It improves BERT pre-training in the following aspects to get better performance:

\begin{itemize}
\tightlist
\item
  Changing the input embedding to Byte Pair Encoding (BPE) \citet{sennrich2015neural}.
\item
  Using dynamic masking: each train has different training data.
\item
  Using full sentence without NSP.
\item
  More Data, larger batch size (8k), and longer training (100k to 300k steps).
\end{itemize}

ALBERT \citet{lan2019albert} mainly makes three improvements to BERT, which reduces the overall parameter amount, accelerates the training speed, and improves the model performance under the same training time.

\begin{itemize}
\tightlist
\item
  Using factorized embedding parameterization.
\item
  Cross-layer parameter sharing, which significantly reduces the number of parameters.
\item
  Replacing NSP with Sentence-order prediction loss (SOP).
\end{itemize}

There are also BERT-like models pre-trained on domain-specific corpora, SciBERT \citet{beltagy2019scibert} on scientific publications ,ERNIE \citet{zhang2019ernie} on a large corpus incorporating knowledge graph in the input. In comparison to fine-tune original BERT, training on the domain-specific corpora then fine-tuning them on downstream NLP tasks has shown to yield better performance.

\hypertarget{generative-pre-traininggpt-2}{%
\section{Generative Pre-Training(GPT-2)}\label{generative-pre-traininggpt-2}}

\hypertarget{auto-regressive-language-modelar}{%
\subsection{Auto-regressive Language Model(AR)}\label{auto-regressive-language-modelar}}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/02-03-transfer-learning-for-nlp/autoregressive} 

}

\caption{Autoregressive}\label{fig:ch02-03-figure06}
\end{figure}

Figure \ref{fig:ch02-03-figure06} shows the modelling of Auto-regressive language model, it tries to estimate the probability distribution of a sequence with a auto-regressive pattern. Specifically, given a text sequence \(X = (x_1,...,x_T)\), AR language model factorizes the log-likelihood into a forward sum \(logp(x) = \sum^T_{t=1} p(x_t|x<t)\) or a backward one \(logp(x) = \sum^{t=1}_{T} p(x_t|x>t)\) \citet{yang2019xlnet}. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information.

\hypertarget{introduction-of-gpt-2}{%
\subsection{Introduction of GPT-2}\label{introduction-of-gpt-2}}

GPT-2 is proposed by researchers at OpenAI in 2019. It captures the attention of the NLP community for the following characters:\\
First of all, instead of the fine-tuning model with specific tasks, GPT-2 demonstrates language models can perform down-stream tasks in a zero-shot setting, which means without any parameter or architecture modification. Secondly, in order to perform better under the zero-shot setting, GPT-2 becomes extremely large. On the other hand, training GPT-2 also needs enormous data, so researchers also create a new dataset ``WebText'', which contains millions of webpages.\\
With the characters above, GPT-2 achieves state-of-the-art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText.

Readers can experiment with GPT-2 by using \href{https://demo.allennlp.org/next-token-lm?text=AllenNLP\%20is}{AllenAI GPT-2}. You can input a sentence and see the prediction of next words.

\hypertarget{input-representation-of-gpt-2}{%
\subsection{Input Representation of GPT-2}\label{input-representation-of-gpt-2}}

GPT-2 uses a human-curated dataset called \href{https://skylion007.github.io/OpenWebTextCorpus/}{``WebText''}, that contains text scraped from 45 million web-links. All results presented in paper use a preliminary version of WebText which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40GB of text \citet{radford2019gpt2}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-03-transfer-learning-for-nlp/gpt_input_representation} 

}

\caption{GPT-2 Input Representation  
 Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-gpt2/}\label{fig:ch02-03-figure07}
\end{figure}

Figure \ref{fig:ch02-03-figure07} shows the input representation of GPT-2. The input sequence has a start token {[}S{]}, and input embedding of each token is the corresponding specially designed Byte Pair Encoding (BPE) \citet{sennrich2015neural}, adding up the positional encoding vector.

\hypertarget{the-decoder-only-block}{%
\subsection{The Decoder-Only Block}\label{the-decoder-only-block}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-03-transfer-learning-for-nlp/gpt_decoder} 

}

\caption{GPT-2 Model  
 Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-gpt2/}\label{fig:ch02-03-figure08}
\end{figure}

Figure \ref{fig:ch02-03-figure08} shows the model of GPT-2. This model essentially is the Transformer decoder, except they threw away the second self-attention layer. In each decoder, Layer normalization \citet{ba2016layer} was moved to the input of each sub-block, similar to a pre-activation residual network \citet{he2016identity} and an additional layer normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used \citet{radford2019gpt2}.

GPT-2 reads from the start token {[}s{]} till the last predicted token to predict the next token. For example, in the first round, model uses {[}s{]} to predict {[}robot{]}, in the next round the input is updated as \{{[}s{]},{[}robot{]}\} since {[}robot{]} has been predicted. This is how Masked self-Attention in Decoder block works.

Otherwise, since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed.
The model of GPT-2 should be :
\[log p(x)=log\sum^{n}_{i=1}p(s_n|s_1,...,s_{n-1};task_i)\]

For example, a translation training example can be written as the sequence (translate to french, English text, French text). Likewise, a reading comprehension training example can be written as (answer the question, document, question, answer).

By using the specially designed WebText, GPT-2 can be used by following patterns for different tasks.

\begin{itemize}
\item
  Reading Comprehension: data sequence, ``Q:'', question sequence, ``A:''
\item
  Summarization: data sequence, ``TL;DR:''
\item
  Translation: English sentence 1 = ``French sentence 1, English sentence 2 = French sentence 2, English sentence 3 =''
\end{itemize}

\hypertarget{gpt-2-models}{%
\subsection{GPT-2 Models}\label{gpt-2-models}}

\begin{table}

\caption{\label{tab:ch02-03-table01}GPT-2 models size}
\centering
\begin{tabular}[t]{l|r|r}
\hline
Parameters & Layers & Dimensionality\\
\hline
117M & 12 & 768\\
\hline
345M & 24 & 1024\\
\hline
762M & 36 & 1280\\
\hline
1542M & 48 & 1600\\
\hline
\end{tabular}
\end{table}

Four versions models are trained, the architectures are summarized in \ref{tab:ch02-03-table01}. The smallest model is equivalent to the original GPT, and the second smallest equivalent to the largest model from BERT \citet{bert}. The largest model is called GPT-2, which has 1.5 billion parameters.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

The framework of GPT-2 is the combination of pre-training based on Transformer Decoder and fine-tuning based on unsupervised downstream tasks.

After the great success of bidirectional models like BERT, GPT-2 insists on using unidirectional models and still achieves state-of-the-art performance. It proves that the performance of language models can be significantly improved by simply increasing the size of training datasets, which is exactly what GPT-2 did and even GPT-2, which has 1.5 billion parameters, still underfits WebText. This result suggests that datasets are as important as models.

\hypertarget{xlnet}{%
\section{XLNet}\label{xlnet}}

\hypertarget{introduction-of-xlnet}{%
\subsection{Introduction of XLNet}\label{introduction-of-xlnet}}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/02-03-transfer-learning-for-nlp/xlnet_twomodels} 

}

\caption{AR Language Modeling  and AE}\label{fig:ch02-03-figure09}
\end{figure}

XLNet is proposed by researchers at Google in 2019. Since the autoregressive language model (e.g.GPT-2) is only trained to encode a unidirectional context and not effective at modeling deep bidirectional contexts and autoencoding (e.g.BERT) suffers from the pretrain-finetune discrepancy, XLNet borrows ideas from the two types of objectives while avoiding their limitations. It is a new objective called Permutation Language Modeling. By using a permutation operation during training time, bidirectional context information can be captured and makes it a generalized order-aware autoregressive language model. Besides, XLNet introduces a two-stream self-attention to solve the problem that standard parameterization will reduce the model to bag-of-words.

Two XLNet are released, i.e.~XLNet-Base and XLNet-Large, and include the similar settings of corresponding BERT. Empirically, XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.

\hypertarget{permutation-language-modelingplm}{%
\subsection{Permutation Language Modeling(PLM)}\label{permutation-language-modelingplm}}

Now Figure \ref{fig:ch02-03-figure010} illustrates the permutation language modeling objective.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/02-03-transfer-learning-for-nlp/xlnet_pml} 

}

\caption{Illustration of the permutation language modeling objective for predicting x3 given the same input sequence x but with different factorization orders.}\label{fig:ch02-03-figure010}
\end{figure}

Specifically, for a sequence \(X\) of length \(T\), there are \(T!\) different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides. Let \(P_T\) be the set of all possible permutations of a sequence {[}1,2,\ldots{}, T{]} and use \(z_t\) and \(z_{<t}\) to denote the t-th element and the first t−1 elements of a permutation \(p\in P_T\). Then the permutation language modeling objective can be expressed as follows:

\begin{equation}
\max_{\theta} \mathbb{E}_{p\sim P_T} \left[\sum_{t=1}^Tlogp_{\theta}(x_{z_t|x_{z_{<t}}})\right]
\end{equation}

For instance, when we have a factorization order: \{ New York is a city \}. The probability of sequence can be expressed as follows:

\begin{equation}
\begin{aligned}
P(New, York, is, a, city) = P(New) * P(York | New) * P(is | New, York) * \\  P(a | New, York, is) * P(city | New, York, is, a)
\end{aligned}
\end{equation}

As for another factorization order: \{city a is New York\}, then the probability of sequence can be expressed differently:

\begin{equation}
\begin{aligned}
P(New, York, is, a, city) = P(city) * P(a | city) * P(is | city, a) * \\ P(New | city, a, is) * P(York | city, a, is, New)
\end{aligned}
\end{equation}

It is noteworthy that sequence order is not shuffled and only attention masks are changed to reflect factorization order. With PLM, XLNet can model bidirectional context and the dependency within each token of the sequence.

\hypertarget{the-problem-of-standard-parameterization}{%
\subsection{The problem of Standard Parameterization}\label{the-problem-of-standard-parameterization}}

The Standard Parameterization can be expressed as follows:

\[p_{\theta}(X_{p_t}=x|x_{p<t})=\frac{e(x)^Th_{\theta}(x_{p<t})}{\sum_{x^{'}}e(x^{'})^Th_{\theta}(x_{p<t})}\]
where \(e(x)\) denotes the embedding of input token and \(h_{\theta}(x_{p<t})\) denotes the hidden representation of \(x_{p<t}\).

While the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. Specifically, let's consider two different permutations \(p^1\text{:{New York is a city}}\) and \(p^2:\text{{New York a city is}}\). The probability of \(\text{{is}}\) in \(p^1\): \(\text{P(is|New, York)}\) and the probability of \(\text{{a}}\) in \(p^2\): \(\text{P(a|New, York)}\) are identical. The model will be reduced to predicting a bag-of-words, because \(h_{\theta}(x_{z<t})\) does not contain the position of the target.

XLNet resolves the problem by reparameterizing with positions:

\[p_{\theta}(X_{p_t}=x|x_{p<t})=\frac{e(x)^Tg_{\theta}(x_{p<t},p_t)}{\sum_{x^{'}}e(x^{'})^Tg_{\theta}(x_{p<t},p_t)}\]
where \(e(x)\) denotes the embedding of input token and \(g_{\theta}(x_{p<t},p_t)\) denotes the hidden representation of \(x_{p<t}\) and position \(p_t\).
But reparameterization with positions brings another contradiction \citet{yang2019xlnet}:\\
(1) To predict the token \(x_{p_t}\) , \(g_{\theta}(x_{p<t},p_t)\) should only use the position \(p_t\) and not the content \(x_{p_t}\), otherwise the objective becomes trivial.\\
(2) To predict the other tokens \(x_{p_j}\) with j \textgreater{} t, \(g_{\theta}(x_{p<t},p_t)\) should also encode the content \(x_{p_t}\) to provide full contextual information.

XLNet proposes the Two-Stream Self-Attention to resolve the contradiction.

\hypertarget{two-stream-self-attention}{%
\subsection{Two-Stream Self-Attention}\label{two-stream-self-attention}}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/02-03-transfer-learning-for-nlp/xlnet_ts} 

}

\caption{Two-Stream Self-Attention}\label{fig:ch02-03-figure011}
\end{figure}

Instead of one, two sets of hidden representation are proposed:

\begin{itemize}
\tightlist
\item
  The content representation \(h_{\theta}(x_{p \leq t})\), this representation encodes both the context and \(x_{p_t}\) itself.
\item
  The query representation \(g_{\theta}(x_{p<t},p_t)\), which only has information \(x_{p<t}\) and the position \(p_t\) but not the content \(x_{p_t}\).
\end{itemize}

Figure \ref{fig:ch02-03-figure011} is an example with the Factorization order: \(3,2,4,1\):

\begin{itemize}
\item
  \(h_i^{(t)}\) denotes the content representation of the i-th token in the t-th layer of self-attention. It is the same as the standard self-attention. For instance, \(h_1^{(1)}\) can see all the \(h_i^{(0)}\) since the 1-st token is after token \({3,2,4}\).
\item
  \(g_i^{(t)}\) denote the query representation of the i-th token on the t-th layer of self-attention. It does not have access information about the content \(x_{p_t}\), other trivial.
\item
  Computationally, \(h_i^{(0)}\) is the word embeddings, and \(g_i^{(0)}\) is a trainable parameter initialized with a trainable vector. Only \(h_i^{(t)}\) is used during fine-tuning. The last \(g_i^{(t)}\) is used for optimizing the LM loss. A self-attention layer \(m=1,...,M\) are schematically updated with a shared set of parameters as follows:
\end{itemize}

\[
h_{p_t}^{m} \leftarrow Attention (Q=h_{p_t}^{m-1}, KV=h^{(m-1)}_{p \leq t};\theta)
\text{  (content stream: use both $p_t$ and $x_{p_t}$)}
\\
g_{p_t}^{m}\leftarrow Attention (Q=g_{p_t}^{(m-1)}, KV=h^{(m-1)}_{p<t};\theta)
\text{  (query stream: use $p_t$ but cannot see }x_{p_t})
\]

where Q, K, V denote the query, key, and value in an attention operation \citet{vaswani2017attention}.
More details are included in Appendix A.2 for reference \citet{yang2019xlnet}.

\hypertarget{partial-prediction}{%
\subsection{Partial Prediction}\label{partial-prediction}}

While the PLM has several benefits, optimization is challenging due to the permutation operator. To reduce the optimization difficulty, XLNet only predicts the last tokens in a factorization order. It sets a cutting point \(c\) and split the permutation \(p\) into a non-target subsequence \(p_{\leq c}\) and a target subsequence \(p_{>c}\). The objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e.,

\begin{equation}
\max_{\theta}   \mathbb{E}_{p\sim P_T} \left[logp_{\theta}(x_{z_{>c}|x_{z_{\leq c}}})\right] =
\mathbb{E}_{p\sim P_T} \left[\sum_{t=c+1}^{|z|} logp_{\theta}(x_{z_t|x_{z_{\leq t}}})\right]
\end{equation}
For unselected tokens, their query representations need not be computed, which saves speed and memory. XLNet incorporates ideas from Transformer-XL and inherits two important characters of it, Segment-Level Recurrence and Relative Position Encoding, to enable the learning of long-term dependency and resolve the context fragmentation.

\hypertarget{xlnet-pre-training-model}{%
\subsection{XLNet Pre-training Model}\label{xlnet-pre-training-model}}

After tokenization with SentencePiece \citet{kudo2018sentencepiece}, Researchers obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total, to pre-train the XLNet.

Analogous to BERT, two versions of XLNet have been trained:

\begin{itemize}
\tightlist
\item
  XLNet-Base: L = 12, H = 768, A = 12, Total parameters = 110M (on BooksCorpus and Wikipedia only)
\item
  XLNet-Large: L = 24, H = 1024, A = 16, Total parameters = 340M (on total datasets)
\end{itemize}

\hypertarget{conclusion-1}{%
\subsection{Conclusion}\label{conclusion-1}}

Language modeling has been a rapidly developing research area. However, most language modelings are unidirectional and BERT \citet{bert} shows that bidirectional modeling can significantly improve model performance. Unidrectional modelings without specific structure are hard to capture the bidirectional context. Now with the permutation operator, the unidirectional language modelings can become bidirectional modeling. XLNet has built a bridge between language modeling and bidrectional model. Overall, XLNet is a generalized AR pre-training method that uses a permutation language modeling objective to combine the advantages of AR and AE methods.

\hypertarget{latest-nlp-models}{%
\section{Latest NLP models}\label{latest-nlp-models}}

Nowadays NLP has become a competition between big companies. When BERT first came, people talked about it may cost thousands of dollars to train it. Then came GPT-2, which has 1.5 billion parameters and is trained on 40GB data. As I mentioned above, GPT-2 of Open-AI shows that increasing the size of models and datasets is at least as important as proposing a new model architecture.

After GPT-2, researchers at Google did the same thing, they proposed a general language model called T5 \citet{raffel2019exploring}, which is trained on 750GB corpus - \href{https://www.tensorflow.org/datasets/catalog/c4}{``C4 (Colossal Clean Crawled Corpus)''}. If you read the paper, the last page of it is a table of several experience results, which may cost millions of dollars to reproduce it.

On 28th May 2020, the ``Arms race'' goes into another level, GPT-3 \citet{brown2020language} emerged, the new paper takes GPT to the next level by making it even bigger - GPT-3 has 175 billion parameters and is trained on a dataset that has 450 billion of tokens \href{https://github.com/openai/gpt-3}{``GPT-3 Dataset''}. GPT-3 experiments with the three different settings: zero-shot, one-shot, and Few-shot to show that scaling up language models can greatly improve performance, sometimes even reaching competitiveness with prior SOTA approaches. However, it is conservatively estimated that training GPT-3 will cost one hundred million dollars.

Models like T5 and GPT-3 are very impressive, but the biggest problem at the moment is to find a way to make the current model put into use in industry. If it can't bring benefits, the AI industry can't be sustained by burning money. As for researchers, the truth is, with the resources it is also possible to fail, but it is certainly impossible to succeed without resources now.

\hypertarget{introduction-resources-for-nlp}{%
\chapter{Introduction: Resources for NLP}\label{introduction-resources-for-nlp}}

\emph{Authors: Nico Hahn}

\emph{Supervisor: Daniel Schalk}

As natural language processing has become one of the hottest topics in data science/statistics in recent years, tons of new resources have been created. As a result, NLP has become accessible to a wide range of people, making it easier to train and compare models. Competitions have emerged to determine who can develop the best model for specific tasks such as question answering.

In the following chapter, we will take a look at some of these benchmark datasets to see what tasks they are used for and how performance is assessed on these datasets.

In addition to that we will be talking about pre-training resources, what they are and why they are useful.

Lastly, we will be introducing the huggingface transformers module.

\hypertarget{resources-and-benchmarks-for-nlp}{%
\chapter{Resources and Benchmarks for NLP}\label{resources-and-benchmarks-for-nlp}}

\emph{Authors: Nico Hahn}

\emph{Supervisor: Daniel Schalk}

Frameworks such as TensorFlow or Keras allow users to train a wide range of different models for different tasks. Let us assume that two models for a simple question-answer system are trained, one with attention and one without attention. How can these models be evaluated in order to find the model better suited to the task? Quite simply, through benchmarking. This section looks at some of the most commonly used benchmarking datasets and at pre-training resources.

\hypertarget{metrics}{%
\section{Metrics}\label{metrics}}

For many of the benchmarking datasets in natural language processing, a leaderboard exists in which different models are compared with each other. Depending on the task, the models are evaluated with different metrics. In this section we will introduce those used for the benchmarking datasets presented later.

\textbf{Exact match (EM):} The percentage of predictions that match any one of the answers exactly.

\textbf{(Macro-averaged) F1 score (F1):} Each answer and prediction is tokenized into words. For every answer to a given question, the overlap between the prediction and each answer is calculated and the maximum F1 is chosen. This score is then averaged over all the questions. Formally speaking:

\[
\begin{aligned}
F1 &= \frac{2 \cdot \hbox{precision}\cdot\hbox{recall}}{\hbox{precision}+\hbox{recall}} \\
\hbox{precision} &= \frac{\hbox{number of same tokens}}{\hbox{length(predicted tokens)}} \\
\hbox{recall} &= \frac{\hbox{number of same tokens}}{\hbox{length(labeled tokens)}} 
\end{aligned}
\]

\textbf{Perplexity:} Perplexity is a measurement of how well a probability model predicts a sample. A low perplexity indicates the probability distribution is good at predicting the sample. In NLP, perplexity is a way of evaluating language models. A model of an unknown probability distribution \(p\), may be proposed based on a training sample that was drawn from \(p\). Given a proposed probability model \(q\), one may evaluate \(q\) by asking how well it predicts a separate test sample \(x_1, x_2, ..., x_N\) also drawn from \(p\). The perplexity of the model \(q\) is defined as
\[ b^{-\frac{1}{N}\sum_{i=1}^N\log_bq(x_i)} \]
where \(b\) is customarily \(2\). \citep{martinc2019supervised}

\textbf{BLEU:} BLEU (\textbf{B}i\textbf{l}ingual \textbf{E}valuation \textbf{U}nderstudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Scores are calculated for individual translated segments---generally sentences---by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Intelligibility or grammatical correctness are not taken into account. \citep{papineni2002bleu}

\textbf{Accuracy:} Accuracy is the ratio of number of correct predictions to the total number of input samples.

\[\hbox{Accuracy}=\frac{\hbox{TP}+\hbox{TN}}{\hbox{TP}+\hbox{TN}+\hbox{FP}+\hbox{FN}}\]

\textbf{Matthews correlation coefficient}: The MCC is used as a measure of quality of binary classifications. It takes true and false positives and negatives into account and is regarded as a balanced measure which can be used even if the classes are imbalanced. The MCC can be calculated directly from the confusion matrix using the following formula:

\[\hbox{MCC}=\frac{\hbox{TP}\cdot\hbox{TN}-\hbox{FP}\cdot\hbox{FN}}{\sqrt{(\hbox{TP}+\hbox{FP})(\hbox{TP}+\hbox{FN})(\hbox{TN}+\hbox{FP})(\hbox{TN}+\hbox{FN})}} \]
\citep{boughorbel2017optimal}

\hypertarget{benchmark-datasets}{%
\section{Benchmark Datasets}\label{benchmark-datasets}}

\hypertarget{squad}{%
\subsection{SQuAD}\label{squad}}

The first Version of the \textbf{S}tanford \textbf{Qu}estion \textbf{A}nswering \textbf{D}ataset was released in 2016. The dataset was created with the aim of advancing the field of reading comprehension. Reading text and answering questions about it is a demanding task for machines and requires large data sets of high quality. Most of the datasets before the release of the first version of SQuAD were either of high quality or of large size, but not both.

With the help of crowdworkers, 107.785 question-answer pairs were created for 536 Wikipedia articles. For each question, the answer is a segment of text, or span, from the corresponding reading passage.
Pairs were collected in a two-step process. In the first step the crowdworkers were asked to generate five questions and their answers per paragraph.

In the second step, each crowdworker was shown only the questions along with the paragraphs of the corresponding article and was asked to choose the shortest span in the paragraph that answered the question. As a result of this process, questions in the dev-set multiple answers.

The goal of this procedure was to get a more robust evaluation and to obtain an indicator of human performance on SQuAD.

One shortcoming of reading comprehension systems is that they tend to make unreliable guesses on questions to which no correct answer is possible. With this in mind, the second version of SQuAD was released in 2018. In addition to the approximately 100.000 questions from the first version, 53.775 new, unanswerable questions on the same paragraphs are contained in this dataset.

The accuracy of models trained on SQuAD is evaluated using two different metrics, exact match and (Macro-averaged) F1 score, both ignoring punctuation and articles.

To evaluate human performance, the second answer to each question is treated as the human prediction. \citep{rajpurkar2016squad, rajpurkar2018know}

Humans achieve an \textbf{EM} score of 86.831 and a \textbf{F1} score of 89.452.

Currently, the best performing model achieves an \textbf{EM} score of 90.386 and a \textbf{F1} score of 92.777.

Examples of SQuAD and the leaderboard and can be viewed here:

\center \url{https://rajpurkar.github.io/SQuAD-explorer/}

\flushleft

\hypertarget{coqa}{%
\subsection{CoQA}\label{coqa}}

CoQA is a dataset for building \textbf{Co}nversational \textbf{Q}uestion \textbf{A}nswering systems. Humans are capable of gathering information through conversations that include several interrelated questions and answers. The aim of CoQA is to enable machines to answers conversational questions.

The data set is made up of 127k Q/A pairs, covering seven different domains such as Children's Stories or Reddit. Five of these domains are used for in-domain evaluation, meaning models have already seen questions from these domains, and two are used for out-of-domain evaluation., meaning models have not seen any questions from these domains. To create the Q/A pairs, two people received a text passage, with one person asking the other person questions about the text and the other person answering. Using multiple annotators has a few advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A natural flow of conversation is created.
\item
  If one person gives an incorrect answer or a vague questions is asked, the other person can raise a flag. Thus bad annotators can easily be identified.
\item
  If there is a disagreement, the two annotators can discuss it via a chat window.
\end{enumerate}

Similar to SQuAD, three additional answers are collected for each question. However, since the answers influence the flow of the conversation, the next question always depends on the answer to the previous question. For this reason, two different answers to the same question can lead to two different follow-up questions. In order to avoid incoherent discussions, annotators are shown a question that they must answer first. After answering, they are shown the original answer, and they must then confirm that their answer has an identical meaning.

Compared to SQuAD 2.0, there is a greater variety of question types in CoQA. While almost half of the questions in the SQuAD start with \emph{what}, less than a quarter of the questions in the CoQA begin with this token. Another major difference is that questions in CoQA are on average 5.5 words long, compared to an average length of 10.1 in SQuAD. It is also worth mentioning that about 10\% of the answers in CoQA are either yes or no, whereas there are no such answers in SQuAD.

Like SQuAD, trained models are evaluated using a macro-average F1 score. Models are evaluated separately on the in-domain dataset and the out-of-domain dataset. \citep{coqa2019}

Humans achieve a \textbf{F1} score of 89.4 for in-domain and a \textbf{F1} score of 87.4 for out-of-domain.

Currently, the best performing model achieves a \textbf{F1} score of 91.4 for in-domain and a \textbf{F1} score of 89.2 for out-of-domain.

Examples of CoQA and the leaderboard and can be viewed here:

\center \url{https://stanfordnlp.github.io/coqa/}

\flushleft

\hypertarget{superglue}{%
\subsection{(Super)GLUE}\label{superglue}}

Most models in NLP are designed to solve a specific task, such as answering questions from a particular domain. This limits the use of models for understanding natural language. In order to process language in a way that is not limited to a specific task, genre, or dataset, models should be able to solve a variety of tasks well.

The \textbf{G}eneral \textbf{L}anguage \textbf{U}nderstanding \textbf{E}valuation benchmark dataset is a collection of tools created with this in mind. It is designed to encourage and favor models that share common linguistic knowledge across tasks. These tasks include textual entailment, sentiment analysis and question answering. Some tasks come with a lot of training data, others with less. Common to all datasets is that they were not created specifically for GLUE, but are existing datasets. Models that are evaluated on GLUE only need to have the ability to process single-sentence and sentence-pair inputs and make appropriate predictions.
This test suite contains a total of nine sentence or sentence-pair NLU tasks, built on established annotated datasets. There are three distinct types of tasks in GLUE: Single-Sentence Tasks, Similarity and Paraphrase Tasks and Inference Tasks.

\textbf{Single-Sentence Tasks}:

The first single-sentence task is CoLA, the \textbf{Co}rpus of \textbf{L}inguistic \textbf{A}cceptability, which consists of English acceptability judgments derived from books and journal articles on linguistic theory. Each datapoint consists of a sequence of words and an annotation as to whether this sequence is a grammatical English sentence. Matthews correlation coefficient is used as the evaluation metric.

The \textbf{S}tanford \textbf{S}entiment \textbf{T}reebank task consists of sentences from movie reviews and the corresponding sentiment (positive/negative). Accuracy is used for evaluation.

\textbf{Similarity and Paraphrase Tasks}

The \textbf{M}icrosoft \textbf{R}esearch \textbf{P}araphrase \textbf{C}orpus consists of pairs of sentences and the goal is to predict whether two sentences are semantically equivalent. For evaluation, F1 score and accuracy is used.

\textbf{Q}uora \textbf{Q}uestion \textbf{P}airs is similar to MRP in that the aim is to predict whether two questions are semantically equivalent and F1 and accuracy is used for evaluation.

The \textbf{S}emantic \textbf{T}extual \textbf{S}imilarity \textbf{B}enchmark consists of sentence pairs human-annotated with a similarity score from 1 to 5. The goal is to predict these scores. Pearson and Spearman correlation coefficients are used for evaluation.

\textbf{Inference Tasks}:

The \textbf{M}ulti-Genre \textbf{N}atural \textbf{L}anguage \textbf{I}nference Corpus is a collection of sentence pairs with textual entailment annotations. Based on a premise sentence and a hypothesis sentence, the aim is to predict whether the premise entails the hypothesis, contradicts the hypothesis, or neither of the two. Models are evaluated using accuracy.

\textbf{R}ecognizing \textbf{T}extual \textbf{E}ntailment is akin to MNLI, only this time with a two-class split.

The Winograd Schema Challenge is a reading comprehension task in which a system must read a sentence containing a pronoun and pick the speaker of that pronoun from a list of choices. To transform this task into a classification problem, pairs of sentences are constructed by replacing the ambiguous pronoun with any possible referent. The task is to predict if the sentence with the pronoun substituted is entailed by the original sentence. Evaluation is done using accuracy.

The last task in GLUE is based on the first version of SQuAD. The task is converted into a sentence pair classification task by pairing each question and each sentence in the respective context, with the aim of determining whether or not a sentence contains the answer to the question. The task is evaluated on accuracy.

The models are scored separately for each task and then a macro-average of these scores is calculated to determine a system's position on the ranking. If a task has multiple metrics, an unweighted average of these metrics is used as the score for the task when calculating the overall macro average. \citep{wang2018glue}

The human baseline score is 87.1, while the best model score is currently 90.6.

Roughly one year after the release of GLUE, models surpassed human performance. In response to this, a new benchmark, SuperGLUE, was introduced. It follows the same principles as GLUE, however the tasks included are more challenging. The two hardest tasks in GLUE, Recognizing Textual Entailment and the Winograd Schema Challenge, remain, the rest were selected based on difficulty for current NLP approaches. There are a total of eight different tasks in this benchmark.

\textbf{Bool}ean \textbf{Q}uestions consists of a text passage together with a corresponding yes/no question. Models are evaluated using accuracy.

\textbf{C}ommitment \textbf{B}ank is a three-class textual entailment task. Accuracy and F1 is used for evaluation, where for multi-class F1 the unweighted average of the F1 per class is calculated.

\textbf{C}hoice \textbf{o}f \textbf{P}lausible \textbf{A}nswers is a causal reasoning task in which a model is given a premise sentence, a question and two possible answers. It must then decide which answer is the correct one. Accuracy is used for the evaluation.

\textbf{Multi}-Sentence \textbf{R}eading \textbf{C}omprehension is a QA task where each example consists of a paragraph, a question and a list of answers. Models must predict which answers are correct. Evaluation metrics are F1 over all answer choices and the exact match of the answer set of each question.

\textbf{Re}ading \textbf{Co}mprehension with \textbf{Co}mmonsense \textbf{R}easoning \textbf{D}ataset is a multiple choice QA task. Each data point consists of a paragraph, a fill-in-the-gap sentence in which an entity is masked, and a list of possible entities to choose from. The entities can be expressed using several different surface forms, all of which are considered correct. Models are evaluated using max (over all mentions) token-level F1 and exact match.

\textbf{W}ords \textbf{i}n \textbf{C}ontext is a word sense disambiguation task in which a model is given two sentences and a polysemous word. Models must decide whether the word is used with the same meaning in both sentences. Accuracy is used for evaluation.

\citep{wang2019superglue}

For SuperGLUE, the human baseline score is 89.8, which is above the best model score, presently 89.3.

More information about the tasks and the leaderboard for both GLUE and SuperGLUE is available here:

\center \url{https://super.gluebenchmark.com/}

\flushleft

\hypertarget{aqua-rat}{%
\subsection{AQuA-Rat}\label{aqua-rat}}

One task that most people know from their time at school is solving algebraic word problems. For humans, this task can be easy, depending on a person's mathematical abilities, since we only have to perform a series of arithmetic operations. However, since programs can be endlessly complicated, it is a considerable challenge to induce them directly from question-answer pairs. The \textbf{A}lgebra \textbf{Qu}estion \textbf{A}nswering with \textbf{Rat}ionales dataset attempts to make this task more feasible for machines by providing not only the correct answer but also step-by-step instructions for deriving the correct answer, the so-called rationale. Models trained on AQuA-Rat must not only predict the correct answer, but also the rationale.

The dataset contains over 100.000 questions, and each question has five different options as to what the correct answer is. It also contains the answer rationale and the correct option. The problems cover a wide range of topics, for instance probability theory or calculus, with a variety of difficulty levels. To create the dataset, examples of exams such as the GMAT (Graduate Management Admission Test) and GRE (General Test) were taken from the Internet. This part of the dataset is called the seed dataset.
Besides, crowdsourcing was used to generate further questions. For this users were presented with five questions from the seed dataset and asked to select one of these questions and write a similar question. Users were also forced to rephrase the rationales and answers to avoid paraphrasing the original questions.
These created questions were then passed to another user for quality control.

The rationales are evaluated using average sentence level perplexity and the BLEU score. If a model is unable to generate a token for perplexity computation, an unknown token is predicted. The correctness of the answers is evaluated by calculating the accuracy of the predictions.

This is a relatively new dataset and as of now there is no online leaderboard for it. The authors of the original paper used an attention-based sequence to sequence model as their baseline method. The authors generated a program containing both instructions that generate output and instructions that simply generate intermediate values used by following instructions. The program uses a latent predictor network which generates an output sequence conditioned on an arbitrary number of input functions and staged back-propagation to save memory. Going into further depth about this program would be beyond this book so I'd advise to have a look at the original paper.
The program outperformed the baseline model and achieved a perplexity of 28.5, a BLEU score of 27.2 and has and accuracy of 36.4. \citep{ling2017program}

The paper and examples of the dataset can be found here:
\center \url{https://github.com/deepmind/AQuA}

\flushleft

\hypertarget{snli}{%
\subsection{SNLI}\label{snli}}

When it comes to understanding natural language, the understanding of entailment and contradiction is essential. The characterization and use of these relationships in computational systems is called natural language inference and is fundamental for tasks such as commonsense reasoning and information retrieval. The \textbf{S}tanford \textbf{N}atural \textbf{L}anguage \textbf{I}nference Corpus is a collection of sentence pairs that are labeled either as entailment, contradiction or semantic independence.
While there are other datasets that try to accomplish this particular task, they all have problems of size, quality and vagueness.

SNLI consists of about 570k record pairs. Again, crowdworkers were used to construct the dataset. For this purpose they were shown the caption of a photo but not the photo itself, and asked to write three alternative captions: One that is definitely a true description of the photo, one that could be a true description of the photo, and one caption that is definitely a false description of the photo. By not showing the photo, the authors wanted to ensure that each pair of sentences could be reconstructed based on the available text alone.
To quantify the quality of the corpus, about 10\% of all created sentence pairs were validated. For this purpose, each crowdworker was shown five pairs of sentences and asked to mark them with one of the three labels. Each set was shown to a total of five crowdworkers. For each pair, a gold label was awarded if at least three of the five annotators chose the same label. About 98\% of all sentence pairs received a gold label, the rest were given a placeholder label. \citep{bowman2015large}

The models are evaluated again with the accuracy of the predicted label. There is no measurement of human performance for the SNLI corpus. At present, the most accurate model is a semantics-aware BERT (SemBERT) with an accuracy of 91.9.
The paper and examples of the dataset can be found here:
\center \url{https://nlp.stanford.edu/projects/snli/}

\flushleft

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

Below is a brief overview of all of the datasets discussed in this chapter, including some other interesting datasets. If you would like to learn more about one of the datasets, for each dataset the corresponding paper is linked.

\begin{longtable}[]{@{}llll@{}}
\toprule
\begin{minipage}[b]{0.31\columnwidth}\raggedright
Name\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\raggedright
Task\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedright
Size\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://arxiv.org/pdf/1806.03822.pdf}{SQuAD 2.0}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Question Answering, Reading Comprehension\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
150,000\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Paragraphs w questions and answers\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://arxiv.org/pdf/1808.07042.pdf}{CoQA}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Question Answering, Reading Comprehension\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
127,000\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Answering interconnected questions\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://openreview.net/pdf?id=rJ4km2R5t7}{GLUE}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
General Language Understanding\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
---\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Nine different NLU tasks\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://w4ngatang.github.io/static/papers/superglue.pdf}{SuperGLUE}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
General Language Understanding\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
---\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Eight different NLU tasks\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://arxiv.org/pdf/1705.04146.pdf}{AQuA-Rat}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Question Answering, Reading Comprehension, Mathematical Reasoning\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
100,000\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Solving algebraic word problems\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://nlp.stanford.edu/pubs/snli_paper.pdf}{SNLI}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Natural Language Inference\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
570,000\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Understanding entailment and contradiction\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{http://www.roman-klinger.de/publications/ling2016.pdf}{Irony Sarcasm Analysis Corpus}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Classification, Sentiment Analysis\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
33,000\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Ironic, sarcastic, regular and figurative tweets\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://arxiv.org/pdf/1609.07843.pdf}{WikiText-103 \& 2}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Language Modelling\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
100M+\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Word and character level tokens from Wikipedia\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf}{WMT 14 English-German}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Language Translation\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
4.5M\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Sentence pairs for translation\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.31\columnwidth}\raggedright
\href{https://arxiv.org/pdf/1804.05053.pdf}{VOiCES}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Speech Recognition\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright
3,900\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Voices in complex environmental settings. 15h material\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{pre-trained-models}{%
\section{Pre-Trained Models}\label{pre-trained-models}}

In the last chapters we've already heard quite a bit about pre-trained models like BERT or GPT-3. But exactly on what data are they trained on? Let's find out.

\hypertarget{bert}{%
\subsection{BERT}\label{bert}}

The pre-training corpus used for BERT consists of the BookCorpus and the entirety of the English Wikipedia.

\textbf{The BookCorpus}: This dataset was released in 2015. To create the corpus, 11,038 free books were collected from the Internet. All of these were written by authors who have not yet been published. To be included, a book had to have more than 20,000 words to filter out the shorter stories that might be noisy. The dataset includes over 16 different genres, for example \emph{Romance}, \emph{Science Fiction} or \emph{Fantasy}. In total there are about 1 billion words, 1.3 million unique words and 74 million sentences with an average sentence length of 13 words in the BookCorpus. \citep{bookCorpus}

\textbf{English Wikipedia}: For Wikipedia only text passages were extracted, while lists, tables and headings were ignored. In total, this dataset contains 2.5 billion words.

According to the authors, it is crucial to use a document-level corpus rather than a shuffled sentence-level corpus like the Billion Word benchmark dataset to extract long sentences. \citep{bert}

\hypertarget{openai-gpt-3}{%
\subsection{OpenAI GPT-3}\label{openai-gpt-3}}

The dataset used for pre-training GPT-3 consists of a filtered version of the Common Crawl dataset and multiple curated high quality datasets, including an extended version of WebText, two books corpora and the English language Wikipedia.

\textbf{Common Crawl}: The Common Crawl corpus contains petabytes of data collected over 8 years of web crawling. The corpus contains raw web page data, metadata and text extracts. To improve the quality of Common Crawl, two techniques are used: (1) filtering Common Crawl and (2) fuzzy deduplication.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To improve the quality, the original WebText was used as a proxy for high quality documents. A classifier was trained to distinguish these documents from the raw text in the Common Crawl. This classifier was used to re-sample Common Crawl by prioritizing documents for which higher quality was predicted. A logistic regression was trained for the classifier using characteristics from the standard Spark and HashingTF tokenizer. A document was kept in the dataset, if
\end{enumerate}

\[\hbox{np.random.pareto}(\alpha) > 1 - \hbox{document\_score}.\] A value of 9 was chosen for \(\alpha\) in order to obtain both high and low scoring, but mostly high scoring documents.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  To prevent overfitting, documents were fuzzily deduplicated using Spark's MinHashLSH implementation with 10 hashes. WebText was also fuzzily removed from Common Crawl. This decreased dataset size by around 10\%.
\end{enumerate}

\textbf{WebText}: The WebText dataset is a dataset created by web scraping that emphasizes the quality of the documents. Only websites that have been curated/filtered by humans have been scrapped. To simplify this task, all outbound links from Reddit, a social media platform, which received at least 3 Karma, were used. The resulting dataset contains the text subset of these 45 million links. Fuzzy deduplication was also used here.

\textbf{Books1 and Books2}: These are two internet-based books corpora on which fuzzy deduplication was performed. Nothing more is known about these datasets.

The datasets used to train GPT-3 are shown in the table below.

\begin{tabular}{l|l|l}
\hline
Dataset & Quantity (tokens) & Weight in training mix\\
\hline
Common Crawl (filtered) & 410 billion & 60\%\\
\hline
WebText2 & 19 billion & 22\%\\
\hline
Books1 & 12 billion & 8\%\\
\hline
Books2 & 55 billion & 8\%\\
\hline
Wikipedia & 3 billion & 3\%\\
\hline
\end{tabular}

\citep{brown2020language}

\hypertarget{google-5t}{%
\subsection{Google 5T}\label{google-5t}}

Google 5T also uses a dataset based on Common Crawl for pre-training their model, called the ``Colossal Clean Crawled Corpus'' (C4). To improve the quality of Common Crawl, the following heuristics were used:

\begin{itemize}
\tightlist
\item
  Only keep lines that end in a period, exclamation mark, question mark, or closing quotation mark.
\item
  Remove any page that contains a word from the ``List of Dirty, Naughty, Obscene or Otherwise Bad Words''.
\item
  Remove any line containing the word Javascript to remove warnings about enabling Javascript.
\item
  Remove any page containing the phrase ``lorem ipsum''.
\item
  Remove all pages that contain ``\{'' because some pages may have accidentally contained code.
\item
  To deduplicate the dataset, discard all but one on any three-sentence span occurring more than once in the dataset.
\end{itemize}

Furthermore, \textbf{langdetect} was used to filter out any pages that were not classified as English with a probability of at least 99\%.

\citep{raffel2019exploring}

\hypertarget{resources-for-resources}{%
\section{Resources for Resources}\label{resources-for-resources}}

If you are interest in further NLP tasks or dataset, there are two websites worth checking out.

Papers With Code highlights trending Machine Learning research and the code to implement it. Their mission is to create a free and open resource with ML papers, code and evaluation tables. Anyone can contribute by downloading data, training their own model and comparing their model to others.

To see the newest trends in NLP, check out the link below.

\center \url{https://paperswithcode.com/area/natural-language-processing}

\flushleft

If you want to refine your natural language processing (NLP) skills, finding accessible and relevant datasets can be one of the biggest bottlenecks. A lot of time can be spent searching for accessible datasets for the learning task at hand or trying to curate your own data instead. This is where The Big Bad NLP Database, managed by Quantum Stat, comes in. It is a central location for NLP datasets. Currently there are over 500 data entries for general NLP tasks, such as question answering or language modeling. While most of the datasets are in English, there are also a number of datasets in other languages.
Just have a look for yourself!

\center \url{https://datasets.quantumstat.com/}

\flushleft

\hypertarget{use-cases-for-nlp}{%
\chapter{Use-Cases for NLP}\label{use-cases-for-nlp}}

\emph{Author: Matthias Aßenmacher}

Since it is all fine and good to know the theory and how everything's working on the inside,
we thought that this booklet could also benefit from showcasing some exemplary use case(s).
Initially we had planned to include two chapters on different use cases, but during the course
a student assigned to one of these chapters dropped out of the course. So we were left with only
one (but highly motivated) student willing to pursue this issue.

The following chapter will contain a use case on Natural Language Generation (NLG) which is a task
that (oftentimes) strongly relies on an encoder-decoder style architecture. Recently, these types
of models could benefit \textbf{a lot} from the use of Attention mechanisms (\citet{bahdanau2014neural}) and the
proposal of the Transformer architecture (\citet{vaswani2017attention}) presented in Chapter 8.
Nevertheless the task of NLU requires more than just good architectures, it is also necessary to
handle the output of these models in a suitable way in order to generate meaningful text.

So without further ado: The following chapter will showcase the challenges of NLU by touching upon
the the tow topic \emph{Chatbots} and \emph{Image Captioning}, where the latter represents a hybrid task of
Computer Vision and NLP.

\hypertarget{natural-language-generation}{%
\chapter{Natural Language Generation}\label{natural-language-generation}}

\emph{Author: Haris Jabbar}

\emph{Supervisor: Matthias Aßenmacher}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Machine learning systems can be differentiated into two types: Discriminative and Generative. While discriminative systems like classification, regression, clustering are the more well known type, it's the Generative systems that hold greater promise of achieving Artificial General Intelligence. In essence, a Generative system is expected to produce images, text or audio that would be meaningful to the users. Generating a picture of a horse is a much harder problem than just identifying whether there is a horse in the picture.

In this chapter, I will tackle the generative processes in NLP. Understandably, the field is called Natural Language Generation (NLG).

\hypertarget{definition-and-taxonomy}{%
\section{Definition and Taxonomy}\label{definition-and-taxonomy}}

Reiter and Dale (2000) defined Natural Language Generation (NLG) as ``the sub-field of artificial intelligence and computational linguistics that is concerned with the construction of computer systems than can produce understandable texts in English or other human languages from some underlying non-linguistic representation of information''.

Two aspects need to be highlighted. First is generation of understandable text in a human language and second is the input to such a generation system is `non linguistic' representation of information. For our purposes, we will drop the second requirement; which means that the source can be text as well. With such a definition in mind, we can have following taxonomy of NLG systems :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Text-to-Text

  \begin{itemize}
  \tightlist
  \item
    Machine Translation : Automatically translating between various human languages
  \item
    Text Summarization : Summarizing a (big) text document into a shorter summary/abstract.
  \end{itemize}
\item
  Data-to-Text

  \begin{itemize}
  \tightlist
  \item
    Image Captioning : Describe the image in a short sentence.
  \item
    Business Intelligence : Creating text summaries of data from conventional databases (e.g SQL)
  \end{itemize}
\item
  Ideas-to-Text

  \begin{itemize}
  \tightlist
  \item
    Poetry/Song Generation : Generating a song from a few keywords or mimicking the style of a certain artist.
  \item
    Fake News : Automatically generating news items that look credible but are not.
  \end{itemize}
\item
  Dialog Systems (Chatbots)

  \begin{itemize}
  \tightlist
  \item
    Goal Oriented : Chatting with a computer system (agent) with a specific purpose (e.g.~booking a flight)
  \item
    Open ended conversations : When the conversation with the agent is casual chit-chat but has the components of information, emotion and human like empathy.
  \end{itemize}
\end{enumerate}

\begin{figure}
\centering
\includegraphics{figures/04-01-use-case1/nlg_use_cases.png}
\caption{NLG Use Cases}
\end{figure}

\hypertarget{common-architectures}{%
\section{Common Architectures}\label{common-architectures}}

There are many architectures that are common across most of above mentioned NLG systems. While some are used in other NLP domains as well, in the following sections I will explain them with a focus on language generation.

\hypertarget{encoder-decoder-architecture}{%
\subsection{Encoder-Decoder Architecture}\label{encoder-decoder-architecture}}

The most ubiquitous architecture for NLG is the encoder-decoder architecture, and especially the decoder part of it. Hence I will explain it in some detail. The architecture is shown in the following figures:

\begin{figure}
\centering
\includegraphics{figures/04-01-use-case1/encoder_decoder_trg.jpg}
\caption{Encoder-Decoder (Training)}
\end{figure}

\begin{figure}
\centering
\includegraphics{figures/04-01-use-case1/encoder_decoder_inf.jpg}
\caption{Encoder-Decoder (Inference)}
\end{figure}

The architecture can be seen as conditional probability P(y/x) with `y' being the output of the decoder and it is conditioned on `x' (the output of the encoder). Hence the NLG task becomes generating text through decoder conditioned on some input, coming from the encoder.

\hypertarget{encoder}{%
\subsubsection{Encoder :}\label{encoder}}

As stated above, the purpose of this part of the network is to provide conditional information on which the decoder generates text. As such, this part can have \textbf{ANY} architecture that provides some form of embedding of the input. It can be a convolutional neural network to condition the generated text on some properties of an image (for example image captioning), or RNN/LSTM/Transformer architecture for text or audio based conditioning; or even a simple feed forward network to condition it on SQL database for example. For the purpose of illustration we will be using an RNN/LSTM with text as input condition (as shown in the figure).

The thing to note here is that the richer the feature vector going from encoder to decoder, the more information decoder would have to generate better output. This was the motivation to move from single feature vector (\textbackslash{}cite) to multiple vectors (\textbackslash{}cite) and to attention based models (\textbackslash{}cite). This trend finally led to the transformer based models.

\hypertarget{decoder}{%
\subsubsection{Decoder :}\label{decoder}}

The decoder is the most distinctive part of an NLG system. Almost all decoders have the same form as shown in the figures above. The purpose is to generate text tokens (\textbackslash{}cite) one after the other until a terminating criteria is met. This termination is usually a termination token (\textless{}end\textgreater{} in the figures) or a max length criteria.

During training, we are given an input (text/image/audio) and the `gold label text' that we want the system to learn to generate for that particular input. In the given example, the input is the text ``How are you?'' and the gold label is ``I am good''. The input goes through the encoder and produces a feature vector that is used as the input to decoder. The decoder then generates tokens one by one and the loss is calculated after the softmax layer from the generated token and the gold label token. Note the inclusion of an extra token `\textless{}null\textgreater{}' as the first token. The last token of the gold label should produce the `\textless{}end\textgreater{}' token.

During inference, we don't have the gold label, so the output of one step is used as input to next step, as shown in the figure. Note that it matches with the setup during training. The generator stops when `\textless{}end\textgreater{}' token is emitted; thus completing the inference.

\hypertarget{attention-architecture}{%
\subsection{Attention Architecture}\label{attention-architecture}}

The attention architecture is introduced in detail in section (\textbackslash{}cite). Here I will briefly mention its use in NLG systems. Looking at the picture below, we can see that the attention is from decoder to encoder.

\begin{figure}
\centering
\includegraphics{figures/04-01-use-case1/encoder_decoder_attn.jpg}
\caption{Encoder-Decoder (Attention)}
\end{figure}

In other words, before generating each token, the decoder attends to all tokens in the encoder, as shown. The query is the decoder token and key/values are all encoder token. That way the decoder has much richer information to base its output on.

\hypertarget{decoding-algorithm-at-inference}{%
\subsection{Decoding Algorithm at Inference}\label{decoding-algorithm-at-inference}}

Now I will explain the decoding algorithms that are used to generate text from the softmax layer.

As explained above, during inference, the tokens are generated sequentially. In a vanilla version of decoding, at each step of the sequence, the token with highest probability in the softmax layer is generated. This is called `greedy decoding', but it has been shown to produce suboptimal text. There are few improvements over this greedy approach.

\hypertarget{beam-search}{%
\subsubsection{Beam Search}\label{beam-search}}

In greedy decoder we simply output the maximum probability token at each step. But if we track multiple words at each step and then output the sequence formed by highest probability combination, we get beam search. The number of tokens we keep track of is the length of beam (k). The algorithm then goes as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select k-tokens with highest probability at step 1.
\item
  Use these k-tokens to generate k softmax vectors at step 2.
\item
  Keep the k highest scoring combinations.
\item
  Repeat steps 2 and 3 till \textless{}end\textgreater{} token is generated, or a predefined max is reached
\item
  At each step, we have only k-hypothesis, which is the length of beam search.
\end{enumerate}

While beam search tends to improve the quality of generated output, it has its own issues. Chiefly among them is that it tends to produce shorter sequences. Although it can be controlled by the max parameter (of step 4), it's another hyperparameter to be reckoned with.

\hypertarget{pure-sampling-decoder}{%
\subsubsection{Pure Sampling Decoder}\label{pure-sampling-decoder}}

Here, at each step, instead of taking the token with maximum probability like in greedy search, the token is sampled from the whole vocabulary according to the probability distribution predicted by the softmax layer.

\hypertarget{k-sampling-decoder}{%
\subsubsection{K-sampling Decoder}\label{k-sampling-decoder}}

It's like pure sampling decoder, but instead of sampling from whole vocabulary, the token is sampled only from the k-highest probability tokens.

\hypertarget{memory-networks}{%
\subsection{Memory Networks}\label{memory-networks}}

Memory Networks is another architecture that is potentially quite useful in language generation tasks. The basic premise is that LSTMs/RNNs and even Transformer architecture stores all the information only in the weights of the network. When we want to generate text that should include information from a large knowledge base, this `storage' of network weights is insufficient. Memory networks resolve this problem by employing an external storage (the memory) that it can use during language generation. Conceptual diagram is showing in the following figure, followed by a brief description.

\includegraphics{figures/04-01-use-case1/memory_networks.jpg}
Memory (M) in this context can be any database that can be queried by the network. Usually it is of the form of key-value pairs or a simple array of vectors embedding a corpus of knowledge (eg DBPedia/wikipedia). For any query input (x), first we get an embedding. This is then used to attend over the memory M in the usual attention mechanism. The output is the weighted sum of memory that incorporates information from complete knowledge corpus. In some cases the output can also be used to update the memory database.

\hypertarget{language-models}{%
\subsection{Language Models}\label{language-models}}

Language models are probably the most important ingredient of generating text. As the name implies, they model the probability distribution of generating words and characters. More concretely, they model the conditional probability distribution P(w\_t/w\_\{t-1\}). Thus with a given input vector coming from a source (image/database/text), this model can be used to generate words one after the other.

\hypertarget{question-answer-systems}{%
\section{Question-Answer Systems}\label{question-answer-systems}}

The question-answer systems attempt to extract or generate answers from a given question and either a fixed or open ended context. The context here is the corpus from which the answer needs to be generated. For example in SQUAD dataset (\textbackslash{}cite) the context is a given paragraph from wikipedia and the question is asked from that paragraph. In open ended contexts, the whole wikipedia (or other corpus) can be the contexts.

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

\begin{itemize}
\tightlist
\item
  msmarco
\item
  google natural questions
\item
  multiple choice

  \begin{itemize}
  \tightlist
  \item
    swag/trivia qa
  \end{itemize}
\item
  conversational qa

  \begin{itemize}
  \tightlist
  \item
    coqa, wizard of wikipedia, quac.ai
  \end{itemize}
\item
  many others e.g.~visual qa, KB qa etc.
\end{itemize}

\hypertarget{types}{%
\subsection{Types}\label{types}}

\begin{itemize}
\tightlist
\item
  Question Answer Systems

  \begin{itemize}
  \tightlist
  \item
    Structured knowledge source
    The sources can be e.g.~Freebase, Wikidata, DBpedia or RDBMS systems
  \item
    Unstructured knowledge soure
    Free text e.g.~Wikipedia
  \item
    FAQs
    Extract answers for similar questions to the given in a corpus of question-answer pairs.
  \end{itemize}
\end{itemize}

\hypertarget{architectures}{%
\subsection{Architectures}\label{architectures}}

\begin{itemize}
\tightlist
\item
  Context
\item
  Question
\end{itemize}

Five conceptual levels
- Token level features (embeddings)
- Context and question encoder
- Attention from context to question or vice versa
- Modeling layer
- Output layer

\begin{itemize}
\item
  Pointer Networks
\item
  Open Domain QA

  \begin{itemize}
  \tightlist
  \item
    DrQA
  \item
    Distant Supervision
  \end{itemize}
\end{itemize}

\hypertarget{evaluation-metrics}{%
\subsection{Evaluation Metrics}\label{evaluation-metrics}}

There are generally two metrics commonly used in QA systems. The exact match (EM) and F1 score.

\hypertarget{dialog-systems}{%
\section{Dialog Systems}\label{dialog-systems}}

These are the systems where an agent chats with a human being either with a specific purpose (goal oriented) or it is a general open ended chat. The examples of goal oriented chats include tasks like booking an appointment or a flight ticket. Open ended chats can be talking about a general topic which may or may not include a `personality' for the chatbot.

\hypertarget{types-1}{%
\subsection{Types}\label{types-1}}

\begin{itemize}
\tightlist
\item
  Chatbots

  \begin{itemize}
  \tightlist
  \item
    Open domain
  \item
    Goal Oriented
  \end{itemize}
\end{itemize}

\hypertarget{architectures-1}{%
\subsection{Architectures}\label{architectures-1}}

\begin{itemize}
\tightlist
\item
  Information Retrieval

  \begin{itemize}
  \tightlist
  \item
    Inbuilt in the model weights
  \item
    External Source

    \begin{itemize}
    \tightlist
    \item
      Memory Networks
    \item
      API calls (?)
    \end{itemize}
  \end{itemize}
\item
  Text Generation

  \begin{itemize}
  \tightlist
  \item
    Encoder-Decoder
  \end{itemize}
\end{itemize}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Natural Language Generation (NLG) has huge potential to be not only an academic domain for research but to affect and improve our daily lives. In this chapter I have talked about only two of its manifestations.

\hypertarget{epilogue}{%
\chapter{Epilogue}\label{epilogue}}

\emph{Author: Matthias Aßenmacher}

Since this project was realized in a limited time frame and accounted for about one third
of the ECTS points which should be achieved during one semester, it is obvious that this
booklet cannot provide exhaustive coverage of the vast research field of \emph{Natural Language Processing}.

Furthermore this area of research is moving very rapidly at the moment, which means that
certain architectures, improvements or ideas had net yet even been published when we sat down
and came up with the chapter topics in February 2020. Thus, this epilogue tries to put the content
of this booklet into context and relate it to what is currently happening. Thereby we will focus on
mainly three aspects:

\begin{itemize}
\tightlist
\item
  New influential (or even state-of-the-art) architectures
\item
  Improvements and work on the Attention-mechanism and Transformers
\item
  Work on proper evaluation and interpretability
\end{itemize}

\hypertarget{new-influentioal-architectures}{%
\section{New influentioal architectures}\label{new-influentioal-architectures}}

In \href{./transfer-learning-for-nlp-i.html}{\textbf{Chapter 7: ``Transfer Learning for NLP I''}} and \href{./transfer-learning-for-nlp-ii.html}{\textbf{Chapter 9: ``Transfer Learning for NLP I''}} some of the most important models for \emph{sequential transfer learning} have been presented. We chose to narrow ourselves down to this type of models, since we considered them to be most important to begin with, in order to unterstand the overall concept. Nevertheless, other influential architectures shall also be addressed:

\begin{itemize}
\tightlist
\item
  An architecture with a relatively interesting pre-training objective the interested reader might want to have a look at, is \href{https://openreview.net/pdf?id=r1xMH1BtvB}{\textbf{ELECTRA}} (\citet{Clark2020ELECTRA}).
\item
  \href{https://arxiv.org/pdf/1910.10683.pdf}{\textbf{Google's T5}} (\citet{raffel2019exploring}) (already briefly mentioned in \href{./transfer-learning-for-nlp-ii.html}{Chapter 9}) does not fit into the category of sequential tansfer learning but rather belongs to \emph{multi-task learning} models (cf.~Fig. 7.1) in \href{./transfer-learning-for-nlp-i.html}{Chapter 7})) since it is trained on multiple tasks at once. This is possible due to transformation of the entire input \emph{and} output to strings, which essentially converts every tasks to as seq-to-seq task.
\item
  In May 2020 the \href{https://arxiv.org/pdf/2005.14165.pdf}{\textbf{OpenAI GPT-3}} (\citet{brown2020language}) shook the NLP community and triggered \emph{a lot} of subsequent research. This model puts, as already mentioned in \href{./transfer-learning-for-nlp-ii.html}{Chapter 9}, is by far bigger then every previous model and put a special focus on \emph{few-shot learning}-
\end{itemize}

\hypertarget{improvements-of-the-selfattention-mechanism}{%
\section{Improvements of the SelfAttention mechanism}\label{improvements-of-the-selfattention-mechanism}}

Recently, there has been a lot effort put in improving the Self-Attention, mostly by reducing its computational cost
and this enabling models to process longer sequences. One interesting article has already been discussed at the end of \href{./attention-and-self-attention-for-nlp.html}{Chapter 8}, while another interesting piece of work has been published recently by \citet{wang2020linformer}: The so-called \href{https://arxiv.org/pdf/2006.04768.pdf}{Linformer}

\hypertarget{evaluation-and-interpretability}{%
\section{Evaluation and Interpretability}\label{evaluation-and-interpretability}}

While ``traditional'' Benchmark (collections) have been discussed in \href{./resources-and-benchmarks-for-nlp.html}{Chapter 11}, there is a lot of ongoing research about proper evaluation and interpretability of NLP models. Here are just two examples of impressive work, which was published very recently:

\begin{itemize}
\tightlist
\item
  \citet{ribeiro-etal-2020-beyond} won the best paper award at ACL 2020 for their article \href{https://www.aclweb.org/anthology/2020.acl-main.442.pdf}{Beyond Accuracy: Behavioral Testing of NLP Models with CheckList}
\item
  Google launched a \href{https://github.com/pair-code/lit}{Language Interpretability Tool} together with an accompanying \href{https://arxiv.org/pdf/2008.05122.pdf}{research article} (\citet{tenney2020language})
\end{itemize}

We hope that this little outlook can adequately round off this nice piece of academic work created by extremely motivated students and we hope that you enjoyed reading.

\hypertarget{acknowledgements}{%
\chapter{Acknowledgements}\label{acknowledgements}}

The most important contributions are from the students themselves.
The success of such projects highly depends on the students.
And this book is a success, so thanks a lot to all the authors!
The other important role is the supervisor.
Thanks to all the supervisors who participated!
Special thanks to \href{https://www.misoda.statistik.uni-muenchen.de/personen/professoren/heumann/index.html}{Christian Heumann} who enabled us to conduct the seminar in such an experimental way, supported us and gave valuable feedback for the seminar structure.
Thanks a lot as well to the entire \href{https://www.statistik.uni-muenchen.de/}{Department of Statistics} and the \href{http://www.en.uni-muenchen.de/index.html}{LMU Munich} for the infrastructure.

The authors of this work take full responsibilities for its content.

\bibliography{book.bib,packages.bib}

\backmatter
\printindex

\end{document}
