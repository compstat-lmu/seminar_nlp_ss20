---
output:
  pdf_document: default
  html_document: default
---
# Introduction: Transfer Learning for NLP

*Authors: Carolin Becker, Joshua Wagner, Bailan He*

*Supervisor: Matthias AÃŸenmacher*


As discussed in the previous chapters, natural language processing (NLP) belongs to a very powerful tool in the field of processing human language. In recent years, there have been many proceedings and improvements in NLP to the state-of-art models like BERT. A decisive further development in the past was the way to transfer learning, but also self-attention. 

In the next three chapters, various NLP models will be presented, which will be taken to a new level with the help of transfer learning in a first and a second step with self-attention and transformer-based model architectures. To understand the models of the next chapters, the idea and the advantages of the model architecture transfer learning, its benefits for NLP, and the concept of self-attention will be presented next, followed by an overview of the most important models. 

## What is Transfer Learning?

```{r ch02-figure01, echo=FALSE, out.width="70%", fig.cap="(ref:ch02-figure01)", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/compare-classical-transferlearning-ml.PNG")
```
(ref:ch02-figure01) Classic Machine Learning and Transfer Learning

In figure \@ref(fig:ch02-figure01) the difference between classical machine learning and transfer learning is shown. For classical machine learning for every special task or domain, one model is trained. 

Transfer learning allows us to deal with the learning task by using the existing labeled data of some related tasks or domains. Tasks are the objective of the model. e.g. the sentiment of a sentence, whereas the domain is where data comes from. e.g. all sentences are selected from Reddit. In the example above, we try to store the knowledge gained in task A in the source domain A and apply it to our problem of interest(domain B).

Generally, transfer learning has several advantages over classical machine learning: saving time for model training, mostly better performance, and not a need for a lot of training data. 

Especially, the last advantage besides the better results and time is an important topic in NLP problems, as there is a lot of knowledge about many texts, but normally the training data only contains a small piece of it. @evolutiontransferlearning An classical NLP model captures and learns a variety of linguistic phenomena, such as long-term dependencies and negation, from a large-scale corpus. This knowledge can be transferred to initialize another model to perform well on a specific NLP task, such as sentiment analysis.

## (Self-)attention
The most common models for language modeling and maschine translation were, and still are to some extent, recurrent neural networks with long short-term memory [@hochreiter1997long] or gated recurrent units [@gru]. These models commonly used an encoder and a decoder. Advanced models used attention, either based on Bahdanau attention [@bahdanau2014neural] or Loung attention [@luong2015effective].

[@vaswani2017attention] introduced a new form of attention, self-attention, and with it a new class of models, the \textit{transformers}. A Transformer still consists of the typical encoder-decoder setup but uses a novel new architecture for both. The encoder consists of 6 Layers with 2 sublayers each. The newly developed self-attention in the first sublayer allows a transformer model to process all input words at once and model the relationships between all words in a sentence. This allows transformers to model long-range dependencies in a sentence faster than the before seen RNN and CNN based models. The speed improvement and the fact that ``individual attention heads clearly learn to perform different tasks''[@vaswani2017attention] lead to the eventual development of **B**idirectional **E**ncoder **R**epresentations from **T**ransformers by @bert. **BERT** and its successors are, at the time of writing, the state-of-the-art models used for transfer learning in NLP. For a more in-depth explanation of the architecture of early transformers and self-attention see Chapter [9](#Attention-and-self-Attention-for-nlp) and [10](#Transfer-Learning-for-NLP-II) for **BERT**.

## Overview over important NLP models

```{r ch02-figure02, echo=FALSE, out.width="70%", fig.cap="(ref:ch02-figure02)", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/overview-tranferlearning.PNG")
```
(ref:ch02-figure02) Overview of the models and concepts presented in the following chapters

In figure \@ref(fig:ch02-figure02) the following models will be presented in the next chapters. 

First, the two model architectures ELMo and ULMFit will be presented which are mainly based on transfer learning and LSTMs in **Chapter 8: "Transfer Learning for NLP I"**: 

* **ELMo** (Embeddings from Language Models) first published in @elmopaper uses a deep, bi-directional LSTM model to create word representations. This method goes beyond traditional embedding methods, as it analyses the words within the context

* **ULMFiT**(Universal Language Model Fine-tuning for Text Classification) consists of three steps: first, there is a general pre-training of the LM on a general domain (like WikiText-103 dataset), second, the LM is finetuned on the target task and the last step is the multilabel classifier fine tuning where the model provides a status for every input sentence.

Following this the concepts attention and self-attention will be further discussed in the **"Chapter 9: Attention and Self-Attention for NLP "**.

In the **"Chapter 10: Transfer Learning for NLP II"** models like BERT, GTP2 and XLNet will be introduced as they include transfer learning in combination with self-attention: 

* **BERT**

* **GTP2**

* **XLNet**