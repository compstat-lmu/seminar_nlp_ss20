<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Introduction: Deep Learning for NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Introduction: Deep Learning for NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Introduction: Deep Learning for NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-06-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter-1.html"/>
<link rel="next" href="foundationsapplications-of-modern-nlp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter-1.html"><a href="chapter-1.html"><i class="fa fa-check"></i><b>2</b> Chapter 1</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter-1.html"><a href="chapter-1.html#lorem-ipsum"><i class="fa fa-check"></i><b>2.1</b> Lorem Ipsum</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-1.html"><a href="chapter-1.html#using-figures"><i class="fa fa-check"></i><b>2.2</b> Using Figures</a></li>
<li class="chapter" data-level="2.3" data-path="chapter-1.html"><a href="chapter-1.html#using-tex"><i class="fa fa-check"></i><b>2.3</b> Using Tex</a></li>
<li class="chapter" data-level="2.4" data-path="chapter-1.html"><a href="chapter-1.html#using-stored-results"><i class="fa fa-check"></i><b>2.4</b> Using Stored Results</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>3</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>3.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>3.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>3.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>4</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>4.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="4.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>4.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="4.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>4.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="4.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>4.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="4.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>4.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#model-improvements"><i class="fa fa-check"></i><b>4.3</b> Model Improvements</a><ul>
<li class="chapter" data-level="4.3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#fasttext"><i class="fa fa-check"></i><b>4.3.1</b> fastText</a></li>
<li class="chapter" data-level="4.3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word-phrases"><i class="fa fa-check"></i><b>4.3.2</b> Word Phrases</a></li>
<li class="chapter" data-level="4.3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#multiple-meanings-per-word"><i class="fa fa-check"></i><b>4.3.3</b> Multiple Meanings per Word</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>4.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="4.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>4.5</b> Evaluation Methods</a></li>
<li class="chapter" data-level="4.6" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#sources-and-applications-of-word-embeddings"><i class="fa fa-check"></i><b>4.6</b> Sources and Applications of Word Embeddings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>5.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="5.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>5.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="5.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>5.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="5.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>5.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>5.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="5.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>5.2.1</b> LSTM</a></li>
<li class="chapter" data-level="5.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>5.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extentions-of-simple-rnns"><i class="fa fa-check"></i><b>5.3</b> Extentions of Simple RNNs</a><ul>
<li class="chapter" data-level="5.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#bidirectional-rnns"><i class="fa fa-check"></i><b>5.3.1</b> Bidirectional RNNs</a></li>
<li class="chapter" data-level="5.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>5.3.2</b> Deep RNNs</a></li>
<li class="chapter" data-level="5.3.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>5.3.3</b> Encoder-Decoder Architecture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>6</b> Convolutional neural networks and their applications in NLP</a></li>
<li class="chapter" data-level="7" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>7</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>7.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>7.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="7.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>7.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>8</b> Transfer Learning for NLP I</a></li>
<li class="chapter" data-level="9" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>9</b> Attention and Self-Attention for NLP</a></li>
<li class="chapter" data-level="10" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>10</b> Transfer Learning for NLP II</a></li>
<li class="chapter" data-level="11" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="12" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Resources and Benchmarks for NLP</a></li>
<li class="chapter" data-level="13" data-path="software-for-nlp-the-huggingface-transformers-module.html"><a href="software-for-nlp-the-huggingface-transformers-module.html"><i class="fa fa-check"></i><b>13</b> Software for NLP: The huggingface transformers module</a></li>
<li class="chapter" data-level="14" data-path="use-bases-for-nlp.html"><a href="use-bases-for-nlp.html"><i class="fa fa-check"></i><b>14</b> Use-Bases for NLP</a></li>
<li class="chapter" data-level="15" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>15</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="15.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#introduction-1"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition"><i class="fa fa-check"></i><b>15.2</b> Definition</a></li>
<li class="chapter" data-level="15.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#domains-of-nlg-systems"><i class="fa fa-check"></i><b>15.3</b> Domains of NLG Systems</a></li>
<li class="chapter" data-level="15.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#commercial-activity"><i class="fa fa-check"></i><b>15.4</b> Commercial Activity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="the-architectures.html"><a href="the-architectures.html"><i class="fa fa-check"></i><b>16</b> The Architectures</a><ul>
<li class="chapter" data-level="16.1" data-path="the-architectures.html"><a href="the-architectures.html#encoder-decoder-architecture-1"><i class="fa fa-check"></i><b>16.1</b> Encoder-Decoder Architecture</a><ul>
<li class="chapter" data-level="16.1.1" data-path="the-architectures.html"><a href="the-architectures.html#encoder"><i class="fa fa-check"></i><b>16.1.1</b> Encoder :</a></li>
<li class="chapter" data-level="16.1.2" data-path="the-architectures.html"><a href="the-architectures.html#decoder"><i class="fa fa-check"></i><b>16.1.2</b> Decoder :</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="the-architectures.html"><a href="the-architectures.html#attention-architecture"><i class="fa fa-check"></i><b>16.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="16.3" data-path="the-architectures.html"><a href="the-architectures.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>16.3</b> Decoding Algorithm at Inference</a><ul>
<li class="chapter" data-level="16.3.1" data-path="the-architectures.html"><a href="the-architectures.html#beam-search"><i class="fa fa-check"></i><b>16.3.1</b> Beam Search</a></li>
<li class="chapter" data-level="16.3.2" data-path="the-architectures.html"><a href="the-architectures.html#pure-sampling-decoder"><i class="fa fa-check"></i><b>16.3.2</b> Pure Sampling Decoder</a></li>
<li class="chapter" data-level="16.3.3" data-path="the-architectures.html"><a href="the-architectures.html#k-sampling-decoder"><i class="fa fa-check"></i><b>16.3.3</b> K-sampling Decoder</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="the-architectures.html"><a href="the-architectures.html#memory-networks"><i class="fa fa-check"></i><b>16.4</b> Memory Networks</a></li>
<li class="chapter" data-level="16.5" data-path="the-architectures.html"><a href="the-architectures.html#language-models"><i class="fa fa-check"></i><b>16.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="question-answer-systems.html"><a href="question-answer-systems.html"><i class="fa fa-check"></i><b>17</b> Question-Answer Systems</a><ul>
<li class="chapter" data-level="17.1" data-path="question-answer-systems.html"><a href="question-answer-systems.html#datasets"><i class="fa fa-check"></i><b>17.1</b> Datasets</a></li>
<li class="chapter" data-level="17.2" data-path="question-answer-systems.html"><a href="question-answer-systems.html#types"><i class="fa fa-check"></i><b>17.2</b> Types</a></li>
<li class="chapter" data-level="17.3" data-path="question-answer-systems.html"><a href="question-answer-systems.html#architectures"><i class="fa fa-check"></i><b>17.3</b> Architectures</a></li>
<li class="chapter" data-level="17.4" data-path="question-answer-systems.html"><a href="question-answer-systems.html#evaluation-metrics"><i class="fa fa-check"></i><b>17.4</b> Evaluation Metrics</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="dialog-systems.html"><a href="dialog-systems.html"><i class="fa fa-check"></i><b>18</b> Dialog Systems</a><ul>
<li class="chapter" data-level="18.1" data-path="dialog-systems.html"><a href="dialog-systems.html#types-1"><i class="fa fa-check"></i><b>18.1</b> Types</a></li>
<li class="chapter" data-level="18.2" data-path="dialog-systems.html"><a href="dialog-systems.html#architectures-1"><i class="fa fa-check"></i><b>18.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>19</b> Conclusion</a></li>
<li class="chapter" data-level="20" data-path="use-case-ii.html"><a href="use-case-ii.html"><i class="fa fa-check"></i><b>20</b> Use-Case II</a></li>
<li class="chapter" data-level="21" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>21</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-deep-learning-for-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Introduction: Deep Learning for NLP</h1>
<p><em>Authors: Viktoria Szabo, Marianna Plesiak, Rui Yang</em></p>
<p><em>Supervisor: Prof. Dr. Christian Heumann</em></p>
<div id="word-embeddings-and-neural-network-language-models" class="section level2">
<h2><span class="header-section-number">3.1</span> Word Embeddings and Neural Network Language Models</h2>
<p>In natural language processing computers try to analyze and understand human language for the purpose of performing useful tasks. Therefore, they extract relevant information from words and sentences. But how exactly are they doing this? After the first wave of rationalist approaches with handwritten rules didn’t work out too well, neural networks were introduced to find those rules by themselves (see <span class="citation">Bengio et al. (<a href="#ref-Bengio.2003">2003</a>)</span>). But neural networks and other machine learning algorithms cannot handle non-numeric input, so we have to find a way to convert the text we want to analyze into numbers.
There are a lot of possibilities to do that. Two simple approaches would be labeling each word with a number (One-Hot Encoding, figure <a href="introduction-deep-learning-for-nlp.html#fig:onehot-bow">3.1</a>) or counting the frequency of words in different text fragments (Bag-of-Words, figure <a href="introduction-deep-learning-for-nlp.html#fig:onehot-bow">3.1</a>). Both methods result in high-dimensional, sparse (mostly zero) data. And there is another major drawback using such kind of data as input. It does not convey any similarities between words. The word “cat” would be as similar to the word “tiger” as to “car”. That means the model cannot reuse information it already learned about cats for the much rarer word tiger. This will usually lead to poor model performance and is called a lack of generalization power.</p>
<div class="figure"><span id="fig:onehot-bow"></span>
<img src="figures/01-00-deep-learning-for-nlp/01-01_one-hot.png" alt="One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure." width="50%" /><img src="figures/01-00-deep-learning-for-nlp/01-01_bow.png" alt="One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure." width="50%" />
<p class="caption">
FIGURE 3.1: One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure.
</p>
</div>
<p>The solution to this problem is word embedding. Word embeddings use dense vector representations for words. That means they map each word to a continuous vector with n dimensions. The distance in the vector space denotes semantic (dis)similarity. These word embeddings are usually learned by neural networks, either within the final model in an additional layer or in its own model. Once learned they can be reused across different tasks. Practically all NLP projects these days build upon word embeddings, since they have a lot of advantages compared to the aforementioned representations.
The basic idea behind learning word embeddings is the so called “distributional hypothesis” (see <span class="citation">Harris (<a href="#ref-Harris.1954">1954</a>)</span>). It states that words that occur in the same contexts tend to have similar meanings. The two best known approaches for calculating word embeddings are Word2vec from <span class="citation">Mikolov et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> and GloVE from <span class="citation">Pennington et al. (<a href="#ref-Pennington.2014">2014</a>)</span>. The Word2vec models (Continous Bag-Of-Words (CBOW) and Skip-gram) try to predict a target word given his context or context words given a target word using a simple feed-forward neural network. In contrast to these models GloVe not only uses the local context windows, but also incorporates global word co-occurrence counts.
As mentioned, a lot of approaches use neural networks to learn word embeddings. A simple feed-forward network with fully connected layers for learning such embeddings while predicting the next word for a given context is shown in figure <a href="introduction-deep-learning-for-nlp.html#fig:nnlm">3.2</a>. In this example the word embeddings are first learnt in a projection layer and are then used in two hidden layers to model the probability distribution over all words in the vocabulary. With this distribution one can predict the target word. This simple structure can be good enough for some tasks but it also has a lot of limitations. Therefore, recurrent and convolutional networks are used to overcome the limitations of a simple neural network.</p>
<div class="figure"><span id="fig:nnlm"></span>
<img src="figures/01-00-deep-learning-for-nlp/01-01_nnlm.png" alt="Feed-forward Neural Network. Source: Own figure based on Bengio et al. 2013." width="100%" />
<p class="caption">
FIGURE 3.2: Feed-forward Neural Network. Source: Own figure based on Bengio et al. 2013.
</p>
</div>
</div>
<div id="recurrent-neural-networks" class="section level2">
<h2><span class="header-section-number">3.2</span> Recurrent Neural Networks</h2>
<p>The main drawback of feedforward neural networks is that they assume a fixed length of input and output vectors which is known in advance. But for many natural language problems such as machine translation and speech recognition it is impossible to define optimal fixed dimensions a-priori. Other models that map a sequence of words to another sequence of words are needed <span class="citation">(Sutskever, Vinyals, and Le <a href="#ref-sutskever2014sequence">2014</a>)</span>. Recurrent neural networks or RNNs are a special family of neural networks which were explicitely developed for modeling sequential data like text. RNNs process a sequence of words or letters <span class="math inline">\(x^{(1)}, ..., x^{(t)}\)</span> by going through its elements one by one and capturing information based on the previous elements. This information is stored in hidden states <span class="math inline">\(h^{(t)}\)</span> as the network memory. Core idea is rather simple: we start with a zero vector as a hidden state (because there is no memory yet), process the current state at time <span class="math inline">\(t\)</span> as well as the output from the previous hidden state, and give the result as an input to the next iteration <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016deep">2016</a>)</span>.</p>
<p>Basically, a simple RNN is a for-loop that reuses the values which are calculated in the previous iteration <span class="citation">(Chollet <a href="#ref-chollet2018deep">2018</a>)</span>. An unfolded computational graph (figure <a href="introduction-deep-learning-for-nlp.html#fig:01-00-unfolded">3.3</a>) can display the structure of a classical RNN. The gray square on the left represents a delay of one time step and the arrows on the right express the flow of information in time <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016deep">2016</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:01-00-unfolded"></span>
<img src="figures/01-00-deep-learning-for-nlp/01_02_unfolded_graph.png" alt="Right: Circuit diagram (left) and unfolded computational graph (right) of a simple RNN. Source: Own figure." width="100%" />
<p class="caption">
FIGURE 3.3: Right: Circuit diagram (left) and unfolded computational graph (right) of a simple RNN. Source: Own figure.
</p>
</div>
<p>One particular reason why recurrent networks have become such a powerful technique in processing sequential data is parameter sharing. Weight matrices remain the same through the loop and they are used repeatedly, which makes RNNs extremely convenient to work with sequential data because the model size does not grow for longer inputs. Parameter sharing allows application of models to inputs of different length and enables generalization across different positions in real time <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016deep">2016</a>)</span>.</p>
<p>As each part of the output is a function of the previous parts of the output, backpropagation for the RNNs requires recursive computations of the gradient. The so-called backpropagation through time or BPTT is rather simple in theory and allows for the RNNs to access information from many previous steps <span class="citation">(Boden <a href="#ref-boden2002guide">2002</a>)</span>. In practice though, RNNs in their simple form are subject to two big problems: exploding and vanishing gradients. As the gradients are computed recursively, they may become either very small or very large, which leads to a complete loss of information about long-term dependencies. To avoid these problems, gated RNNs were developed and accumulation of information about specific features over a long duration became possible. The two most popular types of gated RNNs, which are widely used in modern NLP, are Long Short-Term Memory models (LSTMs, presented by <span class="citation">Hochreiter and Schmidhuber (<a href="#ref-hochreiter1997long">1997</a>)</span>) and Gated Recurrent Units (GRUs, presented by <span class="citation">Cho et al. (<a href="#ref-cho2014learning">2014</a>)</span>).</p>
<p>Over last couple of years, various extentions of RNNs were developed which resulted in their wide application in different fields of NLP. Encoder-Decoder architectures aim to map input sequences to output sequences of different length and therefore are often applied in machine translation and question answering <span class="citation">(Sutskever, Vinyals, and Le <a href="#ref-sutskever2014sequence">2014</a>)</span>. Bidirectional RNNs feed sequences in their original as well as reverse order because the prediction may depend on the future context, too <span class="citation">(Schuster and Paliwal <a href="#ref-schuster1997bidirectional">1997</a>)</span>. Besides classical tasks as document classification and sentiment analysis, more complicated challenges such as machine translation, part-of-speech tagging or speech recognition can be solved nowadays with the help of advanced versions of RNNs.</p>
</div>
<div id="convolutional-neural-networks" class="section level2">
<h2><span class="header-section-number">3.3</span> Convolutional Neural Networks</h2>
<p>Throughout machine learning or deep learning algorithms, no one algorithm is only applicable to a certain field. Most algorithms that have achieved significant results in a certain field can still achieve very good results in other fields after slight modification. We know that convolutional neural networks (CNN) are widely used in computer vision. For instance, a remarkable CNN model called AlexNet achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge on 30 September 2012 (see <span class="citation">Krizhevsky, Sutskever, and Hinton (<a href="#ref-Krizhevsky2012ImageNetCW">2012</a>)</span>). Subsequently, a majority of models submitted by ImageNet teams from around 2014 are based on CNN. After the convolutional neural network achieved great results in the field of images, some researchers began to explore convolutional neural networks in the field of natural language processing (NLP). Early research was restricted to sentence classification tasks, CNN-based models have achieved very significant effects as well, which also shows that CNN is applicable to some problems in the field of NLP. Similarly, as mentioned before, one of the most common deep learning models in NLP is the recurrent neural network (RNN), which is a kind of sequence learning model and this model is also widely applied in the field of speech processing. In fact, some researchers have tried to implement RNN models in the field of image processing, such as (<span class="citation">Visin et al. (<a href="#ref-Visin2015ReNetAR">2015</a>)</span>). It can be seen that the application of CNN or RNN is not restricted to a specific field.</p>
<p>As the Word2vec algorithm from <span class="citation">Mikolov et al. (<a href="#ref-Mikolov.2013c">2013</a>)</span> and the GloVe algorithm from <span class="citation">Pennington et al. (<a href="#ref-Pennington.2014">2014</a>)</span> for calculating word embeddings became more and more popular, applying this technique as a model input has become one of the most common text processing methods. Simultaneously, significant effectiveness of CNN in the field of computer vision has been proven. As a result, utilizing CNN to word embedding matrices and automatically extract features to handle NLP tasks appeared inevitable.</p>
<p>The following figure <a href="introduction-deep-learning-for-nlp.html#fig:figintro1">3.4</a> illustrates a basic structure of CNN, which is composed of multiple layers. Many of these layers are described and developed with some technical detail in later chapters of this paper.</p>
<div class="figure"><span id="fig:figintro1"></span>
<img src="figures/01-00-deep-learning-for-nlp/01_03_basic_structure.png" alt="Basic structure of CNN. Source: Own figure." width="105%" />
<p class="caption">
FIGURE 3.4: Basic structure of CNN. Source: Own figure.
</p>
</div>
<p>It is obvious that neural networks consist of a group of multiple neurons (or perceptron) at each layer, which uses to simulate the structure and behavior of biological nervous systems, and each neuron can be considered as logistic regression.</p>
<div class="figure" style="text-align: center"><span id="fig:figintro2"></span>
<img src="figures/01-00-deep-learning-for-nlp/01_03_Comparison_Fully_Partial.png" alt="Comparison between the fully-connected and partial connected architecture. Source: Own figure." width="50%" />
<p class="caption">
FIGURE 3.5: Comparison between the fully-connected and partial connected architecture. Source: Own figure.
</p>
</div>
<p>The structure of CNN is different compared with traditional neural networks as illustrated in figure <a href="introduction-deep-learning-for-nlp.html#fig:figintro2">3.5</a>. In traditional neural networks structure, the connections between neurons are fully connected. To be more specific, all of the neurons in the layer <span class="math inline">\(m-1\)</span> are connected to each neuron in the layer <span class="math inline">\(m\)</span>, but CNN sets up spatially-local correlation by performing a local connectivity pattern between neurons of neighboring layers, which means that the neurons in the layer <span class="math inline">\(m-1\)</span> are partially connected to the neurons in the layer <span class="math inline">\(m\)</span>. In addition to this, the left picture presents a schematic diagram of fully-connected architecture. It can be seen from the figure that there are many edges between neurons in the previous layer to the next layer, and each edge has parameters. The right side is a local connection, which shows that there are relatively few edges compared with fully-connected architecture and the number of visible parameters has significantly decreased.</p>
<div style="page-break-after: always;"></div>
<p>A detailed description of CNN will be presented in the later chapters and the basic architecture of if will be further explored. Subsection 5.1 gives an overview of CNN model depends upon (<span class="citation">Kim (<a href="#ref-Kim2014ConvolutionalNN">2014</a>)</span>). At its foundation, it is also necessary to explain various connected layers, including the convolutional layer, pooling layer, and so on. In 5.2 and later subsections, some practical applications of CNN in the field of NLP will be further explored, and these applications are based on different CNN architecture at diverse level, for example, exploring the model performance at character-level on text classification research (see <span class="citation">Zhang, Zhao, and LeCun (<a href="#ref-Zhang2015CharacterlevelCN">2015</a>)</span>) and based on multiple data sets to detect the Very Deep Convolutional Networks (VD-CNN) for text classification (see <span class="citation">Schwenk et al. (<a href="#ref-Schwenk2017VeryDC">2017</a>)</span>).</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bengio.2003">
<p>Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. “A Neural Probabilistic Language Model.” <em>Journal of Machine Learning Research</em>, no. 3: 1137–55.</p>
</div>
<div id="ref-boden2002guide">
<p>Boden, Mikael. 2002. “A Guide to Recurrent Neural Networks and Backpropagation.” <em>The Dallas Project</em>.</p>
</div>
<div id="ref-cho2014learning">
<p>Cho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine Translation.” <em>arXiv Preprint arXiv:1406.1078</em>.</p>
</div>
<div id="ref-chollet2018deep">
<p>Chollet, Francois. 2018. <em>Deep Learning Mit Python Und Keras: Das Praxis-Handbuch Vom Entwickler Der Keras-Bibliothek</em>. MITP-Verlags GmbH &amp; Co. KG.</p>
</div>
<div id="ref-goodfellow2016deep">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div id="ref-Harris.1954">
<p>Harris, Zellig S. 1954. “Distributional Structure.” <em>WORD</em> 10 (2-3): 146–62.</p>
</div>
<div id="ref-hochreiter1997long">
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8). MIT Press: 1735–80.</p>
</div>
<div id="ref-Kim2014ConvolutionalNN">
<p>Kim, Yoon. 2014. <em>Convolutional Neural Networks for Sentence Classification</em>.</p>
</div>
<div id="ref-Krizhevsky2012ImageNetCW">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. <em>ImageNet Classification with Deep Convolutional Neural Networks</em>.</p>
</div>
<div id="ref-Mikolov.2013c">
<p>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” <em>Advances in Neural Information Processing Systems</em>, 3111–9.</p>
</div>
<div id="ref-Pennington.2014">
<p>Pennington, Jeffrey, Richard Socher, Manning, and Christopher D. 2014. “GloVe: Global Vectors for Word Representation.” <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43.</p>
</div>
<div id="ref-schuster1997bidirectional">
<p>Schuster, Mike, and Kuldip K Paliwal. 1997. “Bidirectional Recurrent Neural Networks.” <em>IEEE Transactions on Signal Processing</em> 45 (11). Ieee: 2673–81.</p>
</div>
<div id="ref-Schwenk2017VeryDC">
<p>Schwenk, Holger, Loïc Barrault, Alexis Conneau, and Yann LeCun. 2017. <em>Very Deep Convolutional Networks for Text Classification</em>.</p>
</div>
<div id="ref-sutskever2014sequence">
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3104–12.</p>
</div>
<div id="ref-Visin2015ReNetAR">
<p>Visin, Francesco, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron C. Courville, and Yoshua Bengio. 2015. <em>ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks</em>. <em>ArXiv</em>. Vol. abs/1505.00393.</p>
</div>
<div id="ref-Zhang2015CharacterlevelCN">
<p>Zhang, Xiang, Junbo Jake Zhao, and Yann LeCun. 2015. <em>Character-Level Convolutional Networks for Text Classification</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="foundationsapplications-of-modern-nlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/01-00-deep-learning-for-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
