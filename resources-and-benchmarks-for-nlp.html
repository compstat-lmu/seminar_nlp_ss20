<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-08-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-resources-for-nlp.html"/>
<link rel="next" href="software-for-nlp-the-huggingface-transformers-module.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#model-improvements"><i class="fa fa-check"></i><b>3.3</b> Model Improvements</a><ul>
<li class="chapter" data-level="3.3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#fasttext"><i class="fa fa-check"></i><b>3.3.1</b> fastText</a></li>
<li class="chapter" data-level="3.3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word-phrases"><i class="fa fa-check"></i><b>3.3.2</b> Word Phrases</a></li>
<li class="chapter" data-level="3.3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#multiple-meanings-per-word"><i class="fa fa-check"></i><b>3.3.3</b> Multiple Meanings per Word</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>3.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.5</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.6" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#sources-and-applications-of-word-embeddings"><i class="fa fa-check"></i><b>3.6</b> Sources and Applications of Word Embeddings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extentions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extentions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#bidirectional-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Bidirectional RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.2</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.3</b> Encoder-Decoder Architecture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>6</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>6.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>6.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>6.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>7</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#transfer-learning-in-nlp"><i class="fa fa-check"></i><b>7.2</b> Transfer Learning in NLP</a></li>
<li class="chapter" data-level="7.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#steps-in-sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>7.3</b> Steps in sequential inductive transfer learning</a></li>
<li class="chapter" data-level="7.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#most-popular-models"><i class="fa fa-check"></i><b>7.4</b> Most popular models</a><ul>
<li class="chapter" data-level="7.4.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>7.4.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="7.4.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit---cutting-edge-model-using-lstms"><i class="fa fa-check"></i><b>7.4.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="7.4.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>7.4.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>8</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="8.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>8.1</b> Attention</a><ul>
<li class="chapter" data-level="8.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>8.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>8.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="8.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>8.1.3</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>8.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="8.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers"><i class="fa fa-check"></i><b>8.2.1</b> Transformers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>9</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="9.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>9.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>9.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="9.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>9.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="9.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>9.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="9.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>9.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="9.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-tasks"><i class="fa fa-check"></i><b>9.1.5</b> Next-sentence Tasks</a></li>
<li class="chapter" data-level="9.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>9.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>9.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>9.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>9.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="9.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>9.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="9.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>9.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="9.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>9.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="9.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>9.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="9.2.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion"><i class="fa fa-check"></i><b>9.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>9.3</b> XLNet</a><ul>
<li class="chapter" data-level="9.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>9.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="9.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>9.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="9.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>9.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="9.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>9.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="9.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>9.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="9.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>9.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="9.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>9.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>9.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>10</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="11" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="11.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>11.1</b> Metrics</a></li>
<li class="chapter" data-level="11.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>11.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="11.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>11.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="11.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>11.2.2</b> CoQA</a></li>
<li class="chapter" data-level="11.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>11.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="11.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>11.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="11.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>11.2.5</b> SNLI</a></li>
<li class="chapter" data-level="11.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#lambada"><i class="fa fa-check"></i><b>11.2.6</b> LAMBADA</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-training-resources"><i class="fa fa-check"></i><b>11.3</b> Pre-Training Resources</a></li>
<li class="chapter" data-level="11.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>11.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="software-for-nlp-the-huggingface-transformers-module.html"><a href="software-for-nlp-the-huggingface-transformers-module.html"><i class="fa fa-check"></i><b>12</b> Software for NLP: The huggingface transformers module</a></li>
<li class="chapter" data-level="13" data-path="use-cases-for-nlp.html"><a href="use-cases-for-nlp.html"><i class="fa fa-check"></i><b>13</b> Use-Cases for NLP</a></li>
<li class="chapter" data-level="14" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>14</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="14.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>14.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="14.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>14.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="14.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>14.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="14.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>14.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="14.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>14.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="14.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>14.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="14.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>14.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#question-answer-systems"><i class="fa fa-check"></i><b>14.4</b> Question-Answer Systems</a><ul>
<li class="chapter" data-level="14.4.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>14.4.1</b> Datasets</a></li>
<li class="chapter" data-level="14.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>14.4.2</b> Types</a></li>
<li class="chapter" data-level="14.4.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>14.4.3</b> Architectures</a></li>
<li class="chapter" data-level="14.4.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#evaluation-metrics"><i class="fa fa-check"></i><b>14.4.4</b> Evaluation Metrics</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>14.5</b> Dialog Systems</a><ul>
<li class="chapter" data-level="14.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types-1"><i class="fa fa-check"></i><b>14.5.1</b> Types</a></li>
<li class="chapter" data-level="14.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures-1"><i class="fa fa-check"></i><b>14.5.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="resources-and-benchmarks-for-nlp" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Resources and Benchmarks for NLP</h1>
<p><em>Authors: Nico Hahn</em></p>
<p><em>Supervisor: Daniel Schalk</em></p>
<p>Frameworks such as TensorFlow or Keras allow users to train a wide range of different models for different tasks. Let us assume that two models for a simple question-answer system are trained, one with attention and one without attention. How can these models be evaluated in order to find the model better suited to the task? Quite simply, through benchmarking. This section looks at some of the most commonly used benchmarking datasets and at pre-training resources.</p>
<div id="metrics" class="section level2">
<h2><span class="header-section-number">11.1</span> Metrics</h2>
<p>For many of the benchmarking datasets in natural language processing, a leaderboard exists in which different models are compared with each other. Depending on the task, the models are evaluated with different metrics. In this section we will introduce those used for the benchmarking datasets presented later.</p>
<p><strong>Exact match (EM):</strong> The percentage of predictions that match any one of the answers exactly.</p>
<p><strong>(Macro-averaged) F1 score (F1):</strong> Each answer and prediction is tokenized into words. For every answer to a given question, the overlap between the prediction and each answer is calculated and the maximum F1 is chosen. This score is then averaged over all of the questions. Formally speaking:
<span class="math display">\[
F1 = \frac{2 \cdot \hbox{precision}\cdot\hbox{recall}}{\hbox{precision}+\hbox{recall}} 
\]</span>
<span class="math display">\[
\hbox{precision} = \frac{\hbox{number of same tokens}}{\hbox{length(predicted tokens)}} \\
\]</span>
<span class="math display">\[
\hbox{recall} = \frac{\hbox{number of same tokens}}{\hbox{length(labeled tokens)}} \\
\]</span></p>
<p><strong>Perplexity:</strong> Perplexity is a measurement of how well a probability model predicts a sample. A low perplexity indicates the probability distribution is good at predicting the sample. In NLP, perplexity is a way of evaluating language models. A model of an unknown probability distribution <span class="math inline">\(p\)</span>, may be proposed based on a training sample that was drawn from <span class="math inline">\(p\)</span>. Given a proposed probability model <span class="math inline">\(q\)</span>, one may evaluate <span class="math inline">\(q\)</span> by asking how well it predicts a separate test sample <span class="math inline">\(x_1, x_2, ..., x_N\)</span> also drawn from <span class="math inline">\(p\)</span>. The perplexity of the model <span class="math inline">\(q\)</span> is defined as
<span class="math display">\[ b^{-\frac{1}{N}\sum_{i=1}^N\log_bq(x_i)} \]</span>
where <span class="math inline">\(b\)</span> is customarily <span class="math inline">\(2\)</span>.</p>
<p><strong>BLEU:</strong> BLEU (<strong>B</strong>i<strong>l</strong>ingual <strong>E</strong>valuation <strong>U</strong>nderstudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation’s overall quality. Intelligibility or grammatical correctness are not taken into account. <span class="citation">(Papineni et al. <a href="#ref-papineni2002bleu">2002</a>)</span></p>
<p><strong>Accuracy:</strong> Accuracy is the ratio of number of correct predictions to the total number of input samples.</p>
</div>
<div id="benchmark-datasets" class="section level2">
<h2><span class="header-section-number">11.2</span> Benchmark Datasets</h2>
<div id="squad" class="section level3">
<h3><span class="header-section-number">11.2.1</span> SQuAD</h3>
<p>The first Version of the <strong>S</strong>tanford <strong>Qu</strong>estion <strong>A</strong>nswering <strong>D</strong>ataset was released in 2016. The dataset was created with the aim of advancing the field of reading comprehension. Reading text and answering questions about it is a demanding task for machines and requires large data sets of high quality. Most of the datasets before the release of the first version of SQuAD were either of high quality or of large size, but not both.</p>
<p>With the help of crowdworkers, 107.785 question-answer pairs were created for 536 Wikipedia articles. For each question, the answer is a segment of text, or span, from the corresponding reading passage.
Pairs were collected in a two-step process. In the first step the crowdworkers were asked to generate five questions and their answers per paragraph.</p>
<p>In the second step, each crowdworker was shown only the questions along with the paragraphs of the corresponding article and was asked to choose the shortest span in the paragraph that answered the question. As a result of this process, questions in the dev-set multiple answers.</p>
<p>The goal of this procedure was to get a more robust evaluation and to obtain an indicator of human performance on SQuAD.</p>
<p>One shortcoming of reading comprehension systems is that they tend to make unreliable guesses on questions to which no correct answer is possible. With this in mind, the second version of SQuAD was released in 2018. In addition to the approximately 100.000 questions from the first version, 53.775 new, unanswerable questions on the same paragraphs are contained in this dataset.</p>
<p>The accuracy of models trained on SQuAD is evaluated using two different metrics, exact match and (Macro-averaged) F1 score, both ignoring punctuation and articles.</p>
<p>To evaluate human performance, the second answer to each question is treated as the human prediction.<span class="citation">(Rajpurkar et al. <a href="#ref-rajpurkar2016squad">2016</a>; Rajpurkar, Jia, and Liang <a href="#ref-rajpurkar2018know">2018</a>)</span></p>
<p>Humans achieve an <strong>EM</strong> score of 86.831 and a <strong>F1</strong> score of 89.452.</p>
<p>Currently, the best performing model achieves an <strong>EM</strong> score of 90.386 and a <strong>F1</strong> score of 92.777.</p>
<p>Examples of SQuAD and the leaderboard and can be viewed here:</p>
<p><a href="https://rajpurkar.github.io/SQuAD-explorer/" class="uri">https://rajpurkar.github.io/SQuAD-explorer/</a></p>

</div>
<div id="coqa" class="section level3">
<h3><span class="header-section-number">11.2.2</span> CoQA</h3>
<p>CoQA is a dataset for building <strong>Co</strong>nversational <strong>Q</strong>uestion <strong>A</strong>nswering systems. Humans are capable of gathering information through conversations that include several interrelated questions and answers. The aim of CoQA is to enable machines to answers conversational questions.</p>
<p>The data set is made up of 127k Q/A pairs, covering seven different domains such as Children’s Stories or Reddit. Five of these domains are used for in-domain evaluation, meaning models have already seen questions from these domains, and two are used for out-of-domain evaluation., meaning models have not seen any questions from these domains. To create the Q/A pairs, two people received a text passage, with one person asking the other person questions about the text and the other person answering. Using multiple annotators has a few advantages:</p>
<ol style="list-style-type: decimal">
<li>A natural flow of conversation is created.</li>
<li>If one person gives an incorrect answer or a vague questions is asked, the other person can raise a flag. Thus bad annotators can easily be identified.</li>
<li>If there is a disagreement, the two annotators can discuss it via a chat window.</li>
</ol>
<p>Similar to SQuAD, three additional answers are collected for each question. However, since the answers influence the flow of the conversation, the next question always depends on the answer to the previous question. For this reason, two different answers to the same question can lead to two different follow-up questions. In order to avoid incoherent discussions, annotators are shown a question that they must answer first. After answering, they are shown the original answer, and they must then confirm that their answer has an identical meaning.</p>
<p>Compared to SQuAD 2.0, there is a greater variety of question types in CoQA. While almost half of the questions in the SQuAD start with <em>what</em>, less than a quarter of the questions in the CoQA begin with this token. Another major difference is that questions in CoQA are on average 5.5 words long, compared to an average length of 10.1 in SQuAD. It is also worth mentioning that about 10% of the answers in CoQA are either yes or no, whereas there are no such answers in SQuAD.</p>
<p>Like SQuAD, trained models are evaluated using a macro-average F1 score. Models are evaluated separately on the in-domain dataset and the out-of-domain dataset. <span class="citation">(Reddy, Chen, and Manning <a href="#ref-coqa2019">2018</a>)</span></p>
<p>Humans achieve a <strong>F1</strong> score of 89.4 for in-domain and a <strong>F1</strong> score of 87.4 for out-of-domain.</p>
<p>Currently, the best performing model achieves a <strong>F1</strong> score of 91.4 for in-domain and a <strong>F1</strong> score of 89.2 for out-of-domain.</p>
<p>Examples of CoQA and the leaderboard and can be viewed here:</p>
<p><a href="https://stanfordnlp.github.io/coqa/" class="uri">https://stanfordnlp.github.io/coqa/</a></p>

</div>
<div id="superglue" class="section level3">
<h3><span class="header-section-number">11.2.3</span> (Super)GLUE</h3>
<p>Most models in NLP are designed to solve a specific task, such as answering questions from a particular domain. This limits the use of models for understanding natural language. In order to process language in a way that is not limited to a specific task, genre, or dataset, models should be able to solve a variety of tasks well.</p>
<p>The <strong>G</strong>eneral <strong>L</strong>anguage <strong>U</strong>nderstanding <strong>E</strong>valuation benchmark dataset is a collection of tools created with this in mind. It is designed to encourage and favour models that share common linguistic knowledge across tasks. These tasks include textual entailment, sentiment analysis and question answering. Some tasks come with a lot of training data, others with less. Common to all datasets is that they were not created specifically for GLUE, but are existing datasets. Models that are evaluated on GLUE only need to have the ability to process single-sentence and sentence-pair inputs and make appropriate predictions.
This test suite contains a total of nine sentence or sentence-pair NLU tasks, built on established annotated datasets.
The models are scored separately for each task and then a macro-average of these scores is calculated to determine a system’s position on the ranking. If a task has multiple metrics, an unweighted average of these metrics is used as the score for the task when calculating the overall macro average. <span class="citation">(Wang et al. <a href="#ref-wang2018glue">2018</a>)</span>
The human baseline score is 87.1, while the best model score is currently 90.6.</p>
<p>Roughly one year after the release of GLUE, models surpassed human performance. In response to this, a new benchmark, SuperGLUE, was introduced. It follows the same principles as GLUE, however the tasks included are more challenging. The two hardest tasks in GLUE remain, the rest were selected based on difficulty for current NLP approaches. <span class="citation">(Wang et al. <a href="#ref-wang2019superglue">2019</a>)</span>
For SuperGLUE, the human baseline score is 89.8, which is above the best model score, presently 89.3.</p>
<p>More information about the tasks and the leaderboard for both GLUE and SuperGLUE is available here:</p>
<p><a href="https://super.gluebenchmark.com/" class="uri">https://super.gluebenchmark.com/</a></p>

</div>
<div id="aqua-rat" class="section level3">
<h3><span class="header-section-number">11.2.4</span> AQuA-Rat</h3>
<p>One task that most people know from their time at school is solving algebraic word problems. For humans, this task can be easy, depending on a person’s mathematical abilities, since we only have to perform a series of arithmetic operations. However, since programs can be endlessly complicated, it is a considerable challenge to induce them directly from question-answer pairs. The <strong>A</strong>lgebra <strong>Qu</strong>estion <strong>A</strong>nswering with <strong>Rat</strong>ionales dataset attempts to make this task more feasible for machines by providing not only the correct answer but also step-by-step instructions for deriving the correct answer, the so-called rationale. Models trained on AQuA-Rat must not only predict the correct answer, but also the rationale.</p>
<p>The dataset contains over 100.000 questions, and each question has five different options as to what the correct answer is. It also contains the answer rationale and the correct option. The problems cover a wide range of topics, for instance probability theory or calculus, with a variety of difficulty levels. To create the dataset, examples of exams such as the GMAT (Graduate Management Admission Test) and GRE (General Test) were taken from the Internet. This part of the dataset is called the seed dataset.
Besides, crowdsourcing was used to generate further questions. For this users were presented with five questions from the seed dataset and asked to select one of these questions and write a similiar question. Users were also forced to rephrase the rationales and answers to avoid paraphrasing the original questions.
These created questions were then passed to another user for quality control.</p>
<p>The rationales are evaluated using average sentence level perplexity and the BLEU score. If a model is unable to generate a token for perplexity computation, an unknown token is predicted. The correctness of the answers is evaluated by calculating the accuracy of the predictions.</p>
<p>This is a relatively new dataset and as of now there is no online leaderboard for it. The authors of the original paper used an attention-based sequence to sequence model as their baseline method. The authors generated a program containing both instructions that generate output and instructions that simply generate intermediate values used by following instructions. The program uses a latent predictor network which generates an output sequence conditioned on an arbitrary number of input functions and staged back-progagation to save memory. Going into further depth about this program would be beyond this book so I’d advise to have a look at the original paper.
The program outperformed the baseline model and achieved a perplexity of 28.5, a BLEU score of 27.2 and has and accuracy of 36.4. <span class="citation">(Ling et al. <a href="#ref-ling2017program">2017</a>)</span></p>
<p>The paper and examples of the dataset can be found here:
<a href="https://github.com/deepmind/AQuA" class="uri">https://github.com/deepmind/AQuA</a></p>

</div>
<div id="snli" class="section level3">
<h3><span class="header-section-number">11.2.5</span> SNLI</h3>
<p>Understanding entailment and contradiction is fundamental to understanding natural language, and conclusions about entailment and contradiction are a valuable testing ground for the development of semantic representations. The <strong>S</strong>tanford <strong>N</strong>atural <strong>L</strong>anguage <strong>I</strong>nference Corpus is a collection of sentence pairs that are labeled either as entailment, contradiction or semantic independence. While other datasets exist that try to tackle this specific task, they all have issues of size, quality, and indeterminacy.</p>
<p>SNLI consists of about 570k record pairs. Again, croworkers were used to create the data set. To do this, they were shown the caption of a photo but not the photo, and they were asked to write three alternative captions: One that is definitely a true description of the photo, one that could be a true description of the photo, and one caption that is definitely a false description of the photo. By not showing the photo, the authors wanted to ensure that each pair of sentences could be reconstructed based on the available text alone.
To measure the quality of the corpus, about 10% of all created sentence pairs were validated. For this purpose, each crowdorker was shown five sentences and asked to mark them with one of the three labels. Each sentence was shown to a total of five crowdorkers. For each pair, a gold label was assigned if at least three of the five annotators chose the same label. Around 98% of all sentence pairs were assigned a gold label, the rest was assigned a placeholder label.<span class="citation">(Bowman et al. <a href="#ref-bowman2015large">2015</a>)</span></p>
<p>Models are once again evaluated with the accuracy of the predicted label. For the SNLI corpus there is no measurement for human performance. Currently, the best performing model is a semantics-aware BERT (SemBERT) with an accuracy of 91.9.
- Understanding entailment and contradiction
- <a href="https://nlp.stanford.edu/projects/snli/" class="uri">https://nlp.stanford.edu/projects/snli/</a></p>
</div>
<div id="lambada" class="section level3">
<h3><span class="header-section-number">11.2.6</span> LAMBADA</h3>
<ul>
<li><strong>LA</strong>nguage <strong>M</strong>odeling <strong>B</strong>roadened to <strong>A</strong>ccount for <strong>D</strong>iscourse <strong>A</strong>spects</li>
<li>Word prediction task
<ul>
<li>Predict the last word of a sentence depending on the context</li>
</ul></li>
<li>Models can’t rely on local context, must be able to keep track of information in the broader discourse</li>
<li><a href="https://www.aclweb.org/anthology/P16-1144.pdf" class="uri">https://www.aclweb.org/anthology/P16-1144.pdf</a></li>
</ul>
</div>
</div>
<div id="pre-training-resources" class="section level2">
<h2><span class="header-section-number">11.3</span> Pre-Training Resources</h2>
<ul>
<li>BERT/ALBERT/CamemBERT
<ul>
<li>Trained on unlabeled data over different pre-training tasks</li>
<li>WordPiece embeddings with a 30k token vocabulary</li>
</ul></li>
<li>OpenAI GPT2</li>
<li>GPT-3</li>
<li>Google 5T</li>
</ul>
</div>
<div id="resources-for-resources" class="section level2">
<h2><span class="header-section-number">11.4</span> Resources for Resources</h2>
<p><a href="https://paperswithcode.com/area/natural-language-processing" class="uri">https://paperswithcode.com/area/natural-language-processing</a>
<a href="https://datasets.quantumstat.com/" class="uri">https://datasets.quantumstat.com/</a></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bowman2015large">
<p>Bowman, Samuel R, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. “A Large Annotated Corpus for Learning Natural Language Inference.” <em>arXiv Preprint arXiv:1508.05326</em>.</p>
</div>
<div id="ref-ling2017program">
<p>Ling, Wang, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. “Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.” <em>arXiv Preprint arXiv:1705.04146</em>.</p>
</div>
<div id="ref-papineni2002bleu">
<p>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “BLEU: A Method for Automatic Evaluation of Machine Translation.” In <em>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, 311–18. Association for Computational Linguistics.</p>
</div>
<div id="ref-rajpurkar2018know">
<p>Rajpurkar, Pranav, Robin Jia, and Percy Liang. 2018. “Know What You Don’t Know: Unanswerable Questions for Squad.”</p>
</div>
<div id="ref-rajpurkar2016squad">
<p>Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.”</p>
</div>
<div id="ref-coqa2019">
<p>Reddy, Siva, Danqi Chen, and Christopher D. Manning. 2018. “CoQA: A Conversational Question Answering Challenge.” <em>CoRR</em> abs/1808.07042. <a href="http://arxiv.org/abs/1808.07042" class="uri">http://arxiv.org/abs/1808.07042</a>.</p>
</div>
<div id="ref-wang2019superglue">
<p>Wang, Alex, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. “Superglue: A Stickier Benchmark for General-Purpose Language Understanding Systems.” In <em>Advances in Neural Information Processing Systems</em>, 3261–75.</p>
</div>
<div id="ref-wang2018glue">
<p>Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. “Glue: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” <em>arXiv Preprint arXiv:1804.07461</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-resources-for-nlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="software-for-nlp-the-huggingface-transformers-module.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/03-01-resources-and-benchmarks-for-nlp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
