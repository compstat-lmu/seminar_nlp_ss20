<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Modern Approaches in Natural Language Processing</title>
  <meta name="description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Modern Approaches in Natural Language Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Modern Approaches in Natural Language Processing" />
  
  <meta name="twitter:description" content="In this seminar, we are planning to review modern NLP frameworks starting with a methodology that can be seen as the beginning of modern NLP: Word Embeddings." />
  

<meta name="author" content="" />


<meta name="date" content="2020-08-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="acknowledgements.html"/>

<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modern Approaches in Natural Language Processing</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html"><i class="fa fa-check"></i><b>2</b> Introduction: Deep Learning for NLP</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#word-embeddings-and-neural-network-language-models"><i class="fa fa-check"></i><b>2.1</b> Word Embeddings and Neural Network Language Models</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>2.2</b> Recurrent Neural Networks</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-deep-learning-for-nlp.html"><a href="introduction-deep-learning-for-nlp.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>2.3</b> Convolutional Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html"><i class="fa fa-check"></i><b>3</b> Foundations/Applications of Modern NLP</a><ul>
<li class="chapter" data-level="3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#the-evolution-of-word-embeddings"><i class="fa fa-check"></i><b>3.1</b> The Evolution of Word Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#methods-to-obtain-word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Methods to Obtain Word Embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#feedforward-neural-network-language-model-nnlm"><i class="fa fa-check"></i><b>3.2.1</b> Feedforward Neural Network Language Model (NNLM)</a></li>
<li class="chapter" data-level="3.2.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word2vec"><i class="fa fa-check"></i><b>3.2.2</b> Word2Vec</a></li>
<li class="chapter" data-level="3.2.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#glove"><i class="fa fa-check"></i><b>3.2.3</b> GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#model-improvements"><i class="fa fa-check"></i><b>3.3</b> Model Improvements</a><ul>
<li class="chapter" data-level="3.3.1" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#fasttext"><i class="fa fa-check"></i><b>3.3.1</b> fastText</a></li>
<li class="chapter" data-level="3.3.2" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#word-phrases"><i class="fa fa-check"></i><b>3.3.2</b> Word Phrases</a></li>
<li class="chapter" data-level="3.3.3" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#multiple-meanings-per-word"><i class="fa fa-check"></i><b>3.3.3</b> Multiple Meanings per Word</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>3.4</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="3.5" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#evaluation-methods"><i class="fa fa-check"></i><b>3.5</b> Evaluation Methods</a></li>
<li class="chapter" data-level="3.6" data-path="foundationsapplications-of-modern-nlp.html"><a href="foundationsapplications-of-modern-nlp.html#sources-and-applications-of-word-embeddings"><i class="fa fa-check"></i><b>3.6</b> Sources and Applications of Word Embeddings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>4</b> Recurrent neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="4.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#structure-and-training-of-simple-rnns"><i class="fa fa-check"></i><b>4.1</b> Structure and Training of Simple RNNs</a><ul>
<li class="chapter" data-level="4.1.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#network-structure-and-forwardpropagation"><i class="fa fa-check"></i><b>4.1.1</b> Network Structure and Forwardpropagation</a></li>
<li class="chapter" data-level="4.1.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#backpropagation"><i class="fa fa-check"></i><b>4.1.2</b> Backpropagation</a></li>
<li class="chapter" data-level="4.1.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#vanishing-and-exploding-gradients"><i class="fa fa-check"></i><b>4.1.3</b> Vanishing and Exploding Gradients</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gated-rnns"><i class="fa fa-check"></i><b>4.2</b> Gated RNNs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#lstm"><i class="fa fa-check"></i><b>4.2.1</b> LSTM</a></li>
<li class="chapter" data-level="4.2.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#gru"><i class="fa fa-check"></i><b>4.2.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#extentions-of-simple-rnns"><i class="fa fa-check"></i><b>4.3</b> Extentions of Simple RNNs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#bidirectional-rnns"><i class="fa fa-check"></i><b>4.3.1</b> Bidirectional RNNs</a></li>
<li class="chapter" data-level="4.3.2" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#deep-rnns"><i class="fa fa-check"></i><b>4.3.2</b> Deep RNNs</a></li>
<li class="chapter" data-level="4.3.3" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>4.3.3</b> Encoder-Decoder Architecture</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html"><i class="fa fa-check"></i><b>5</b> Convolutional neural networks and their applications in NLP</a><ul>
<li class="chapter" data-level="5.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#introduction-to-basic-architecture-of-cnn"><i class="fa fa-check"></i><b>5.1</b> Introduction to Basic Architecture of CNN</a><ul>
<li class="chapter" data-level="5.1.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#convolutional-layer"><i class="fa fa-check"></i><b>5.1.1</b> Convolutional Layer</a></li>
<li class="chapter" data-level="5.1.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#relu-layer"><i class="fa fa-check"></i><b>5.1.2</b> ReLU layer</a></li>
<li class="chapter" data-level="5.1.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#pooling-layer"><i class="fa fa-check"></i><b>5.1.3</b> Pooling layer</a></li>
<li class="chapter" data-level="5.1.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#fully-connected-layer"><i class="fa fa-check"></i><b>5.1.4</b> Fully-connected layer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-for-sentence-classification"><i class="fa fa-check"></i><b>5.2</b> CNN for sentence classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#cnn-randcnn-staticcnn-non-staticcnn-multichannel"><i class="fa fa-check"></i><b>5.2.1</b> CNN-rand/CNN-static/CNN-non-static/CNN-multichannel</a></li>
<li class="chapter" data-level="5.2.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#character-level-convnets"><i class="fa fa-check"></i><b>5.2.2</b> Character-level ConvNets</a></li>
<li class="chapter" data-level="5.2.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#very-deep-cnn"><i class="fa fa-check"></i><b>5.2.3</b> Very Deep CNN</a></li>
<li class="chapter" data-level="5.2.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#deep-pyramid-cnn"><i class="fa fa-check"></i><b>5.2.4</b> Deep Pyramid CNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets-and-experimental-evaluation"><i class="fa fa-check"></i><b>5.3</b> Datasets and Experimental Evaluation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>5.3.1</b> Datasets</a></li>
<li class="chapter" data-level="5.3.2" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#experimental-evaluation"><i class="fa fa-check"></i><b>5.3.2</b> Experimental Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>5.4</b> Conclusion and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html"><i class="fa fa-check"></i><b>6</b> Introduction: Transfer Learning for NLP</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>6.1</b> What is Transfer Learning?</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>6.2</b> (Self-)attention</a></li>
<li class="chapter" data-level="6.3" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#overview-over-important-nlp-models"><i class="fa fa-check"></i><b>6.3</b> Overview over important NLP models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html"><i class="fa fa-check"></i><b>7</b> Transfer Learning for NLP I</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#transfer-learning-in-nlp"><i class="fa fa-check"></i><b>7.2</b> Transfer Learning in NLP</a></li>
<li class="chapter" data-level="7.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#steps-in-sequential-inductive-transfer-learning"><i class="fa fa-check"></i><b>7.3</b> Steps in sequential inductive transfer learning</a></li>
<li class="chapter" data-level="7.4" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#most-popular-models"><i class="fa fa-check"></i><b>7.4</b> Most popular models</a><ul>
<li class="chapter" data-level="7.4.1" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#elmo---the-new-age-of-embeddings"><i class="fa fa-check"></i><b>7.4.1</b> ELMo - The “new age” of embeddings</a></li>
<li class="chapter" data-level="7.4.2" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#ulmfit---cutting-edge-model-using-lstms"><i class="fa fa-check"></i><b>7.4.2</b> ULMFiT - cutting-edge model using LSTMs</a></li>
<li class="chapter" data-level="7.4.3" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#gpt---first-step-towards-transformers"><i class="fa fa-check"></i><b>7.4.3</b> GPT - First step towards transformers</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="transfer-learning-for-nlp-i.html"><a href="transfer-learning-for-nlp-i.html#summary"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html"><i class="fa fa-check"></i><b>8</b> Attention and Self-Attention for NLP</a><ul>
<li class="chapter" data-level="8.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention"><i class="fa fa-check"></i><b>8.1</b> Attention</a><ul>
<li class="chapter" data-level="8.1.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#bahdanau-attention"><i class="fa fa-check"></i><b>8.1.1</b> Bahdanau-Attention</a></li>
<li class="chapter" data-level="8.1.2" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#luong-attention"><i class="fa fa-check"></i><b>8.1.2</b> Luong-Attention</a></li>
<li class="chapter" data-level="8.1.3" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#attention-models"><i class="fa fa-check"></i><b>8.1.3</b> Attention Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="introduction-transfer-learning-for-nlp.html"><a href="introduction-transfer-learning-for-nlp.html#self-attention"><i class="fa fa-check"></i><b>8.2</b> Self-Attention</a><ul>
<li class="chapter" data-level="8.2.1" data-path="attention-and-self-attention-for-nlp.html"><a href="attention-and-self-attention-for-nlp.html#transformers"><i class="fa fa-check"></i><b>8.2.1</b> Transformers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html"><i class="fa fa-check"></i><b>9</b> Transfer Learning for NLP II</a><ul>
<li class="chapter" data-level="9.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bidirectional-encoder-representations-from-transformers-bert"><i class="fa fa-check"></i><b>9.1</b> Bidirectional Encoder Representations from Transformers (BERT)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#autoencoding"><i class="fa fa-check"></i><b>9.1.1</b> Autoencoding</a></li>
<li class="chapter" data-level="9.1.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-bert"><i class="fa fa-check"></i><b>9.1.2</b> Introduction of BERT</a></li>
<li class="chapter" data-level="9.1.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-bert"><i class="fa fa-check"></i><b>9.1.3</b> Input Representation of BERT</a></li>
<li class="chapter" data-level="9.1.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#masked-language-model"><i class="fa fa-check"></i><b>9.1.4</b> Masked Language Model</a></li>
<li class="chapter" data-level="9.1.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#next-sentence-tasks"><i class="fa fa-check"></i><b>9.1.5</b> Next-sentence Tasks</a></li>
<li class="chapter" data-level="9.1.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#pre-training-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.6</b> Pre-training Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#fine-tuning-procedure-of-bert"><i class="fa fa-check"></i><b>9.1.7</b> Fine-tuning Procedure of BERT</a></li>
<li class="chapter" data-level="9.1.8" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#feature-extraction"><i class="fa fa-check"></i><b>9.1.8</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.1.9" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#bert-like-models"><i class="fa fa-check"></i><b>9.1.9</b> BERT-like models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#generative-pre-traininggpt-2"><i class="fa fa-check"></i><b>9.2</b> Generative Pre-Training(GPT-2)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#auto-regressive-language-modelar"><i class="fa fa-check"></i><b>9.2.1</b> Auto-regressive Language Model(AR)</a></li>
<li class="chapter" data-level="9.2.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-gpt-2"><i class="fa fa-check"></i><b>9.2.2</b> Introduction of GPT-2</a></li>
<li class="chapter" data-level="9.2.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#input-representation-of-gpt-2"><i class="fa fa-check"></i><b>9.2.3</b> Input Representation of GPT-2</a></li>
<li class="chapter" data-level="9.2.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-decoder-only-block"><i class="fa fa-check"></i><b>9.2.4</b> The Decoder-Only Block</a></li>
<li class="chapter" data-level="9.2.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#gpt-2-models"><i class="fa fa-check"></i><b>9.2.5</b> GPT-2 Models</a></li>
<li class="chapter" data-level="9.2.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion"><i class="fa fa-check"></i><b>9.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet"><i class="fa fa-check"></i><b>9.3</b> XLNet</a><ul>
<li class="chapter" data-level="9.3.1" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#introduction-of-xlnet"><i class="fa fa-check"></i><b>9.3.1</b> Introduction of XLNet</a></li>
<li class="chapter" data-level="9.3.2" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#permutation-language-modelingplm"><i class="fa fa-check"></i><b>9.3.2</b> Permutation Language Modeling(PLM)</a></li>
<li class="chapter" data-level="9.3.3" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#the-problem-of-standard-parameterization"><i class="fa fa-check"></i><b>9.3.3</b> The problem of Standard Parameterization</a></li>
<li class="chapter" data-level="9.3.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#two-stream-self-attention"><i class="fa fa-check"></i><b>9.3.4</b> Two-Stream Self-Attention</a></li>
<li class="chapter" data-level="9.3.5" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#partial-prediction"><i class="fa fa-check"></i><b>9.3.5</b> Partial Prediction</a></li>
<li class="chapter" data-level="9.3.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#xlnet-pre-training-model"><i class="fa fa-check"></i><b>9.3.6</b> XLNet Pre-training Model</a></li>
<li class="chapter" data-level="9.3.7" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion-1"><i class="fa fa-check"></i><b>9.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#latest-nlp-models"><i class="fa fa-check"></i><b>9.4</b> Latest NLP models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduction-resources-for-nlp.html"><a href="introduction-resources-for-nlp.html"><i class="fa fa-check"></i><b>10</b> Introduction: Resources for NLP</a></li>
<li class="chapter" data-level="11" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html"><i class="fa fa-check"></i><b>11</b> Resources and Benchmarks for NLP</a><ul>
<li class="chapter" data-level="11.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#metrics"><i class="fa fa-check"></i><b>11.1</b> Metrics</a></li>
<li class="chapter" data-level="11.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#benchmark-datasets"><i class="fa fa-check"></i><b>11.2</b> Benchmark Datasets</a><ul>
<li class="chapter" data-level="11.2.1" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#squad"><i class="fa fa-check"></i><b>11.2.1</b> SQuAD</a></li>
<li class="chapter" data-level="11.2.2" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#coqa"><i class="fa fa-check"></i><b>11.2.2</b> CoQA</a></li>
<li class="chapter" data-level="11.2.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#superglue"><i class="fa fa-check"></i><b>11.2.3</b> (Super)GLUE</a></li>
<li class="chapter" data-level="11.2.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#aqua-rat"><i class="fa fa-check"></i><b>11.2.4</b> AQuA-Rat</a></li>
<li class="chapter" data-level="11.2.5" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#snli"><i class="fa fa-check"></i><b>11.2.5</b> SNLI</a></li>
<li class="chapter" data-level="11.2.6" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#lambada"><i class="fa fa-check"></i><b>11.2.6</b> LAMBADA</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#pre-training-resources"><i class="fa fa-check"></i><b>11.3</b> Pre-Training Resources</a></li>
<li class="chapter" data-level="11.4" data-path="resources-and-benchmarks-for-nlp.html"><a href="resources-and-benchmarks-for-nlp.html#resources-for-resources"><i class="fa fa-check"></i><b>11.4</b> Resources for Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-cases-for-nlp.html"><a href="use-cases-for-nlp.html"><i class="fa fa-check"></i><b>12</b> Use-Cases for NLP</a></li>
<li class="chapter" data-level="13" data-path="natural-language-generation.html"><a href="natural-language-generation.html"><i class="fa fa-check"></i><b>13</b> Natural Language Generation</a><ul>
<li class="chapter" data-level="13.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#definition-and-taxonomy"><i class="fa fa-check"></i><b>13.2</b> Definition and Taxonomy</a></li>
<li class="chapter" data-level="13.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#common-architectures"><i class="fa fa-check"></i><b>13.3</b> Common Architectures</a><ul>
<li class="chapter" data-level="13.3.1" data-path="recurrent-neural-networks-and-their-applications-in-nlp.html"><a href="recurrent-neural-networks-and-their-applications-in-nlp.html#encoder-decoder-architecture"><i class="fa fa-check"></i><b>13.3.1</b> Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="13.3.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#attention-architecture"><i class="fa fa-check"></i><b>13.3.2</b> Attention Architecture</a></li>
<li class="chapter" data-level="13.3.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#decoding-algorithm-at-inference"><i class="fa fa-check"></i><b>13.3.3</b> Decoding Algorithm at Inference</a></li>
<li class="chapter" data-level="13.3.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#memory-networks"><i class="fa fa-check"></i><b>13.3.4</b> Memory Networks</a></li>
<li class="chapter" data-level="13.3.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#language-models"><i class="fa fa-check"></i><b>13.3.5</b> Language Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#question-answer-systems"><i class="fa fa-check"></i><b>13.4</b> Question-Answer Systems</a><ul>
<li class="chapter" data-level="13.4.1" data-path="convolutional-neural-networks-and-their-applications-in-nlp.html"><a href="convolutional-neural-networks-and-their-applications-in-nlp.html#datasets"><i class="fa fa-check"></i><b>13.4.1</b> Datasets</a></li>
<li class="chapter" data-level="13.4.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types"><i class="fa fa-check"></i><b>13.4.2</b> Types</a></li>
<li class="chapter" data-level="13.4.3" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures"><i class="fa fa-check"></i><b>13.4.3</b> Architectures</a></li>
<li class="chapter" data-level="13.4.4" data-path="natural-language-generation.html"><a href="natural-language-generation.html#evaluation-metrics"><i class="fa fa-check"></i><b>13.4.4</b> Evaluation Metrics</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="natural-language-generation.html"><a href="natural-language-generation.html#dialog-systems"><i class="fa fa-check"></i><b>13.5</b> Dialog Systems</a><ul>
<li class="chapter" data-level="13.5.1" data-path="natural-language-generation.html"><a href="natural-language-generation.html#types-1"><i class="fa fa-check"></i><b>13.5.1</b> Types</a></li>
<li class="chapter" data-level="13.5.2" data-path="natural-language-generation.html"><a href="natural-language-generation.html#architectures-1"><i class="fa fa-check"></i><b>13.5.2</b> Architectures</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="transfer-learning-for-nlp-ii.html"><a href="transfer-learning-for-nlp-ii.html#conclusion"><i class="fa fa-check"></i><b>13.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>14</b> Epilogue</a><ul>
<li class="chapter" data-level="14.1" data-path="epilogue.html"><a href="epilogue.html#new-influentioal-architectures"><i class="fa fa-check"></i><b>14.1</b> New influentioal architectures</a></li>
<li class="chapter" data-level="14.2" data-path="epilogue.html"><a href="epilogue.html#improvements-of-the-selfattention-mechanism"><i class="fa fa-check"></i><b>14.2</b> Improvements of the SelfAttention mechanism</a></li>
<li class="chapter" data-level="14.3" data-path="epilogue.html"><a href="epilogue.html#evaluation-and-interpretability"><i class="fa fa-check"></i><b>14.3</b> Evaluation and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Approaches in Natural Language Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered">
<h1>References</h1>

<div id="refs" class="references">
<div>
<p>Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. “Layer Normalization.” <em>arXiv Preprint arXiv:1607.06450</em>.</p>
</div>
<div>
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” <em>arXiv Preprint arXiv:1409.0473</em>.</p>
</div>
<div>
<p>Beltagy, Iz, Kyle Lo, and Arman Cohan. 2019. “SciBERT: A Pretrained Language Model for Scientific Text.” In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)</em>, 3606–11.</p>
</div>
<div>
<p>Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. “A Neural Probabilistic Language Model.” <em>Journal of Machine Learning Research</em>, no. 3: 1137–55.</p>
</div>
<div>
<p>Boden, Mikael. 2002. “A Guide to Recurrent Neural Networks and Backpropagation.” <em>The Dallas Project</em>.</p>
</div>
<div>
<p>Boureau, Y-Lan, Jean Ponce, and Yann LeCun. 2010. <em>A Theoretical Analysis of Feature Pooling in Visual Recognition</em>. <a href="https://www.di.ens.fr/willow/pdfs/icml2010b.pdf" class="uri">https://www.di.ens.fr/willow/pdfs/icml2010b.pdf</a>.</p>
</div>
<div>
<p>Bowman, Samuel R, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. “A Large Annotated Corpus for Learning Natural Language Inference.” <em>arXiv Preprint arXiv:1508.05326</em>.</p>
</div>
<div>
<p>Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.”</p>
</div>
<div>
<p>Chen, Gang. 2016. “A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation.” <em>arXiv Preprint arXiv:1610.02583</em>.</p>
</div>
<div>
<p>Cho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine Translation.” <em>arXiv Preprint arXiv:1406.1078</em>.</p>
</div>
<div>
<p>Chollet, Francois. 2018. <em>Deep Learning Mit Python Und Keras: Das Praxis-Handbuch Vom Entwickler Der Keras-Bibliothek</em>. MITP-Verlags GmbH &amp; Co. KG.</p>
</div>
<div>
<p>Chung, Junyoung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.” <em>CoRR</em> abs/1412.3555. <a href="http://arxiv.org/abs/1412.3555" class="uri">http://arxiv.org/abs/1412.3555</a>.</p>
</div>
<div>
<p>Clark, Kevin, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. “ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators.” In <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=r1xMH1BtvB" class="uri">https://openreview.net/forum?id=r1xMH1BtvB</a>.</p>
</div>
<div>
<p>Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. <em>Natural Language Processing (Almost) from Scratch</em>. <em>J. Mach. Learn. Res.</em> Vol. 12. <a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" class="uri">http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf</a>.</p>
</div>
<div>
<p>Dai, Zihang, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. “Transformer-Xl: Attentive Language Models Beyond a Fixed-Length Context.” <em>arXiv Preprint arXiv:1901.02860</em>. <a href="https://arxiv.org/abs/1901.02860" class="uri">https://arxiv.org/abs/1901.02860</a>.</p>
</div>
<div>
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” <em>CoRR</em> abs/1810.04805. <a href="http://arxiv.org/abs/1810.04805" class="uri">http://arxiv.org/abs/1810.04805</a>.</p>
</div>
<div>
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.</p>
</div>
<div>
<p>Graves, Alex. 2013. “Generating Sequences with Recurrent Neural Networks.” <em>arXiv Preprint arXiv:1308.0850</em>.</p>
</div>
<div>
<p>Harris, Zellig S. 1954. “Distributional Structure.” <em>WORD</em> 10 (2-3): 146–62.</p>
</div>
<div>
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Identity Mappings in Deep Residual Networks.” In <em>European Conference on Computer Vision</em>, 630–45. Springer.</p>
</div>
<div>
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8). MIT Press: 1735–80.</p>
</div>
<div>
<p>Kaiser, Lukasz, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit. 2017. “One Model to Learn Them All.” <em>arXiv Preprint arXiv:1706.05137</em>.</p>
</div>
<div>
<p>Kalchbrenner, Nal, Lasse Espeholt, Karen Simonyan, Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016. “Neural Machine Translation in Linear Time.” <em>ArXiv</em> abs/1610.10099. <a href="https://arxiv.org/pdf/1610.10099.pdf" class="uri">https://arxiv.org/pdf/1610.10099.pdf</a>.</p>
</div>
<div>
<p>Kalchbrenner, Nal, Edward Grefenstette, and Phil Blunsom. 2014. <em>A Convolutional Neural Network for Modelling Sentences</em>. <em>ArXiv</em>. Vol. abs/1404.2188. <a href="http://mirror.aclweb.org/acl2014/P14-1/pdf/P14-1062.pdf" class="uri">http://mirror.aclweb.org/acl2014/P14-1/pdf/P14-1062.pdf</a>.</p>
</div>
<div>
<p>Kim, Yoon. 2014. <em>Convolutional Neural Networks for Sentence Classification</em>. <a href="https://www.aclweb.org/anthology/D14-1181.pdf" class="uri">https://www.aclweb.org/anthology/D14-1181.pdf</a>.</p>
</div>
<div>
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. <em>ImageNet Classification with Deep Convolutional Neural Networks</em>. <a href="http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf" class="uri">http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf</a>.</p>
</div>
<div>
<p>Kudo, Taku, and John Richardson. 2018. “Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” <em>arXiv Preprint arXiv:1808.06226</em>.</p>
</div>
<div>
<p>Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. “Albert: A Lite Bert for Self-Supervised Learning of Language Representations.” <em>arXiv Preprint arXiv:1909.11942</em>.</p>
</div>
<div>
<p>Ling, Wang, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. “Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.” <em>arXiv Preprint arXiv:1705.04146</em>.</p>
</div>
<div>
<p>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “Roberta: A Robustly Optimized Bert Pretraining Approach.” <em>arXiv Preprint arXiv:1907.11692</em>. <a href="https://arxiv.org/abs/1907.11692" class="uri">https://arxiv.org/abs/1907.11692</a>.</p>
</div>
<div>
<p>Luong, Minh-Thang, Hieu Pham, and Christopher D Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” <em>arXiv Preprint arXiv:1508.04025</em>.</p>
</div>
<div>
<p>Malte, Aditya, and Pratik Ratadiya. 2019. “Evolution of Transfer Learning in Natural Language Processing.”</p>
</div>
<div>
<p>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” <em>Advances in Neural Information Processing Systems</em>, 3111–9.</p>
</div>
<div>
<p>Mikolov, Tomáš, Martin Karafiát, Lukáš Burget, Jan Černocky, and Sanjeev Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In <em>Eleventh Annual Conference of the International Speech Communication Association</em>.</p>
</div>
<div>
<p>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “BLEU: A Method for Automatic Evaluation of Machine Translation.” In <em>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, 311–18. Association for Computational Linguistics.</p>
</div>
<div>
<p>Pascanu, Razvan, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2013. “How to Construct Deep Recurrent Neural Networks.” <em>arXiv Preprint arXiv:1312.6026</em>.</p>
</div>
<div>
<p>Pennington, Jeffrey, Richard Socher, Manning, and Christopher D. 2014. “GloVe: Global Vectors for Word Representation.” <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43.</p>
</div>
<div>
<p>Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.”</p>
</div>
<div>
<p>Prabhavalkar, Rohit, Kanishka Rao, Tara N Sainath, Bo Li, Leif Johnson, and Navdeep Jaitly. 2017. “A Comparison of Sequence-to-Sequence Models for Speech Recognition.” In <em>Interspeech</em>, 939–43.</p>
</div>
<div>
<p>Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.”</p>
</div>
<div>
<p>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” <em>arXiv Preprint arXiv:1910.10683</em>. <a href="https://arxiv.org/abs/1909.11942" class="uri">https://arxiv.org/abs/1909.11942</a>.</p>
</div>
<div>
<p>Rajpurkar, Pranav, Robin Jia, and Percy Liang. 2018. “Know What You Don’t Know: Unanswerable Questions for Squad.”</p>
</div>
<div>
<p>Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.”</p>
</div>
<div>
<p>R Core Team. 2018. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
</div>
<div>
<p>Reddy, Siva, Danqi Chen, and Christopher D. Manning. 2018. “CoQA: A Conversational Question Answering Challenge.” <em>CoRR</em> abs/1808.07042. <a href="http://arxiv.org/abs/1808.07042" class="uri">http://arxiv.org/abs/1808.07042</a>.</p>
</div>
<div>
<p>Ribeiro, Marco Tulio, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. “Beyond Accuracy: Behavioral Testing of NLP Models with CheckList.” In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 4902–12. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.442" class="uri">https://doi.org/10.18653/v1/2020.acl-main.442</a>.</p>
</div>
<div>
<p>Scherer, Dominik, Andreas C. Müller, and Sven Behnke. 2010. “Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition.” In <em>ICANN</em>. <a href="http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf" class="uri">http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf</a>.</p>
</div>
<div>
<p>Schuster, Mike, and Kuldip K Paliwal. 1997. “Bidirectional Recurrent Neural Networks.” <em>IEEE Transactions on Signal Processing</em> 45 (11). Ieee: 2673–81.</p>
</div>
<div>
<p>Schwenk, Holger, Loïc Barrault, Alexis Conneau, and Yann LeCun. 2017. <em>Very Deep Convolutional Networks for Text Classification</em>.</p>
</div>
<div>
<p>Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2015. “Neural Machine Translation of Rare Words with Subword Units.” <em>arXiv Preprint arXiv:1508.07909</em>.</p>
</div>
<div>
<p>Sutskever, Ilya, James Martens, George E. Dahl, and Geoffrey E. Hinton. 2013. “On the Importance of Initialization and Momentum in Deep Learning.” In <em>ICML</em>.</p>
</div>
<div>
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3104–12.</p>
</div>
<div>
<p>Tenney, Ian, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, et al. 2020. “The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for Nlp Models.” <em>arXiv Preprint arXiv:2008.05122</em>.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, 5998–6008.</p>
</div>
<div>
<p>Venugopalan, Subhashini, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2015. “Sequence to Sequence-Video to Text.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 4534–42.</p>
</div>
<div>
<p>Visin, Francesco, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron C. Courville, and Yoshua Bengio. 2015. <em>ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks</em>. <em>ArXiv</em>. Vol. abs/1505.00393.</p>
</div>
<div>
<p>Wang, Alex, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. “Superglue: A Stickier Benchmark for General-Purpose Language Understanding Systems.” In <em>Advances in Neural Information Processing Systems</em>, 3261–75.</p>
</div>
<div>
<p>Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. “Glue: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” <em>arXiv Preprint arXiv:1804.07461</em>.</p>
</div>
<div>
<p>Wang, Sinong, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. “Linformer: Self-Attention with Linear Complexity.” <em>arXiv Preprint arXiv:2006.04768</em>.</p>
</div>
<div>
<p>Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” <em>arXiv Preprint arXiv:1609.08144</em>.</p>
</div>
<div>
<p>Yamaguchi, Kouichi, Kenji Sakamoto, Toshio Akabane, and Yoshiji Fujimoto. 1990. <em>A Neural Network for Speaker-Independent Isolated Word Recognition</em>. <a href="https://www.isca-speech.org/archive/archive_papers/icslp_1990/i90_1077.pdf" class="uri">https://www.isca-speech.org/archive/archive_papers/icslp_1990/i90_1077.pdf</a>.</p>
</div>
<div>
<p>Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” In <em>Advances in Neural Information Processing Systems 32</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, 5753–63. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf" class="uri">http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf</a>.</p>
</div>
<div>
<p>Zhang, Xiang, Junbo Jake Zhao, and Yann LeCun. 2015. <em>Character-Level Convolutional Networks for Text Classification</em>. <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" class="uri">https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf</a>.</p>
</div>
<div>
<p>Zhang, Zhengyan, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. “ERNIE: Enhanced Language Representation with Informative Entities.” <em>arXiv Preprint arXiv:1905.07129</em>.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="acknowledgements.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/seminar_nlp_ss20/edit/master/99-references.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
