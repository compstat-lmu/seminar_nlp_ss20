---
output:
  bookdown::html_document2: default
    fig_caption: true
header-includes: \usepackage{booktabs} \usepackage{longtable} \usepackage{array} \usepackage{multirow}
  \usepackage[table]{xcolor} \usepackage{wrapfig} \usepackage{float} \floatplacement{figure}{H}
bibliography: book.bib
link-citations: yes
---

# Foundations/Applications of Modern NLP

*Authors: Viktoria Szabo*

*Supervisor: Christian Heumann*

```{r setup-01-01, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE,  warning=FALSE)
library(knitr)
```

Word embeddings can be seen as the beginning of modern natural language processing. They are widely used in every kind of NLP task. One of the advantages is that you can download and use pretrained word embeddings. With this, you can save a lot of time for training the final model. But if the task is not a standard one you probably want to train your own embeddings to get a better model performance for your specific task. In the following I will first outline the evolution from sparse representations of words to dense word embeddings. After that I will describe calculation methods for word embeddings within a neural network language model and with word2vec and GloVe. In the third part I will show how to improve the model performance regardless of the chosen model class based on hyperparameter tuning and system design choices and explain some model expansion to tackle problems of the aforementioned methods. The evaluation of word embeddings on different tasks and datasets is another topic which will be covered in the fourth part of this chapter. Last but not least I will present some resources to download pretrained word embeddings. 

## The Evolution of Word Embeddings

Since computers work with numeric representations, converting the text and sentences we want to analyze into numbers is unavoidable. One-Hot Encoding and Bag-of-Words (BOW) are two simple approaches how this could be done. These methods are usually used as input for calculating more elaborate word representations called word embeddings. 
The **One-Hot Encoding** labels each word in the vocabulary with an index. Let n be size of the vocabulary, then each word is represented by a vector with dimension n. Every dimension is zero except for the one corresponding to its index, which is set to $1$. A sentence is represented as a matrix of shape ($n\times n$) where n is the number of unique words in the sentence or a document. You can see an example for this representation on the left side in figure \@ref(fig:onehot-bow-01-01). 
A more elaborate approach compared to the first one is called **Bag-of-Words (BOW)** and belongs to the count-based approaches. This approach counts the occurrences and co-occurrences of all distinct words in a document or a text chunk. Each text chunk is then represented by a row in a matrix, where the columns are the words. That means compared to the One-Hot Encoding this approach already incorporates some of the context information in sentences and text chunks. You can see an example for this approach on the right side in figure \@ref(fig:onehot-bow-01-01).


```{r onehot-bow-01-01, fig.show ='hold', fig.align = "default", fig.cap="One-Hot Encoding on the left and Bag-of-Words on the right. Source: Own figure.", out.width = '50%'}

include_graphics(c("figures/01-01-foundations-applications-of-modern-NLP/01-01_one-hot.png", "figures/01-01-foundations-applications-of-modern-NLP/01-01_bow.png"))
```

These approaches definitely have some **positive points** about them. First of they are very simple to construct. Furthermore, they are robust to changes and it was observed that simple models trained on huge amounts of data outperform complex systems trained on less data. Bag-of-Words is especially useful if the number of distinct words is small and the sequence of the words doesn’t play a key role, like in sentiment analysis. Only use these approaches without calculating word embeddings if you have a small amount of distinct words in your document, the words are not meaningfully correlated and you have a lot of data to learn from.
Nevertheless, the **problems** coming from these approaches usually outweigh the positive points. The most obvious one is that these approaches lead to very sparse input vectors, that means large vectors with relatively few non-zero values. Many machine learning models won’t work well with high dimensional and sparse features (Goldberg, 2015). Neural networks in particular struggle with this type of data. And with growing vocabulary the feature size vectors also increase by the same length. So, the dimensionality of these approaches is the same as the number of different words in your text. That means you need to estimate more parameters and therefore you require exponentially more data to do well enough to build a reasonably generalizable model. This is called the curse of dimensionality. But these problems can be solved with dimensionality reduction methods such as Principal Component Analysis or feature selection models where less informative context word, such as “the” are dropped. 
The major drawback of these methods is that there is no notion of similarity between words. That means words like “cat” and “tiger” are represented as similar as “cat” and “car”. If the words “cat” and “tiger” would be represented as similar words one could use the information won from the more frequent word “cat” for sentences in which the less frequent word “tiger” appears. If the word embedding for “tiger” is similar to that of “cat” the network model can take a similar path instead of having to learn how to handle it completely anew. It’s very difficult to make predictions about things unlike you’ve ever seen before – much easier if it’s related to something you have seen.

To overcome these problems **word embeddings** were introduced. Word embeddings use continuous vectors to represent each word in a vocabulary. These vectors have n dimensions, usually between 100 and 500, which represent different aspects of the word. With this semantic similarity can be maintained in the representation and generalization can be achieved. Through the vectors the words are mapped to a continuous vector space (in NLP usually called semantic space), where semantically similar words occur close to each other, while more dissimilar words are far from each other. Figure \@ref(fig:word-embedding1) shows a simple example to convey the idea behind this approach. In this example the words are represented by a three-dimensional vector.

```{r word-embedding1, fig.align = "default", fig.cap="Example for word embeddings with three dimensions. Source: ..."}

include_graphics("figures/01-01-foundations-applications-of-modern-NLP/01-01_word_embeddings_1.png")
```

If you want to represent higher dimensional word vectors you could use dimension reduction methods such as principal component analysis (PCA) to break down the number of dimensions into two or three and then plot the words. You can see an example of this for a few country names and their capitals in figure \@ref(fig:word-embedding2). You can see that the country names all have negative values on the x-axis and the capitals all have positive values on the x-axis. Furthermore, the countries have similar y-axis values as their corresponding capitals.

```{r word-embedding2, fig.align = "default", fig.cap="Two-dimensional PCA projection of 1000-dimensional word vectors of countries and their capital cities. Source: Mikolov et al. 2013 (3)"}

include_graphics("figures/01-01-foundations-applications-of-modern-NLP/01-01_word_embeddings_2.png")
```

With such word vectors even algebraic computations become possible as shown in Mikolov et al. 2013. For example, vector(“King”)-vector(“Man”) + vector(“Woman”) results in a vector that is closest to the vector representation of the word “Queen”. Another possibility to use word embeddings vectors is translation between languages. Mikolov et al. 2013-1 showed that they can find word translations by comparing vectors learnt in different languages. By searching for a translation one can use the word vector from the source language and search for the closest vector in the target language vector space, this word can then be used as a translation. Because if a word vector from one language is similar to the word vector of the other language, this word is used in a similar context. This method can be used to infer missing dictionary entries. You can see an example for this method depicted in figure ... .In figure ... the vectors for numbers and animals are depicted on the left side and the same words are depicted on the right side. It can be seen, that the vectors for the correct translation align in similar geometric spaces. Again, two-dimensional representation was achieved by using dimension reduction methods.

```{r language, fig.align = "default", fig.cap="Distributed word vector representations of numbers and animals in English (left) and Spanish (right). Source: Mikolov et al. 2013"}

include_graphics("figures/01-01-foundations-applications-of-modern-NLP/01-01_language.png")
```

## Methods to Obtain Word Embeddings

The basic idea behind learning word embeddings is the so called “distributional hypothesis” (Harris, 1954). It states that words that occur in the same contexts tend to have similar meanings. For instance, Jupiter and Venus tend to have similar semantics since they usually appear in similar contexts, e.g., with words such as solar system, star, planet, and astronomy. That’s why machine learning and deep learning algorithms can find representations by themselves by evaluating the context in which a word occurs. Words that are used in similar contexts will be given similar representations. This is usually done as an unsupervised or self-supervised procedure, which is a big advantage and is one of the winning points when compared to other word representation approaches. That means word embeddings can be thought of unsupervised feature extractors for words. However, the methods to find such similarities in the context of words vary. First finding word representations started out with the nowadays so called more traditional count-based techniques, which collected word statistics like occurrence and co-occurrence frequencies of words as seen above with BOW. But these representations are often large and need some sort of dimensionality reduction. Later when neural networks were introduced into NLP the so-called predictive techniques, mainly popularized after 2013 with the introduction of word2vec, supplanted the traditional count-based word representations. It is also said that these models learn “dense representations” since they directly learn low-dimensional word representations, without needing to resort to the additional dimensionality reduction step. In the following I will give an introduction to the best-known predictive approaches to model word embeddings. I will start with neural network language models, where word embeddings are learnt as a part from the final language model. After that I will describe the two popular algorithms word2vec and GloVe, which learn word embeddings in a pre-step before the actual statistical language model.

### Feedforward Neural Network Language Model (NNLM)
Bengio et al. in 2003 were the first to propose learning word embeddings within a statistical neural network language model (NNLM). The goal of the NNLM model of Bengio et al. (2003) is to predict the next word based on a sequence of preceding words. Using a simple feedforward neural network, the model first learns the word embeddings (also called distributed representation) and in a second step the probability function for word sequences. And so, one obtains not only the model itself, but also the learned word representations, which can be used as input for other, potentially unrelated, tasks. 
The proposed neural network architecture has an input layer with one-hot encoded word inputs, a linear projection layer for the word embeddings, a hidden layer with a hyperbolic tangent function, where most of the computation is done, followed by a softmax classifier output layer. The output of the model will be a vector of the probability distribution over all words given a specific context. That means a vector with probability scores for each word of the vocabulary. The i-th element of the output vector is the probability estimation P(w_t = i|context). The softmax classifier is used to guarantee positive probabilities summing to one. It computes the following function:

Formula

The yi are the unnormalized log-probabilities for each output word i, which were computed in the previous layer. You can see the model architecture proposed in Bengio et al. 2003 in figure \@ref(fig:bengio-nnlm).
When training a neural network, you have to define a loss function *L*(**y_hut**, ** y**) stating the loss of predicting **y_hut** when the true output is **y**.  In the NNLM literature a cross-entropy loss is very common (see Goldberg, 2015). **y_hut** is the networks output vector, which was transformed by the softmax activation function and represents the conditional distribution  . **y** is usually either a one-hot vector for the correct output word or it is a vector representing the true multinomial probability distribution over the vocabulary given the specific context. Then the parameter *phi* of the neural network (like the weights for the embedding vectors) are iteratively changed in order to minimize the loss *L* over the training examples. 
This is usually done with the **stochastic gradient descent (SGD)** optimizer where the gradient is obtained via **backpropagation**. The gradient descent optimizer tries to find the direction of the strongest descent via partial derivation and updates the parameter *phi* accordingly. The learning rate *e* defines the size of the step in this direction. (see Goldberg, 2015)
In Bengio et al. (2003) they use a gradient ascent optimizer, which performs the following iterative update after presenting the *t*-th word of the training corpus:

Formula

The method by which parameter adjustments are made during training so that they can be optimized is called backpropagation. Backpropagation essentially consists of six steps:
1.  (Random) Initialization of the weights of the network
2.	Calculation of y(x_i) for the inputs x_i
3.	Determining the cost of the inputs x_i with the loss function
4.	Calculation of the partial derivatives of the loss for each weight (gradient descent optimizer)
5.	Update the weights in the network using the partial derivatives calculated in step 4
6.	Return to step 2 and continue the procedure until the partial derivatives of the loss approach zero


```{r bengio-nnlm, fig.align = "default", fig.cap="Architecture for NNLM proposed by Bengion et al. in 2003.  Source: Bengio et al. 2003"}

include_graphics("figures/01-01-foundations-applications-of-modern-NLP/01-01_bengio_nnlm.png")
```


### Word2Vec
In 2013 Mikolov et. al proposed the two word2vec algorithms which led to a huge wave in NLP popularizing word embeddings. In contrast to the NNLM model above the word2vec algorithms are not used for a statistical language modeling goal, but only to learn the word embeddings. The two word2vec algorithms named Continuous Bag-of-Words (CBOW) and Continuous Skip-Gram use shallow neural networks with an input layer, a projection layer and an output layer. That means compared to the previously explained feedforward NNLM the second, non-linear hidden layer is removed.
The general idea behind CBOW is to predict the focus word based on a window of context words. The order of context words does not influence the prediction, thus the name Bag-of-Words. While Skip-Gram tries to predict the context words given a source word. This is done while adjusting the initial weights during training so that a loss function is reduced.
In the **CBOW** architecture the *N* input (context) words are each one-hot encoded vectors of size *V*, where *V* is the size of the vocabulary. Compared to the NNLM model CBOW uses past and following words as context instead of only the past words. The projection layer is a standard fully connected (dense) layer which has the dimensionality *1 x D*, where D is the size of the dimensions for the word embeddings. The projection layer is shared for all words. That means all words get projected into the same position in a linear manner, where the vectors are averaged. The output layer outputs probabilities for the target words from the vocabulary and has a dimensionality of *V*. That means the output is a probability distribution over all words of the vocabulary as in the NNLM model, where the prediction is the word with the highest probability. But instead of using a standard softmax classifier as in the NNLM model the authors propose to use a log-linear hierarchical softmax classifier  for the calculation of the probabilities. The model architecture is shown in figure \@ref(fig:word2vec). 
The **continuous Skip-gram** architecture also uses a log-linear hierarchical softmax classifier with a continuous projection layer, but the input is only one source word, and the output layer consists of as many probability vectors over all words as the chosen number of context words. Also, since the more distant words are usually less related to the source word, the skip-gram model weighs nearby context words more heavily than more distant context words by sampling less from those words in the training examples. You can find the model architecture on the right side of figure \@ref(fig:word2vec).

```{r word2vec, fig.align = "default", fig.cap="Learning word embeddings with the model architecture of CBOW and Skip-Gram.  Source: Mikolov et al. 2013 (3)"}

include_graphics("figures/01-01-foundations-applications-of-modern-NLP/01-01_word2vec.png")
```

As said before the word2vec models use hierarchical softmax, where the vocabulary is represented as a Huffman binary tree, instead of the standard softmax classifier explained in the section before. With hierarchical softmax the size of the output vector can be reduced from the vocabulary size *V* to the logarithm to base 2 of *V*, which is a dramatical change in computational complexity and number of operations needed for the algorithm. Further explanations for this method can be found in Morin and Bengio 2005. For both models Mikolov et al. (2013-2) use gradient descent optimization and backpropagation as explained above.
Mikolov et al. (2013-2) show, that their word2vec algorithms outperform a lot of other standard NNLM models. CBOW is faster while skip-gram is slower but does a better job for infrequent words. Skip-gram works well with small amounts of training data and represents even words that are considered rare, whereas CBOW trains several times faster and has slightly better accuracy for frequent words. 
 

### GloVe
GloVe stands for **Glo**bal **Ve**ctor word representation, which emphasizes the global character of this model. Unlike the previously described algorithms like word2vec GloVe not only relies on local context information but also incorporates global co-occurrence statistics. Instead of extracting the embeddings from a neural network that is designed to perform a task like predicting neighboring words (CBOW) or predicting the focus word (Skip-Gram), the embeddings are optimized directly, so that the dot product of two word vectors equals the log of the number of times the two words will occur near each other. The model builds on the idea that you can derive semantic relationships between words from the co-occurrence matrix and that the ratio of co-occurrence probabilities of two words with a third word is more indicative of their semantic association than a direct co-occurrence probability. 
Let P_ij = P(j|i) = X_ij/X_i be the probability that word j appears in the context of word i. In figure \@ref(fig:glove)you can see an example with the words “ice” and “steam”. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with other words k. 
For words like “solid” which are related to ice but not steam the ratio is large (>1), while for words like “gas”, which is related to steam but not to ice, the ratio is small (<1). For words like “water” or “fashion”, which are either related to both of the words or to none the ratios are close to 1. Therefore, we can see that the comparison of co-occurrence with a third word is more indicative for the semantic meanings and makes a better job at distinguishing relevant words (solid and gas) from irrelevant words (water and fashion) than the raw probabilities.

```{r glove, fig.align = "default", fig.cap="Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token corpus.  Source: Pennington et al. 2014"}

include_graphics("figures/01-01-foundations-applications-of-modern-NLP/01-01_glove_ratios.png")
```
That’s why Pennington et al. tried to find a way to incorporate the ratio P_ik/P_jk into computing the word embeddings. They propose an optimization problem which aims at fulfilling the following objective:

Formula

Where b_i and b_k are bias terms for word w_i and probe word w_k and X_ik is the number of times w_i co-occurs with w_k. Fulfilling this objective minimizes the difference between the dot product of w_i and w_k and the logarithm of their number of co-occurrences. In other words, the optimization results in the construction of vectors w_i and w_k whose dot product gives a good estimate of their transformed co-occurrence counts. To solve this optimization problem, they reformulate equation … as a least squares problem and introduce a weighting function, since rare co-occurrences add noise to the model and add less information than the more frequent ones. 

## Hyperparameter Tuning and System Design Choices
Once adapted across methods, hyperparameter tuning significantly improves performance in every task. Levy et al. (2015) showed that in a lot of cases, changing the setting of a single hyperparameter could yield a greater increase in performance than switching to a better algorithm or training on a larger corpus. They conducted a series of experiments where they assessed the contributions of diverse hyperparameters. They also show that when all methods are allowed to tune a similar set of hyperparameters, their performance is largely comparable. But they found that choosing wrong hyperparameter can even degrade performance of a model. That’s why tuning the hyperparameter fitting your specific context can be very important. Depending on the model you choose there are a lot of hyperparameter available for tuning. These are parameters like number of epochs, batch-size, learning rate, embedding size, window size, corpus size and some more. I will focus on hyperparameter which are frequently discussed. Furthermore, I describe some system design and setup choices, which will tackle some of the problems posed by the algorithms mentioned above.

**Word embedding size**

The question of how many embedding dimensions should be used is mostly answered empirically. It depends on your task, computing capacity and vocabulary. The trade-off here is between accuracy and computational concerns. More dimensions could potentially increase the accuracy of the representations since the vectors can capture more aspects of the word. But more dimensions mean higher computing time and effort. In practice, people mostly use word embedding vectors with dimensions around 50 to 300 by a rule of thumb. (see Goldberg 2015)
But Patel and Bhattacharyya 2017 found that the dimension size should be chosen based on some text corpus statistics. You can calculate a lower bound from the number of pairwise equidistant words of the corpus vocabulary. Choosing a dimension size below this bound results in a loss of quality of learned word embeddings. They tested this result empirically for the skip-gram algorithm. (see Bhattacharyya 2017)
Penningten et al. 2014 compare performance of their GloVe model for embedding sizes from 1 to 600 for different evaluation tasks (semantic, syntactic and overall). They found that after around 200 dimensions the performance rise starts to stagnate.

**Context Window**

In traditional approaches the context window around the focus word is usually constant-sized and unweighted. For example, if the symmetrical context size is 5 then the 5 words before the focus word and 5 words after the focus words are the context window. It is also possible to use an asymmetric context window, which for example only uses words that appear before the focus word. A window size of 5 is commonly used to capture broad topic/domain information like what other words are used in related discussions (i.e. “dog”, “bark” and “leash” will be grouped together, as well as “walked”, “run” and “walking”), whereas smaller windows contain more specific information about the focus word and produce more functional and syntactic similarities (i.e. “Poodle”, “Pitbull”, “Rottweiler”, or “walking”,“running”,“approaching”). (Levy and Goldberg 2014, Goldberg 2015)
Since words which appear closer to the focus word are usually more indicative of its meaning it is possible to give the context words weights according to their distance from the focus word. Both word2vec and GloVe use such weighting schemes. GloVe’s implementation weights contexts using the harmonic function, e.g. a context word three tokens away will have 1/3 as a weight. On the other hand, word2vec’s implementation is equivalent to weighing by the distance from the focus word divided by the window size. For example, a size-5 window will weigh its contexts by 5/5, 4/5, 3/5, 2/5, 1/5. (Levy et al. 2015)
You can also find a performance comparison for GloVe using different window sizes for different evaluation tasks (semantic, syntactic and overall) in Pennington et al. 2014. Which shows that depending on the task larger performance gains can be expected from a larger window size. But for all three tasks the performance gains decrease after a window size of 4.

**Document Context**

Instead of using a few words as the context window one could consider all the other words that appear with the focus word in the same sentence, paragraph or document. One can either consider this as using very large window sizes or like in the doc2vec algorithm from Mikolov and Le (2014) add another embedding vector for a whole paragraph to the other context word vectors. These approaches will result in word vectors that capture topical similarity (words from the same topic, i.e. words that one would expect to appear in the same document, are likely to receive similar vectors). (Goldberg, 2015; Mikolov and Le 2014)

**Subsampling of Frequent Words**

Mikolov et al 2013 proposed for their word2vec algorithms to only use a subsample of the most frequent words. Very frequent words are often so-called stop-words, like “the” or “a”, which do not provide high information value. Using less of these frequent words leads to a significant speedup and improves accuracy of the representations of less frequent words (see Mikolov et al. 2013-3). The method randomly removes words *w* with a probability *p* that occur more often than a certain threshold *t*. In Mikolov et al. 2013-1 the probability *p* is defined as followed: 

formula

where *f* marks the word’s frequency in the text corpus. In Mikolov et al. 2013-3 the threshold *t* is set to 10^-5, but generally this parameter is open for tuning. In Mikolov et al. 2013-1 this subsampling is done before processing the text corpus. This leads to an artificial enlargement of the context window size. You could do the subsampling without affecting the context window size, but Levy and Goldberg (2015) found that it does not affect performance too much.

**Negative Sampling**

In their first paper Mikilov et al. 2013-2 proposed hierarchical softmax instead of the standard softmax function to speed up the calculation in the neural network. But soon they published a new method called negative sampling, which is even more efficient in the calculation of word embeddings. The negative sampling approach is based on the skip-gram algorithm, but is optimizing a different objective. It tries to maximize a function of the product of word and context pairs (w, c) that occur in the training data, and minimize it for negative examples of word and context pairs (w, c_n) that do not occur in the training corpus. The negative examples are created by drawing *k* negative examples for each observed (w, c) pair. *k* can be tuned as an hyperparameter. Mikolov et al. 2013-3 found that *k* in the range 5-20 are useful for small training datasets, while for large datasets *k* can be as small as 2-5. (see Mikolov et al. 2013-3; Goldberg and Levy 2014)

**Subword Information**

An individual word can convey a lot of information besides the general meaning of the word. Ignoring the internal structure of the words can lead to a huge information loss, especially in morphologically rich languages like Finnish or Turkish. Furthermore, coping with completely unseen words not included in the training data (so-called out-of-vocabulary words) is not possibly if every word gets assigned a distinct new vector representation like in the previously presented models. One way to deal with these problems is to train character-based models instead of word-based models. But as Goldberg (2015) states: “working on the character level is very challenging, as the relationship between form (characters) and function (syntax, semantics) in language is quite loose. Restricting oneself to stay on the character level may be an unnecessarily hard constraint.” Therefore, another approach which came up to solve the problem of OOV words is to use subword information. In this context fastText was introduced in two papers 2016 and 2017 (see Joulin et al. 2016 and Bojanowski et al. 2017). fastText builds upon the previously described continuous skip-gram model. But instead of learning vectors for words directly as done by skip-gram, fastText represents each word as an n-gram of characters. So, for example, take the word, “planning” with n=3, the fastText representation of this word is <pl, pla, lan, ann, nni, nin, ing, ng>, where the angular brackets indicate the beginning and end of the word. This helps capture the meaning of shorter words inside longer words and allows the embeddings to understand suffixes and prefixes. In addition to these n-grams fastText learns embeddings to the whole word. Bojanowski et al. (2017) extracted all the n-grams for n greater or equal to 3 and smaller or equal to 6. But they also state, that “the optimal choice of length ranges depends on the considered task and language and should be tuned appropriately”. In the end one word will be represented by the sum of the vector representations of its n-grams and the word itself. In the case of an unseen word (OOV), the corresponding embedding is induced by averaging the vector representations of its constituent character n-grams. That’s why fastText works well when having data with a lot of rare words. 

**Phrase representation**

The models described in the previous section all focus on individual words as input. But there are a lot of words, which will only be meaningful in combination with other words, or which can change their meaning completely when paired up with another word. That means there are phrases which have a meaning that is not a simple composition of the meanings of its individual words. Often these are names like “New York” for a city or “Toronto Raptors” for a basketball team.  Since the meaning is changing completely when looking at the combination of these words one embedding must be learned for the whole phrase instead of using the embeddings of the individual words.
Mikolov et al. 2013 proposed an approach to deal with such phrases. First, they use following score to find them in the text corpus:

Formula

With this score they try to find words that appear frequently together, and infrequently in other contexts. The score compares the appearance of two words together to they appearance alone in the text corpus. The Delta is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed. When the score of a word pair is above a chosen threshold value then it will be used as a phrase. They repeat this calculation 2 to 4 times with decreasing threshold value, allowing longer phrases that consists of several words to be formed.

## Evaluation Methods

**Word Similarity Task** 

The word similarity task tries to evaluate the distances between embedding vectors, which should represent their similarity, by comparing them to similarity scores given by human. The more similar the embedding distance to the human score the better the embedding. The word vectors are evaluated by ranking the pairs according to their cosine similarities, and measuring the correlation (Spearman’s ρ) with the human ratings. These are five datasets used to evaluate word similarity ranked by their size: 

1. SimVerb-3500, 3 500 pairs of verbs assessed by semantic similarity with a scale from 0 to 4 [Gerz et al., 2016].
2. MEN, 3 000 pairs assessed by semantic relatedness with a discrete scale from 0 to 50 [Bruni et al., 2014].
3. RW (acronym for Rare Word), 2 034 pairs of words with low occurrences (rare words) assessed by semantic similarity with a scale from 0 to 10 [Luong et al., 2013].
4. SimLex-999, 999 pairs assessed with a strong respect to semantic similarity with a scale from 0 to 10 [Hill et al., 2016].

**Word analogy task**

In the word analogy task word relations are predicted in the form “a is to a∗ as b is to b∗”, where b∗ is hidden, and must be guessed from the entire vocabulary. A popular example of an analogy is that *king* relates to *queen* as *man* relates to *woman*.  These are five datasets used to evaluate word analogy ranked by their size:

1. WordRep, 118 292 623 analogy questions (4-word tuples) divided into 26 semantic classes [Gao et al., 2014].
2. BATS (acronym for Bigger Analogy Test Set), 99 200 questions divided into 4 classes (inflectional morphology, derivational morphology, lexicographic semantics and encyclopedic semantics) and 10 smaller subclasses. [Gladkova et al., 2016].
3. Google Analogy (also called Semantic-Syntactic Word Relationship Dataset), 19 544 questions divided into 2 classes (morphological relations and semantic relations) and 10 smaller subclasses (8 869 semantic questions and 10 675 morphological questions) [Mikolov et al., 2013a].
4. SemEval-2012, 10 014 questions divided into 10 semantic classes and 79 subclasses prepared for the SemEval-2017 Task 2 (Measuring Degrees of Relational Similarity) [Jurgens et al., 2012].
5. MSR (acronym for Microsoft Research Syntactic Analogies), 8 000 questions divided into 16 morphological classes [Mikolov et al., 2013b].


## Sources of Pre-calculated embeddings

Resources to download precalculated word embeddings:
Glove:
https://nlp.stanford.edu/projects/glove/ 

fastText:
https://fasttext.cc/docs/en/english-vectors.html 

Different Models and different languages:
http://vectors.nlpl.eu/repository/ 

## Outlook
Problems that remain:

- Word order is not taken into account
- Words with more than one meaning are not considered
  - Sense Embeddings
  - Contextualized Embeddings
- Bias in data -> Word stereotypes
- Domains and Languages with little training data
