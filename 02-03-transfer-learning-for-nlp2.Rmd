# Transfer Learning for NLP II

*Authors: Bailan He*

*Supervisor: M. Aßenmacher*

Unsupervised representation learning has been highly successful in NLP. Typically, these methods first pre-train neural networks on large-scale unlabeled text corpora and then finetune the models on downstream tasks. Here we introduce the three remarkable models, BERT, GPT-2 and XLNet. [Transformers]("https://github.com/huggingface/transformers") is an excellent github repository, where readers can find their implementations.

## Bidirectional Encoder Representations from Transformers (BERT)

### Autoencoding

```{r ch02-03-figure01, echo=FALSE, out.width="50%", fig.cap="Autoencoding", fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/autoencoding.png")
```


Autoencoding(AE) have been most successful pre-training objectives and figure \@ref(fig:ch02-03-figure01) shows the modeling of it. AE based model does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. Specifically, given a text sequence $X = (x_1,...,x_T)$, AE factorizes the log-likelihood into a partial sum $log p(\bar{X}|\hat{X}) = \sum^T_{t=1} mask_t p(x_t|\hat{X})$, where $mask_t$ is an indicator to show if a token is masked,@yang2019xlnet.
The training objective is to reconstuct the masked tokens $\bar{X}$ given the sequence $\hat{X}$, AE trys to find the best model $P$ to predict the masked tokens.

### Introduction of BERT

BERT is published by researchers at Google AI in 2018. It is regarded as a milestone in the NLP community by proposing a bidirectional Language model based on Transformer.

BERT is the notable example of AE, 15% of tokens are replaced by a special symbol [MASK], and the model is trained to reconstruct the original sequence from the masked tokens. By contrast with previous efforts that looked at a text sequence either from left to right(RNN) or combined left-to-right and right-to-left training (ELMO), the Transformer Encoder utilizes bidirectional contexts simutaneously. Therefore BERT uses the Transformer Encoder as the structure of the pre-train model and addresses the unidirectional constraints by proposing a new pre-training objective: the “masked language model”(MLM).

In conclusion, BERT uses masked language models to enable pre-trained deep bidirectional representations and after fine-tuning based the representation, BERT advances state-of-the-art performance for eleven NLP tasks. 

### Input Representation of BERT

The input representation is able to unambiguously represent both a single text sentence or a pair of text sentences in one token sequence. For a given token, its input representation is constructed by summing the corresponding token, segment and position embeddings, @bert.

```{r ch02-03-figure02, echo=FALSE, out.width="80%", fig.cap="BERT input representation", fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/bert_input_representation.png")
```

Figure \@ref(fig:ch02-03-figure02) is the visual representation of input representation. The specifics are:

* Use WordPiece embeddings @wu2016google with a 30,000 token vocabulary and split word pieces denoted with ##. e.g.,[playing = play and ##ing ]

* The first token of every sequence is always the special classification embedding [CLS]. For non-classification tasks, this vector is ignored.  

* Sentence pairs are packed together into a single sequence and are separated with a special token [SEP]. Second, different learned sentence embedding[e.g.,A and B] will be added to every token of each sentence.For single-sentence inputs we only use the sentence A embeddings.

### Masked Language Model  

```{r ch02-03-figure03, echo=FALSE, out.width="70%", fig.cap="BERT Masked Language Model  \n Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-bert/",, fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/bert_masked_task.png")
```

Figure \@ref(fig:ch02-03-figure03) is the visual representation of Masked Language Model.
As shown in the above figure, when word embeddings are fed into BERT, 15% of the words in each sequence will be replaced by a special token [MASK]. The task of MLM is to predict the original value of masked tokens. The output of MSM is the word embeddings of corresponding tokens, then feed the word embedding of [MASK] token into a simple softmax classifier and get the final prediction of [MASK] token. And the most important part is the "yellow" block, it's basically a multi-layer bidirectional Transformer encoder based on implementation described in @kaiser2017one.

* What is the Masked Language Model? 
  * 15% of all WordPiece tokens in each sequence will be randomly masked.
  * Input: token embedding(one sentence, begin with [CLS])
  * Output: BERT token embedding.
  * Using softmax(a simple linear classifier) to predict the masked token. (using token [CLS] to do classification task.)
  (words match each other may have the same BERT embedding)
* How to mask? 
  * 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy $\Rightarrow$ my dog is [MASK]
  * 10% of the time: Replace the word with a random word,e.g., my dog is hairy $\Rightarrow$ my dog is apple.
  * 10% of the time: Keep the word unchanged, e.g., my dog is hairy $\Rightarrow$ my dog is hairy.
* Why there are two other methods to replace the word?
  * Why keep 10% of masked tokens unchanged?  
    A: In some downstream task like POSTagging, all the tokens are known, if BERT only trained the Masked sequences, then the model only use the information of context, exclude the information of the masked words. It will lose a part of the information, then weak the performance of the model.
  * Why replace 10% of masked tokens with random words?  
    A: Since we keep 10% of masked token unchanged, if we do not add random noise, the model will be "lazy" in our training, the model will plagiarize current tokens, rather than learning.

### Next-sentence Tasks

```{r ch02-03-figure04, echo=FALSE, out.width="70%", fig.cap="BERT Next-sentence Tasks  \n Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-bert/",, fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/bert_nsp.png")
```
Figure \@ref(fig:ch02-03-figure04) is the visual representation of Next-sentence Tasks(NST).
Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. So BERT proposes the NST by using the special token [CLS] as the first token of every sequence.

* What is Next-sentence Tasks?
  * Input: token embedding (two sentence, begin with [CLS], each sentence ends with [SEP])
  * Output: BERT token embedding.
  * Using softmax (a simple linear classifier) to explain the relationship between two sentences.
  * Using [CLS] token to pre-train a binary classification tasks.
  * Sentences can be trivially generated from monolingual corpus.
  * Choose sentences A and B for each example, 50% of B is actual next sentence that follows A, and 50% of B is a random sentence from the corpus.
* For example:
  * Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk[SEP]  
  Label = IsNext.
  * Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP].  
  Label = NotNext  

### Pre-training Procedure of BERT

For the pre-training corpus, BERT use the concatenation of BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words) to creat two version of BERT (L the number of layers, H the hidden size, A the number of self-attention heads):

* BERT-Base: L = 12, H = 768, A = 12, Total parameters = 110M
* BERT-Large: L = 24, H = 1024, A = 16, Total parameters = 340M

### Fine-tuning Procedure of BERT

```{r ch02-03-figure05, echo=FALSE, out.width="60%", fig.cap="BERT Task Specific Models", fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/bert_based_model.png")
```

As shown in Figure \@ref(fig:ch02-03-figure05), different types of tasks require different modifications to the model, and the modification of the model before fine-tuning is quite simple. For example, for the sequence-level classification problem (such as sentiment analysis, Task (a) in the figure), take the output representation of the first token[CLS] and feed it to a softmax to get the classification result. As for token-level classification (e.g. NER, Task (d) in the figure), take the output of the last layer transformer of all tokens and feed it to the softmax layer for classification.

### Feature Extraction 

Like other Language model, the pre-trained BERT can create contextualized word embeddings. Then the word embeddings can be used to the existing model. Readers can try out BERT through [BERT FineTuning with Cloud TPUs](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb).


### BERT-like models

The state-of-the-art performance of BERT reveals the deep bidirectional language model can significantly improve the model performance in NLP tasks, and BERT chart a new course that how a real bidirectional model should be. However, BERT has also the following weaknesses: first of all, the input to BERT contains artificial symbols like [MASK] that never occur in downstream tasks, which creates a pre-train-fine-tunning discrepancy problem. Secondly, BERT assumes the predicted tokens are independent of others given the unmasked tokens, which is oversimplified for natural language. Several models are inspired by BERT to solve these problems.

Roberta @liu2019roberta shows hyperparameter choices have a significant impact on the final results. It improves BERT pre-training in the following aspects to get better performance:

* Changing the input embedding to Byte Pair Encoding (BPE) @sennrich2015neural.
* Using dynamic masking: each train has different training data.
* Using full sentence without NSP.
* More Data, larger batch size (8k), and longer training (100k to 300k steps).

ALBERT @lan2019albert mainly makes three improvements to BERT, which reduces the overall parameter amount, accelerates the training speed, and improves the model performance under the same training time.

* Using factorized embedding parameterization.
* Cross-layer parameter sharing, which significantly reduces the number of parameters.
* Replacing NSP with Sentence-order prediction loss (SOP).

There are also BERT-like models pre-trained on domain-specific corpora, SciBERT @beltagy2019scibert on scientific publications ,ERNIE @zhang2019ernie on a large corpus incorporating knowledge graph in the input. In comparison to fine-tune original BERT, training on the domain-specific corpora then fine-tuning them on downstream NLP tasks has shown to yield better performance.

## Generative Pre-Training(GPT-2)

### Auto-regressive Language Model(AR)

```{r ch02-03-figure06, echo=FALSE, out.width="50%", fig.cap="Autoregressive", fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/autoregressive.png")
```

Figure \@ref(fig:ch02-03-figure06) shows the modelling of Auto-regressive language model, it tries to estimate the probability distribution of a sequence with a auto-regressive pattern. Specifically, given a text sequence $X = (x_1,...,x_T)$, AR language model factorizes the log-likelihood into a forward sum $logp(x) = \sum^T_{t=1} p(x_t|x<t)$ or a backward one $logp(x) = \sum^{t=1}_{T} p(x_t|x>t)$ @yang2019xlnet. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information.

### Introduction of GPT-2

GPT-2 is proposed by researchers at OpenAI in 2019. It captures the attention of the NLP community for the following characters:  
First of all, instead of the fine-tuning model with specific tasks, GPT-2 demonstrates language models can perform down-stream tasks in a zero-shot setting, which means without any parameter or architecture modification. Secondly, in order to perform better under the zero-shot setting, GPT-2 becomes extremely large. On the other hand, training GPT-2 also needs enormous data, so researchers also create a new dataset "WebText", which contains millions of webpages.  
With the characters above, GPT-2 achieves state-of-the-art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText.

Readers can experiment with GPT-2 by using [AllenAI GPT-2](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is). You can input a sentence and see the prediction of next words.

### Input Representation of GPT-2

GPT-2 creats a dataset called ["WebText"](https://skylion007.github.io/OpenWebTextCorpus/), contains the text subset of 45 million links, which all curated by humans. All results presented in papaer use a preliminary version of WebText which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40GB of text @radford2019gpt2.

```{r ch02-03-figure07, echo=FALSE, out.width="70%", fig.cap="GPT-2 Input Representation  \n Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-gpt2/",, fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/gpt_input_representation.png")
```

Figure \@ref(fig:ch02-03-figure07) shows the input representation of GPT-2. The input sequence has a start token [S], and input embedding of each token is the corresponding specially designed Byte Pair Encoding (BPE) @sennrich2015neural, adding up the positional encoding vector.


### The Decoder-Only Block

```{r ch02-03-figure08, echo=FALSE, out.width="70%", fig.cap="GPT-2 Model  \n Alammar, Jay (2018). The Illustrated GPT-2 co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-gpt2/",, fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/gpt_decoder.png")
```

Figure \@ref(fig:ch02-03-figure08) shows the model of GPT-2. This model essentially is the Transformer decoder, except they threw away the second self-attention layer. In each decoder, Layer normalization @ba2016layer was moved to the input of each sub-block, similar to a pre-activation residual network @he2016identity and an additional layer normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used @radford2019gpt2.

GPT-2 reads from the start token [s] till the last predicted token to predict the next token. For example, in the first round, model uses [s] to predict [robot], in the next round the input is updated as {[s],[robot]} since [robot] has been predicted. This is how Masked self-Attention in Decoder block works.

Otherwise, since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed.
The model of GPT-2 should be :
$$log p(x)=log\sum^{n}_{i=1}p(s_n|s_1,...,s_{n-1};task_i)$$

For example, a translation training example can be written as the sequence (translate to french, English text, French text). Likewise, a reading comprehension training example can be written as (answer the question, document, question, answer).

By using the specially designed WebText, GPT-2 can be used by following patterns for different tasks.

* Reading Comprehension: data sequence, "Q:", question sequence, "A:"

* Summarization: data sequence, "TL;DR:"

* Translation: English sentence 1 = "French sentence 1, English sentence 2 = French sentence 2, English sentence 3 = "

### GPT-2 Models

```{r  ch02-03-table01, echo=FALSE }
gpt_size <- data.frame("Parameters"= c("117M","345M","762M","1542M"),
                        "Layers"= c(12,24,36,48),
                        "Dimensionlality" = c(768,1024,1280,1600))


knitr::kable(gpt_size,caption = "GPT-2 models size" )
```

Four versions models are trained, the architectures are summarized in \@ref(tab:ch02-03-table01). The smallest model is equivalent to the original GPT, and the second smallest equivalent to the largest model from BERT @bert. The largest model is called GPT-2, which has 1.5 billion parameters.

### Conclusion

The framework of GPT-2 is the combination of pre-training based on Transformer Decoder and fine-tuning based on unsupervised downstream tasks.

After the great success of bidirectional models like BERT, GPT-2 insists on using unidirectional models and still achieves state-of-the-art performance. It proves that the performance of language models can be significantly improved by simply increasing the size of training datasets, which is exactly what GPT-2 did and even GPT-2, which has 1.5 billion parameters, still underfits WebText. This result suggests that datasets are as important as models.

## XLNet

### Introduction of XLNet

```{r ch02-03-figure09, echo = FALSE, fig.cap="AR Language Modeling  and AE", out.width = "90%",fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/xlnet_twomodels.png")
```
XLNet is proposed by researchers at Google in 2019. Since the autoregressive language model(e.g.GPT-2) is only trained to encode a unidirectional context and not effective at modeling deep bidirectional contexts and autoencoding(e.g.BERT) suffers from the pretrain-finetune discrepancy, XLNet borrows ideas from the two types of objectives while avoiding their limitations. It is a new objective called Permutation Language Modeling. By using a permutation operation during training time, bidirectional context information can be captured and makes it a generalized order-aware autoregressive language model. Besides, XLNet introduces a two-stream self-attention to solve the problem that standard parameterization will reduce the model to bag-of-words.

Two XLNet are released, i.e. XLNet-Base and XLNet-Large, and include the similar settings of corresponding BERT. Empirically, XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.

### Permutation Language Modeling(PLM)

Now Figure \@ref(fig:ch02-03-figure010) illustrates the permutation language modeling objective.

```{r ch02-03-figure010, echo = FALSE, fig.cap="Illustration of the permutation language modeling objective for predicting x3 given the same input sequence x but with different factorization orders.", out.width = "70%",fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/xlnet_pml.png")
```

Specifically, for a sequence $X$ of length $T$, there are $T!$ different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides. Let $P_T$ be the set of all possible permutations of a sequence [1,2,..., T] and use $z_t$ and $z_{<t}$ to denote the t-th element and the first t−1 elements of a permutation $p\in P_T$. Then the permutation language modeling objective can be expressed as follows:

\begin{equation}
\max_{\theta} \mathbb{E}_{p\sim P_T} \left[\sum_{t=1}^Tlogp_{\theta}(x_{z_t|x_{z_{<t}}})\right]
\end{equation}

For instance, when we have a factorization order: { New York is a city }. The probability of sequence can be expressed as follows:

\begin{equation}
\begin{aligned}
P(New, York, is, a, city) = P(New) * P(York | New) * P(is | New, York) * \\  P(a | New, York, is) * P(city | New, York, is, a)
\end{aligned}
\end{equation}

As for another factorization order: {city a is New York}, then the probability of sequence can be expressed differently:

\begin{equation}
\begin{aligned}
P(New, York, is, a, city) = P(city) * P(a | city) * P(is | city, a) * \\ P(New | city, a, is) * P(York | city, a, is, New)
\end{aligned}
\end{equation}


It is noteworthy that sequence order is not shuffled and only attention masks are changed to reflect factorization order. With PLM, XLNet can model bidirectional context and the dependency within each token of the sequence.

### The problem of Standard Parameterization
The Standard Parameterization can be expressed as follows:

$$p_{\theta}(X_{p_t}=x|x_{p<t})=\frac{e(x)^Th_{\theta}(x_{p<t})}{\sum_{x^{'}}e(x^{'})^Th_{\theta}(x_{p<t})}$$
where $e(x)$ denotes the embedding of input token and $h_{\theta}(x_{p<t})$ denotes the hidden representation of $x_{p<t}$.

While the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. Specifically, let’s consider two different permutations $p^1\text{:{New York is a city}}$ and $p^2:\text{{New York a city is}}$. The probability of $\text{{is}}$ in $p^1$: $\text{P(is|New, York)}$ and the probability of $\text{{a}}$ in $p^2$: $\text{P(a|New, York)}$ are identical. The model will be reduced to predicting a bag-of-words, because $h_{\theta}(x_{z<t})$ does not contain the position of the target.

XLNet resolves the problem by reparameterizing with positions:

$$p_{\theta}(X_{p_t}=x|x_{p<t})=\frac{e(x)^Tg_{\theta}(x_{p<t},p_t)}{\sum_{x^{'}}e(x^{'})^Tg_{\theta}(x_{p<t},p_t)}$$
where $e(x)$ denotes the embedding of input token and $g_{\theta}(x_{p<t},p_t)$ denotes the hidden representation of $x_{p<t}$ and position $p_t$.

but reparameterization with positions brings another contradiction @yang2019xlnet:  
(1) To predict the token $x_{p_t}$ , $g_{\theta}(x_{p<t},p_t)$ should only use the position $p_t$ and not the content $x_{p_t}$, otherwise the objective becomes trivial.  
(2) To predict the other tokens  $x_{p_j}$ with j > t, $g_{\theta}(x_{p<t},p_t)$ should also encode the content $x_{p_t}$ to provide full contextual information.

XLNet proposes the Two-Stream Self-Attention to resolve the contradiction.

### Two-Stream Self-Attention

```{r ch02-03-figure011, echo = FALSE, fig.cap="Two-Stream Self-Attention", out.width = "90%",fig.align="center"}
knitr::include_graphics("figures/02-03-transfer-learning-for-nlp/xlnet_ts.png")
```

Instead of one, two sets of hidden representation are proposed:

* The content representation $h_{\theta}(x_{p \leq t})$, this representation encodes both the context and $x_{p_t}$ itself.
* The query representation $g_{\theta}(x_{p<t},p_t)$, which only has information $x_{p<t}$ and the postion $p_t$ but not the content $x_{p_t}$.

Figure \@ref(fig:ch02-03-figure011) is an example with the Factorization order: $3,2,4,1$: 

* $h_i^{(t)}$ denote the content representation of the i-th token in the t-th layer of self-attention. It is the same as the standard self-attention. For instance, $h_1^{(1)}$ can see all the $h_i^{(0)}$ since the 1-st token is after token ${3,2,4}$.

* $g_i^{(t)}$ denote the query representation of the i-th token on the t-th layer of self-attention.it does not have access information about the content $x_{p_t}$, otherwise trivial. 

* Computationally, $h_i^{(0)}$ is the word embeddings, and $g_i^{(0)}$ is a trainable parameter initialized with a trainable vector. Only $h_i^{(t)}$ is used during fine-tuning. The last $g_i^{(t)}$ is used for optimizing the LM loss. A self-attention layer $m=1,...,M$ are schematically updated with a shared set of parameters as follows:
$$
h_{p_t}^{m} \leftarrow Attention (Q=h_{p_t}^{m-1}, KV=h^{(m-1)}_{p \leq t};\theta) 
\text{  (content stream: use both $p_t$ and $x_{p_t}$)} 
\\
g_{p_t}^{m}\leftarrow Attention (Q=g_{p_t}^{(m-1)}, KV=h^{(m-1)}_{p<t};\theta) 
\text{  (query stream: use $p_t$ but cannot see }x_{p_t})
$$ 
where Q, K, V denote the query, key, and value in an attention operation @vaswani2017attention.
More details are included in Appendix A.2 for reference @yang2019xlnet.

### Partial Prediction

While the PLM has several benefits, optimization is challenging due to the permutation operator. To reduce the optimization difficulty, XLNet only predicts the last tokens in a factorization order. It sets a cutting point $c$ and split the permutation $p$ into a non-target subsequence $p_{\leq c}$ and a target subsequence $p_{>c}$. The objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e., 

\begin{equation}
\max_{\theta}   \mathbb{E}_{p\sim P_T} \left[logp_{\theta}(x_{z_{>c}|x_{z_{\leq c}}})\right] = 
\mathbb{E}_{p\sim P_T} \left[\sum_{t=c+1}^{|z|} logp_{\theta}(x_{z_t|x_{z_{\leq t}}})\right]
\end{equation}
For unselected tokens, their query representations need not be computed, which saves speed and memory. XLNet incorporates Ideas from Transformer-XL and inherits two important characters of it, Segment-Level Recurrence and Relative Position Encoding, to enable the learning of long-term dependency and resolve the context fragmentation.


### XLNet Pre-training Model

After tokenization with SentencePiece @kudo2018sentencepiece, Researchers obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total, to pre-train the XLNet.

Analogous to BERT, two versions of XLNet have been trained:

* XLNet-Base: L = 12, H = 768, A = 12, Total parameters = 110M (on BooksCorpus and Wikipedia only)
* XLNet-Large: L = 24, H = 1024, A = 16, Total parameters = 340M (on total datasets)


### Conclusion 

Language modeling has been a rapidly developing research area. However, most language modelings are unidirectional but BERT @bert shows that bidirectional modeling can significantly improve model performance. So there has been a gap between language modeling and pre-training due to the lack of the capability of bidirectional context modeling. With the permutation operator, XLNet generalizes language modeling. Now the unidirectional language modelings can become bidirectional modeling with a permutation operator. Overall, XLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods.

## Latest NLP models

Nowadays NLP has become a competition between big companies. When BERT first came, people talked about it may cost thousands of dollars to train it. Then came GPT-2, which has 1.5 billion parameters and is trained on 40GB data. As I mentioned above, GPT-2 of Open-AI shows that increasing the size of models and datasets is at least as important as proposing a new model architecture.  

After GPT-2, researchers at Google did the same thing, they proposed a general language model called T5 @raffel2019exploring, which is trained on 750GB corpus - ["C4 (Colossal Clean Crawled Corpus)"](https://www.tensorflow.org/datasets/catalog/c4). If you read the paper, the last page of it is a table of several experience results, which may cost millions of dollars to reproduce it.  

On 28th May 2020, the "Arms race" goes into another level, GPT-3 @brown2020language emerged, the new paper takes GPT to the next level by making it even bigger - GPT-3 has 175 billion parameters and is trained on a dataset that has 450 billion of tokens. GPT-3 experiments with the three different settings: zero-shot, one-shot, and Few-shot to show that scaling up language models can greatly improve few-shot performance, sometimes even reaching competitiveness with prior SOTA approaches. However, it is conservatively estimated that training GPT-3 will cost one hundred million.

Models like T5 and GPT-3 are very impressive, but the biggest problem at the moment is to find a way to make the current model put into use in industry. If it can't bring benefits, the AI industry can't be sustained by burning money. As for researchers, the truth is, it may not work with resources, but it is certainly not possible without resources now.