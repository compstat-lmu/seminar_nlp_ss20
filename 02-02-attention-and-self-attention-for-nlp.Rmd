# Attention and Self-Attention for NLP

*Authors: Joshua Wagner*

*Supervisor: Matthias AÃŸenmacher*

Both attention and self-attention were important for the advances made in NLP.
The first part of this chapter is an overview of attention as it is a building block for self-attention.
The second part focuses on self-attention which enabled the commonly used models
for transfer learning that are used today.

## Attention
In this part of the chapter, we revisit the Encoder-Decoder architecture that was introduced
in chapter [3](01-02-rnns-and-their-applications-in-nlp). We focus on the improvements
that were made with the development of attention mechanisms on the example of neural machine translation (nmt).

As seen in chapter [3](01-02-rnns-and-their-applications-in-nlp), traditional early
encoder-decoder architecture passes the last hidden state of the encoder to the decoder.
This leads to the problem that information is lost in long input sequences.
Especially information found early in the sequence tends to be "forgotten" after
the entire sequence is processed. The addition of bi-directional layers remedies
this by processing the input in reversed order. While this helps for shorter sequences,
the problem still persists for mid sections of very long input sequences.
The development of attention enables the decoder to attend to the whole sequence and
thus use the context of the entire sequence during decoding step.

### Bahdanau-Attention

In 2015, @bahdanau2014neural proposed attention to fix the information problem that
the before seen encoder-decoder architecture faced. Early decoders are trained to predict $y_{t'}$
given a context vector $c$ and all earlier predicted words $\{y_t, \dots, y_{t'-1}\}$.
$$c=q(\{h_1,\dots,h_T\})$$ where $h_1,\dots,h_T$ are the the hidden states of the encoder for the input sequence
$x_1,\dots, x_T$ and $q$ is a non-linear function. @sutskever2014sequence for example used
$q(\{h_1,\dots,h_T\}) = h_T$ as their non-linear transformation which remains a popular
choice for architecture without attention.

Attention changes the context vector $c$ that a decoder uses for translation from a fixed
length vector $c$ of a sequence of hidden states $h_1, \dots, h_T$ to a sequence
of context vectors $c_i$. The hidden state $h_i$ has a strong focus on the *i*-th
word in the input sequence and its surroundings.
Each $h_i$ is computed by a concatenation of the forward
$\overrightarrow{h_i}$ and backward $\overleftarrow{h_i}$ hidden states of the
bi-directional encoder.

$$
h_i = [\overrightarrow{h_i}; \overleftarrow{h_i}], i = 1,\dots,n
$$
These new context vectors $c_i$ are used in the computation of the the hidden state of the decoder $s_t$.
At time-point $t$ it is computed as $s_t = f(s_{t-1},y_{t-1},c_t)$.
The context vector $c_t$ is computed as a weighted sum of the hidden
states $h_1,\dots, h_{T_x}$:

$$
c_t = \sum^{T_x}_{i=1}\alpha_{t,i}h_i.
$$
Each hidden state $h_i$ is weigthed by a $\alpha_{t,i}$.
The weight $\alpha_{t,i}$ of each hidden state $h_i$ is also called the alignment score.
These alignment scores are computed as:

$$
\alpha_{t,i} = align(y_t, x_i) =\frac{exp(score(s_{t-1},h_i))}{\sum^{n}_{i'=1}exp(score(s_{t-1},h_{i'}))}
$$
with $s_{t-1}$ being the hidden state of the decoder at time-step $t-1$.
The alignment score $\alpha_{t,i}$ models how well input $x_i$ and output $y_t$ match
and assigns the weight to $h_i$. @bahdanau2014neural parametrize their alignment
score with a single-hidden-layer feed-forward neural network which is jointly
trained with the other parts of the architecture. The score function used by Bahdanau et al.
is given as

$$
score(s_t,h_i) = v_\alpha^Ttanh(\mathbf{W}_\alpha[s_t;h_i])
$$
were tanh is used as a non-linear activation function and $v_\alpha$ and $W_\alpha$
are the weight matrices to be learned by the alignment model. The alignment score function
is called "concat" in @luong2015effective and "additive attention" in @vaswani2017attention
because $s_t$ and $h_i$ are concatenated just like the forward and backward hidden states seen above.
A nice by-product of attention is a matrix of alignment scores
which can be visualised to show the correlation between source and target words.

```{r attention-plot-bahdanau, echo = FALSE, fig.align='center', fig.cap= "Alignment Matrix visualised for a French to English translation. Image source: Fig 3 in @bahdanau2014neural", out.width="100%"}
knitr::include_graphics('./figures/02-02-attention-and-self-attention-for-nlp/bahdanau-fig3.png')
```

The attention model proposed by Bahdanau et al. is also called a soft/global attention model as it attends
to every input in the sequence.

### Luong-Attention

While Bahdanau et al. were the first to use attention in neural machine translation (NMT),
@luong2015effective were the first to explore different attention mechanisms and their impact on
NMT. Although the architecture of the neural network proposed by Luong et al. differes,
the overall attention mechanism is similar to Bahdanau et al..
The main difference in the attention mechanism is in the used score function.
Bahdanau et al. only consider
$$
score(s_t,h_i) = v_\alpha^Ttanh(\mathbf{W}_\alpha[s_t;h_i])
$$
, while Luong et al. also use a general, a location-based and a dot-product score function in their publication.

| Score-function | Name           |
|----------------|----------------|
| $\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \boldsymbol{s}_t^\top\mathbf{W}_a\boldsymbol{h}_i$         | General        |
| $\alpha_{t,i} = \text{softmax}(\mathbf{W}_a \boldsymbol{s}_t)$        | Location-based |
| $\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \boldsymbol{s}_t^\top\boldsymbol{h}_i$        | Dot-product    |

Table : (\#tab:luong-score-functions) Different score function proposed by Luong et al.

Other differences are simplifications and generalisations from @bahdanau2014neural.
As @luong2015effective don't use a bidirectional encoder, they simplify the hidden
state of the encoder from a concatination of both forward and backward hidden states
to only the hidden state at the top layer of both encoder and decoder.


Most until now proposed attention models attend the entire input sequence. While
this fixes the problem of forgetful sequential models discussed in the beginning of the chapter,
it also has the drawback that it is expensive and can potentially be impractiacal
during e.g. translation of entire paragraphs or documents. These problems encountered
with global or soft attention mechanisms can be mitigated with a local or hard attention
approach. While it was used by @xu2015show for caption generation of images with a CNN
and by @gregor2015draw for the generation of images, the first application for NMT
is from @luong2015effective. Other than the global attention mechanism, the local
attention mechanism first generates an aligned position $p_t$ at timestep $t$.
The context vector is then computed as a weighted average over only the set of
hidden states in a window $[p_t-D,p_t+D]$ with $D$ being an emipirically selected
parameter. This constrains the above introduced computation for the context vector
to:
$$
c_t = \sum^{p_t+D}_{i=p_t-D}\alpha_{t,i}h_i.
$$

The parts outside of a sentence are ignored if the window crosses sentence
boundaries. The computation of the context vector changes very little compared to
the global model which can be seen in Figure \@ref(fig:attention-plots-luong).
This is only the case if the first variant proposed by Luong et al., the *monotonic* alignment(**local-m**), is used.
This approach sets $p_t= t$ with the assumption that both input and output sequences
are roughly monotonically aligned.

```{r attention-plots-luong, echo = FALSE, fig.align='center', fig.cap= "Global and local attention illustrated. Image source: Fig. 2 and 3 in @luong2015effective", out.width="100%"}
knitr::include_graphics('figures/02-02-attention-and-self-attention-for-nlp/luong2015-fig2-3.png')
```

The other approach, *predictive* alignment (**local-p**), predicts the aligned position:
$$
p_t = S \cdot sigmoid(v_p^\top tanh(W_ph_t))
$$
where $W_p$ and $v_p$ are the parameters that will be trained to predict positions.
$S$ is the length of the input sentence and as a result of the sigmoid function: $p_t \in [0,S]$.
A Gaussian distribution centered around $p_t$ is placed by @luong2015effective to favour alignment points closer to $p_t$.
This changes the alignment weights to:
$$
\alpha_{t,i} = align(y_t, x_i)exp(-\frac{(i-p_t)^2}{2\sigma^2})
$$
where the standard deviation is empirically set to $\sigma = \frac{D}{2}$. This
utilization of $p_t$ to compute $\alpha_{t,i}$ allows the computation of backpropagation
gradients for $W_p$ and $v_p$ and is thus ``differentiable almost everywhere'' @luong2015effective.

### Attention Models

Attention was in the beginning not directly developed for RNNs or even NLP.
Attention was first introduced in @GravesWD14 with a content-based attention mechanism
($\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \text{cosine}[\boldsymbol{s}_t, \boldsymbol{h}_i]$)
for Neural Turing Machines. Their application for NLP related tasks were later developed by 
@luong2015effective, @bahdanau2014neural and @xu2015show. 
The first differentiation between global/soft and local/hard attention was by @xu2015show in the
context of Neural Image Caption with both mechanisms being close to what was used
by Luong et al. in the previous section. Global and local attention mechanisms are
two categories of attention mechanism that contained all possible attention mechanisms
before @cheng2016long. They first introduced the concept of self-attention, the third
category of attention mechanisms.

## Self-Attention

@cheng2016long implement self-attention with a modified LSTM unit, the Long Short-Term
Memory-Network (LSTMN). The LSTMN replaces the memory cell with a memory network to enable
the storage of ``contextual representation of each input token with
a unique memory slot and the size of the memory
grows with time until an upper bound of the memory
span is reached'' @cheng2016long.
Self-Attention, as the name implies, allows an encoder to attend to other parts of the input during processing as seen in Figure \@ref(fig:self-attention-cheng).


```{r self-attention-cheng, echo = FALSE, fig.align='center', fig.cap= "Illustration of the self-attention mechanism. Red indicates the currently fixated word, Blue represents the memories of previous words. Shading indicates the degree of memory activation. Image source: Fig. 1 in @cheng2016long.", out.width="100%"}
knitr::include_graphics('figures/02-02-attention-and-self-attention-for-nlp/cheng2016-fig1.png')
```

While the LSTMN introduced self-attention, it retains the drawbacks that come from
the use of a RNN. @vaswani2017attention propose the Transformer architecture which uses
self-attention extensivly to circumvent these drawbacks.

### The Transformer

RNNs were, prior to Transformers, the state-of-the-art model for machine translation, language modeling
and other NLP tasks. But the sequential nature of a RNN precludes parallelization within
training examples. This becomes critical at longer sequence lengths as memory constraints
limit batching across examples. While much has been done to minimize these problems,
they are inherent in the architecture and thus still remain. The attention mechanisms
allow the modeling of dependencies without regard for the distance in either input
or output sequences. Most attention mechanisms,
as seen in the previous sections of this chapter, use recurent neural networks.
This limits their usefullness for transfer learning because of the previously mentioned
constraints that recurent networks have. Models like ByteNet from @kalchbrenner2016neural
and ConvS2S from @gehring2017convolutional alleviate the problem with sequential models
by using convolutional neural networks as basic building blocks. ConvS2S has a
linear increase in number of operations to relate signals from two arbitrary
input or output positions with growing distance. ByteNet has a logarithmical increase
in number of operations needed. The Transformer architecture from @vaswani2017attention
achieves the relation of two signals with arbitrary positions in input or output
with a constant number of operations. It was also the first model that relied entirely
on self-attention for the computation of representations of input or output without
using sequence-aligned recurent networks or convolutions. 

While the Transformer architecture doesn't use recurent or convolutional networks,
it retains the popular encoder-decoder architecture.



```{r encoder-transformer, echo = FALSE, fig.align='center', fig.cap= "Encoder of a Transformer, Original image: @vaswani2017attention,\\ Additions and cropping: @weng2018attention", out.width="50%"}
knitr::include_graphics('figures/02-02-attention-and-self-attention-for-nlp/transformer-encoder.png')
```

The encoder is composed of a stack of N = 6 identical layers. Each of these layers
has two sub-layers: A mulit-head self-attention mechanism and a position-wise fully
connected feed-forward network. Each of the sub-layers has residual connection around
its main component which is followed by a layer normalization. The output of each sub-layer
is $\text{LayerNorm}(x + \text{Sublayer}(x))$ where $\text{Sublayer}(x)$ is the output of the
function of the sublayer itself. All sub-layers and the embedding layer before the
encoder/decoder produce outputs of $dim = d_{model} = 512$ to allow these residual connections to work.
The position-wise feed-forward network used in the sublayer is applied to each position
seperatly and identically. This network consists of two linear transformations with
a ReLU activation function in between:
$$
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
$$



```{r decoder-transformer, echo = FALSE, fig.align='center', fig.cap= "Decoder of a Transformer, Original image: @vaswani2017attention,\\ Additions and cropping: @weng2018attention", out.width="50%"}
knitr::include_graphics('figures/02-02-attention-and-self-attention-for-nlp/transformer-decoder.png')
```

The decoder is, as seen in Figue \@ref(fig:decoder-transformer), composed of a stack of $N = 6$ identical layers.
It inserts, in addition to the two already known sub-layers from the encoder,
a third sub-layer which performs multi-head attention over the output of the encoder stack.
It uses, same as the encoder, residual connections around each of the sub-layers.
The decoder also uses a modified, masked self-attention sub-layer ``to prevent positions
from attending to subsequent positions'' @vaswani2017attention. This, coupled with the
fact that the output embeddings are shifted by one positon to the right ensures that
the predicitons for position $i$ only depend on previous known outputs.

```{r transformer-full, echo = FALSE, fig.align='center', fig.cap= "The Transformer-model architecture, Image Source: Fig. 1 in @vaswani2017attention", out.width="70%"}
knitr::include_graphics('figures/02-02-attention-and-self-attention-for-nlp/transformer-full-model.png')
```

As seen in \@ref(fig:transformer-full), the Transformer uses positional encodings
added to the embeddings so the model can make use of the order of the sequence.
@vaswani2017attention use the sine and cosine function of different frequencies:
$$
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})
$$
For further reasoning why these functions were chosen see @vaswani2017attention.

#### The self-attention mechanism(s)

@vaswani2017attention describe attention functions as "mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key". The *Q*uery and the *K*ey-*V*alue pairs are used in the
newly proposed attention mechanism that is used in Transformers.
As seen in Figures \@ref(fig:encoder-transformer), \@ref(fig:decoder-transformer)
and \@ref(fig:transformer-full), the Transformer uses an attention mechanism called
``Multi-Head Attention''.

```{r multi-head-attention, echo = FALSE, fig.align='center', fig.cap= "Multi-Head Attention, Image Source: Fig. 2 in @vaswani2017attention", out.width="50%"}
knitr::include_graphics('figures/02-02-attention-and-self-attention-for-nlp/multi-head-attention.png')
```

The multi-head attention projects the queries, keys and values $h$ times instead of performing
a single attention on $d_{model}$-dim. queries and key-value pairs. The projections
are learned, linear and project to $d_k$, $d_k$ and $d_v$ dimensions. Next the
new **scaled dot-product attention** is used on each of these to yield a $d_v$-dim. output.
These values are then concatenated and projected to yield the final values as can be
seen in \@ref(fig:multi-head-attention). This multi-dimensionalty allows the attention
mechanism to jointly attend to different information from different representation
at different positions. The multi-head attention can be written as:
$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\dots, \text{head}_h)W^O
$$
$$
\text{where } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$
and projections are the parameter matrices $W_i^Q\in \mathbb{R}^{d_{model}\times d_k}, W_i^K\in \mathbb{R}^{d_{model}\times d_k}, W_i^V\in \mathbb{R}^{d_{model}\times d_v}\text{ and } W^O\in \mathbb{R}^{hd_{v}\times d_{model}}$.
@vaswani2017attention use $h = 8$ and $d_k = d_v = d_{model}/h = 64$. This reduced dimensionality
leads to a reduction in computational cost that is similar to that of a single-head attention
with the full ininital dimensionality of $512$.

The *scaled dot-product attention* is, as the name suggests, just a scaled version
of the dot-product attention seen in previously in this chapter. 

```{r scaled-dot-prod-attention, echo = FALSE, fig.align='center', fig.cap= "Scaled Dot-Product Attention, Image Source: Fig. 2 in @vaswani2017attention", out.width="50%"}
knitr::include_graphics('figures/02-02-attention-and-self-attention-for-nlp/scaled-dot-prod-attention.png')
```

The optional *Mask*-function seen in Fig. \@ref(fig:scaled-dot-prod-attention) is
only used in the masked-multi-head attention of the decoder. The querys and
keys are of dim. $d_k$ and the values are of dim. $d_v$. The attention
is for practical reasons computed for a set of queries, *Q*. The keys and values
are thus also used in matrix format, *K* and *V*. The matrix of outputs is then
computed as:
$$
\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^\top}{\sqrt{d_k}})V
$$
where $\text{Attention}(Q,K,V)$ corresponds to an non-projected head of multi-head attention.


explain the differences in computational cost to rnns and conv. models