---
title: "Introduction to transfer learning"
output: 
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## What is Transfer Learning?

```{r figure01, echo=FALSE, out.width="70%", fig.cap="Traditional ML vs. Transfer Learning", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/compare-classical-transferlearning-ml.PNG")
```

## Attention
```{r figure02, echo=FALSE, out.width="50%", fig.cap="Global attention model, Loung et al. 2015", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/Depiction-of-Global-Attention-in-an-Encoder-Decoder-Recurrent-Neural-Network.png")
```

## (Self-)attention

```{r figure03, echo=FALSE, out.width="70%", fig.cap="Transformer example, Alammar, Jay (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/The_transformer_encoder_decoder_stack.png")
```

## (Self-)attention
```{r figure04, echo=FALSE, out.width="70%", fig.cap="Transformer Encoder Layer, \n Alammar, Jay (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/encoder_with_tensors.png")
```

## Overview over important NLP models

```{r figure05, echo=FALSE, out.width="70%", fig.cap="Transfer learning overview", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/overview-tranferlearning.PNG")
```

## BERT

```{r figure06, echo=FALSE, out.width="70%", fig.cap="BERT structure, \n Alammar, Jay (2018). The Illustrated BERT, ELMo, and co. [Blog post]. Retrieved from http://jalammar.github.io/illustrated-bert/", fig.align="center"}
knitr::include_graphics("figures/02-00-transfer-learning-for-nlp/bert-masked_task.png")
```


## GPT-2 and XLNet

* GPT-2: a tremendous multilayer Transformer Decoder.

* XLNet: borrows ideas from autoregressive language modeling (e.g., Transformer-XL @dai2019transformer) and autoencoding (e.g., BERT) while avoiding their limitations.
