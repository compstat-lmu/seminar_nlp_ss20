---
title: Recurrent neural networks and their applications in NLP
author: Marianna Plesiak
output: html_document:
  toc: true
  toc_depth: 3
#bibliography: bibliography.bib
---


# Recurrent neural networks and their applications in NLP

*Author: Marianna Plesiak*

*Supervisor: Christian Heumann*


## RNNs 

### Structure

Recurrent neural networks allow to relax the  condition of non-cyclical connections in the classical feedforward neural networks which were described in the previous chapter. This means, while simple MLP (multilayer perceptrons) can only map from input to output vectors, RNNs allow the entire history of previous inputs to influence the network output. (@ Graves 2012)

The repetitive structure of RNNs can be visualised with help of an **unfolded** computational graph(see @ figure 1). 

```{r pressure, echo=FALSE, message=FALSE, fig.align="center",fig.cap="Unfolded computatinal graph of a RNN.", out.width = '80%'}

library(here)
knitr::include_graphics(here("/figures/01-02-rnns-and-their-applications-in-nlp/02_unfolded_graph.png"))
```

Each node is associated with a network layer at a particular time instance. Vectors $h$ represent the outputs of the hidden states. At time $t$, a hidden layer $h^{(t)}$ combines information from the previous hidden state $h^{(t-1)}$ as well as the new input $x^{(t)}$ and passes it through to the next hidden layer. Such an architecture requires the initialization of $h^{(0)}$ since there is no memory at the very beginning of the sequence processing. The unfolded recurrence can be formalized with the following equations:

$h^{(t)} = g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)}) \
 = f(h^{(t-1)},x^{(t)}; \theta)$
 
After $t$ steps, the function $g^{(t)}$ takes into account the whole sequence $(x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)})$ and produces the hidden state $h^{(t)}$. Because of its cyclical structure, $g^{(t)}$ can be factorized into the repeated application of a same function $f$. This function can be considered a universal model which is shared across all time steps and is generalized for all sequence lengths. This is called parameter sharing and is illustrated in the unfolded computational graph as a reuse of the same matrices $W_1$, $W_2$ and $W_3$ through the entire network.

### Backpropagation 

### Drawbacks

## Gated RNNs

### LSTM

### GRU

## Versions

### Bidirectional and Deep RNNs

### Applications

* 




