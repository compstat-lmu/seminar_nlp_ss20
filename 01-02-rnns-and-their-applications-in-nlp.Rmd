---
bibliography: book.bib
link-citations: yes
---

# Recurrent neural networks and their applications in NLP

*Author: Marianna Plesiak*

*Supervisor: Prof. Dr. Christian Heumann*


## Structure and Training of Simple RNNs

Recurrent neural networks allow relaxing the condition of non-cyclical connections in the classical feedforward neural networks which were described in the previous chapter. This means, while simple multilayer perceptrons can only map from input to output vectors, RNNs allow the entire history of previous inputs to influence the network output. [@graves2013generating]

The first part of this chapter provides the structure definition of RNNs, presents the principles of their training and explains problems with backpropagation. In the second part, gated units, an improved way to calculate hidden states, are explained. The third part gives an overview of some extended versions of RNNs and their applications in NLP.

### Network Structure and Forwardpropagation

The repetitive structure of RNNs can be visualized with the help of an unfolded computational graph (see \@ref(fig:01-02-unfold)). 

```{r 01-02-unfold, echo=FALSE, message=FALSE, fig.align="center",fig.cap="Unfolded computatinal graph of an RNN. Source: Own figure.", out.width = '100%'}

knitr::include_graphics("figures/01-02-rnns-and-their-applications-in-nlp/02_unfolded_graph.png")
```

Each node is associated with a network layer at a particular time instance. Inputs $x^{(t)}$ must be encoded as numeric vectors, for instance word embeddings or one-hot encoded vectors, see the previous chapter. Recurrently connected vectors $h$ are called hidden states and represent the outputs of the hidden layer. At time $t$, a hidden state $h^{(t)}$ combines information from the previous hidden state $h^{(t-1)}$ as well as the new input $x^{(t)}$ and passes it through to the next hidden state. Obviously, such an architecture requires the initialization of $h^{(0)}$ since there is no memory at the very beginning of the sequence processing. Given the hidden sequences, output vectors $\hat{y}^{(t)}$ are used to build the predictive distribution $Pr(x^{(t+1)}|y^{(t)})$ for the next input [@graves2013generating]. Since the predictions are created at each time instance $t$, the total output has a shape $[\#\ time\_steps, \#\ output\_features]$. However, in some cases the whole history of output features is not needed. For example, in sentiment analysis the last output of the loop is sufficient because it contains the entire information about the sequence. [@chollet2018deep]

The unfolded recurrence can be formalized as following:

\begin{align}
h^{(t)} & = g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)}) \\
& = f(h^{(t-1)},x^{(t)}| \theta)  (\#eq:recurrent)
\end{align}

After $t$ steps, the function $g^{(t)}$ takes into account the whole sequence $(x^{(t)},x^{(t-1)},...,x^{(2)}, x^{(1)})$ and produces the hidden state $h^{(t)}$. Because of its cyclical structure, $g^{(t)}$ can be factorized into the repeated application of the same function $f$. This function can be considered a universal model with parameters $\theta$ which is shared across all time steps and generalized for all sequence lengths. This concept is called parameter sharing and is illustrated in the unfolded computational graph as a reuse of the same matrices $W_{xh}$, $W_{hh}$ and $W_{hy}$ through the entire network. [@goodfellow2016deep]

Consider a recurrent neural network with one hidden layer that is used to predict words or characters, so the output is discrete and the model maps input sequence to output sequence of the same length. Then the forward propagation is computed by iterating the following equations:

\begin{align}
h^{(t)} & = \mathcal{f}(a+W_{hh}h^{(t-1)}+W_{xh}x^{(t)}) (\#eq:input-to-hidden) \\
y^{(t)} & = \mathcal{s}(b+W_{hy}h^{(t)}) (\#eq:hidden-to-output) 
\end{align}

where the parameters and functions denote the following:

* $\mathcal{f}$: activation function of the hidden layer. Usually it is a saturating nonlinear function such as sigmoid activation function ( @sutskever2014sequence and @mikolov2010recurrent) or tanh or ReLu  
* $W_{hh}$: weight matrix connecting recurrent connections between hidden states    
* $W_{xh}$: weight matrix connecting inputs to a hidden layer  
* $W_{hy}$: weight matrix connecting hidden states to outputs (softmax if we want to predict next word or letter)  
* $\mathcal{s}$: output layer function. If the model is used to predict words, the softmax function is usually chosen as it returns valid probabilities over the possible outputs  [@mikolov2010recurrent]  
* $a$, $b$: input and output bias vectors.  

[@graves2013generating]

Because inputs $x^{(t)}$ are usually encoded as one-hot-vectors, the dimension of a vector representing one word corresponds to the size of vocabulary. The size of a hidden layer must reflect the size of training data. The model training requires initialization of the initial state $h^{(0)}$ as well as the weight matrices, which are usually set to small random values [@mikolov2010recurrent]. Since the model is used to compute the predictive distributions $Pr(x^{(t+1)}|y^{(t)})$ at each time instance $t$, the network distribution is denoted as:

\begin{align}
Pr(x) & =\prod_{t} Pr(x^{(t+1)}|y^{(t)}) (\#eq:rnn-probability) 
\end{align}

and the total loss $\mathcal{L(x)}$ used for training is simply the sum of the losses over all time steps denoted as the negative log-likelihood of $Pr(x)$:

\begin{align}
\mathcal{L(x)} & =-\sum_{t} \log{Pr(x^{(t+1)}|y^{(t)})} (\#eq:rnn-log-loss)   
\end{align}

[@graves2013generating]


### Backpropagation

In order to train the model, one must calculate the gradients for the three weight matrices $W_{xh}$, $W_{hh}$ and $W_{hy}$. The algorithm differs from a regular backpropagation because a chain rule must be applied recursively and the gradients are summed up through the network. [@boden2002guide]

Gradients w.r.t. $W_{hy}$:

* for a single time step $t$:

\begin{align}
\frac{\partial \mathcal{L^{(t)}}}{\partial W_{hy}} & = \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial W_{hy}}(\#eq:rnn-back-hy-one) \\
\end{align}

* for the whole sequence:

\begin{align}
\frac{\partial \mathcal{L}}{\partial W_{hy}} & = \sum_{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}} {\partial W_{hy}}  (\#eq:rnn-back-hy-all) \\    
\end{align}

Gradients w.r.t. $W_{hh}$:

* for a single time step $t$:

\begin{align}
\frac{\partial \mathcal{L^{(t)}}}{\partial W_{hh}} & = \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial W_{hh}} (\#eq:rnn-back-hh-one)
\end{align}

The last part $h^{(t)}$ also depends on $h^{(t-1)}$ and the gradient can be rewritten as:

\begin{align}
\frac{\partial \mathcal{L^{(t)}}}{\partial W_{hh}}
& = \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(t-1)}} \frac{\partial h^{(t-1)}}{\partial W_{hh}} \\
& = \sum_{k=0}^{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W_{hh}} (\#eq:rnn-back-hh-one-one)
\end{align}


* for the whole sequence:

\begin{align}
\frac{\partial \mathcal{L}}{\partial W_{hh}}
& = \sum_{t} \sum_{k=0}^{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W_{hh}} (\#eq:rnn-back-hh-all)
\end{align}


Gradient w.r.t. $W_{xh}$ is similar to $W_{hh}$:

* for a single step:

\begin{align}
\frac{\partial \mathcal{L^{(t)}}}{\partial W_{xh}}
& = \sum_{k=0}^{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W_{xh}} (\#eq:rnn-back-xh-one)
\end{align}

* for the whole sequence:

\begin{align}
\frac{\partial \mathcal{L}}{\partial W_{xh}}
& = \sum_{t} \sum_{k=0}^{t} \frac{\partial \mathcal{L^{(t)}}}{\partial y^{(t)}} \frac{\partial y^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W_{xh}} (\#eq:rnn-back-xh-all)
\end{align}

[@chen2016gentle]

### Vanishing and Exploding Gradients

In order to better understand the mathematical challenges of BPTT, consider equation \@ref(eq:rnn-back-hh-one-one), and in particular the factor $\frac{\partial h^{(t)}}{\partial h^{(k)}}$. @pascanu2013difficulty go into detail and show that it is a chain rule in itself and can be rewritten as a product $\frac{\partial h^{(t)}}{\partial h^{(t-1)}} \frac{\partial h^{(t-1)}}{\partial h^{(t-2)}} \ldots \frac{\partial h^{(2)}}{\partial h^{(1)}}$. Since one computes the derivative of a vector with respect to a vector, the result will be a product of $t-k$ Jacobian matrices whose elements are the pointwise derivatives:

\begin{align}
\frac{\partial h^{(t)}}{\partial h^{(k)}}
& = \prod_{t\geq\ i>k}^{} \frac{\partial h^{(i)}}{\partial h^{(i-1)}}
= \prod_{t\geq\ i>k}^{} W_{hh}^{T}diag(\mathcal{f}^{'}(h^{(i-1)}))  (\#eq:rnn-back-vanishing) 
\end{align}

Thus, with small values in $W_{hh}$ and many matrix multiplications the norm of the gradient shrinks to zero exponentially fast with $t-k$ which results in a vanishing gradient problem and the loss of long term contributions. Exploding gradients refer to the opposite behavior when the norm of the gradient increases largely and leads to a crash of the model. @pascanu2013difficulty give an overview of techniques for dealing with the exploding and vanishing gradients. Among other solutions they mention proper initialisation of weight matrices, sampling $W_{hh}$ and $W_{xh}$ instead of learning them, rescaling or clipping the gradient's components (putting a maximum limit on it) or using L1 or L2 penalties on the recurrent weights. Even more popular models are gated RNNs which explicitly address the vanishing gradients problem and will be explained in the next subchapter.

## Gated RNNs

### LSTM

Long Short-Term Memory networks were introduced by @hochreiter1997long with the purpose to deal with problems of long term dependencies. Instead of a simple hidden unit that combines inputs and previous hidden states linearly and outputs the non-linear transformation to the next step, hidden units are now extended by special input, forget and output gates which help to control the flow of information. Such more complex units are called memory cells and the following equations show how a LSTM uses the gating mechanism to calculate the hidden state within a memory cell: 

\begin{align}
f^{(t)} & = sigm(W_{xf}x^{(t)}+W_{hf}h^{(t-1)}+b_{f}) (\#eq:lstm-forget) \\
i^{(t)} & = sigm(W_{xi}x^{(t)}+W_{hi}h^{(t-1)}+b_{i}) (\#eq:lstm-input) \\
o^{(t)} & = sigm(W_{xo}x^{(t)}+W_{ho}h^{(t-1)}+b_{o}) (\#eq:lstm-output) \\
g^{(t)} & = tanh(W_{xc}x^{(t)}+W_{hc}h^{(t-1)}+b_{c}) (\#eq:lstm-candidates) \\
c^{(t)} & = f^{(t)}c^{(t-1)}+i^{(t)}g^{(t)} (\#eq:lstm-newcell) \\
h^{(t)} & = o^{(t)}tanh(c^{(t)}) (\#eq:lstm-newoutput) \\
\end{align}
[@graves2013generating]

First, forget gate $f^{(t)}$ decides which values of the previous output $h^{(t-1)}$ to forget. The next step is deciding which information will be stored in the internal cell state $c^{(t)}$. This step consists of two parts: 1) multiplication of the old state $c^{(t-1)}$ by $f^{(t)}$ (forgetting information); 2) adding new candidates calculated in $g^{(t)}$ with help of its multiplication by values from the input gate $i^{(t)}$ (adding new information). The output $h^{(t)}$ is produced with help of the output gate $o^{(t)}$ and applying a $tanh$ function to the cell state in order to only output values which were chosen.(@goodfellow2016deep, @graves2013generating)


### GRU

Invented by @cho2014learning.
Is simpler because it includes only two gates: reset and update

The hidden unit is calculated as:

\begin{align}
r^{(t)} & = sigm(W_{xr}x^{(t)}+W_{hr}h^{(t-1)}) (\#eq:gru-reset) \\
z^{(t)} & = sigm(W_{xz}x^{(t)}+W_{hz}h^{(t-1)}) (\#eq:gru-update) \\
\tilde{h}^{(t)} & = tanh(W_{xh}x^{(t)}+W_{hh}(r^{(t)}h^{(t-1)})) (\#eq:gru-hidden) \\
h^{(t)} & = z_jh_j^{(t-1)}+(1-z_j)\tilde{h}^{(t)} (\#eq:gru-hidden-output) \\
\end{align}



Illustration LSTM vs GRU

```{r 01-02-lstm-gru, echo=FALSE, message=FALSE, fig.align="center",fig.cap="Structure of a hidden unit. LSTM on the right and GRU on the left. Source: Own figure inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/", out.width = '100%'}

knitr::include_graphics("figures/01-02-rnns-and-their-applications-in-nlp/03_lstm_vs_gru.png")
```

## Extensions of Simple RNNs

### Deep RNNs

#### Stacked RNNs  

The figure \@ref(fig:01-02-unfold) shows that each layer is associated with one parameter matrix so that the model is considered shallow. This structure can be extended to a deep RNN, although the depth of an RNN is not a trivial concept since its units are already expressed as a nonlinear function of multiple previous units. In their paper @graves2013speech use a deep RNN for speech recognition where the depth is defined as stacking $N$ hidden layers on top of each other with $N>1$. In this case the hidden vectors are computed iteratively for all hidden layers $n=1$ to $N$ and for all time instances $t=1$ to $T$:

\begin{align}
h_{n}^{(t)} & = \mathcal{f}(a_{n}+W_{h_{n}h_{n}}h_{n}^{(t-1)}+W_{h_{n-1}h_{n}}h_{n-1}^{(t)}) (\#eq:deep-hidden)
\end{align}

As a hidden layer function @graves2013speech choose bidirectional LSTM. Compared to regular LSTM, BiLSTM is able to train on inputs in their original as well as reversed order. The idea is to stack two separate hidden layers one on another while one of the layers is responsible for the forward information flow and another one for the backward information flow. Especially in speech recognition one must take in consideration future context, too, because pronouncement depends both on previous and next phonemes. Thus, BiLSTMs are able to access long-time dependencies in both input directions. Finally, @graves2013speech compare different deep BiLSTM models (from 1 to 5 hidden layers) with unidirectional LSTMs and a pretrained RNN transducer (BiLSTM with 3 hidden layers pretrained to predict each phoneme given the previous ones) and show clear advantage of deep networks over shallow designs. 

#### Deep Transition RNNs  

@pascanu2013construct proposed another way to make an RNN deeper by introducing transitions, one or more intermediate nonlinear layers between input to hidden, hidden to output or two consecutive hidden states. They argue that extending input-to-hidden functions helps to better capture temporal structure between successive inputs. A deeper hidden-to-output function, DO-RNN, can make hidden states more compact and therefore enables the model to summarize the previous inputs more efficiently. A deep hidden-to-hidden composition, DT-RNN, allows for the hidden states to effectively add new information to the accumulated summaries from the previous steps. For example, the model preserves useful information from the past although new inputs vary strongly and rapidly. They note though, because of including deep transitions, the distances between two variables at $t$ and $t+1$ become longer and the problem of loosing long-time dependencies may occur. One can add shortcut connections to provide shorter paths for gradients, such networks are referred to as DT(S)-RNNs. If deep transitions with shortcuts are implemented both in hidden and output layers, the resulting model is called DOT(S)-RNNs. @pascanu2013construct evaluate these designs on the tasks of polyphonic music prediction and character- or word-level language modeling. Their results reveal that deep transition RNNs clearly outperform shallow RNNs in terms of perplexity and negative log-likelihood.

### Encoder-Decoder Architecture

#### Design and Training

The problem of mapping variable-length input sequences to variable-length output sequences is known as Sequence-to-Sequence or seq2seq learning in NLP. Although originally applied in machine translation tasks (@sutskever2014sequence, @cho2014learning), the seq2seq approach achieved state-of-the-art results also in speech recognition [@prabhavalkar2017comparison] and video captioning [@venugopalan2015sequence]. According to @cho2014learning, the seq2seq model is composed of two parts as illustrated below:

```{r 01-02-enc-dec, echo=FALSE, message=FALSE, fig.align="center",fig.cap="Encoder-Decoder architecture. Source: Own figure.", out.width = '100%'}

knitr::include_graphics("figures/01-02-rnns-and-their-applications-in-nlp/04_encoder_decoder.png")
```

The first part is an encoder, an RNN which is trained on input sequences in order to obtain a large summary vector $c$ with a fixed dimension. This vector is called context and is usually a simple function of the last hidden state. @sutskever2014sequence used the final encoder hidden state as context such that $c=h_{e}^{(T)}$. The second part of the model is a decoder, another RNN which generates predictions given the context $c$ and all the previous outputs. In contrast to a simple RNN described at the beginning of this chapter, decoder hidden states $h_{d}^{(t)}$ are now conditioned on the previous outputs $y^{(t)}$, previous hidden states $h_{d}^{(t)}$ and the summary vector $c$ from the encoder part. Therefore, the conditional distribution of the one-step prediction is obtained by:

\begin{align}
h_{d}^{(t)} & = f(h_{d}^{(t-1)},y^{(t-1)},c) (\#eq:decoder-hidden) \\
P(y^{(t)}|y^{(t-1)},...y^{(1)},c) & = \mathcal{s}(h_{d}^{(t)},y^{(t-1)},c) (\#eq:decoder-output) \\
\end{align}

Both parts are trained simultaneously to maximize the conditional log-likelihood $\frac{1}{N}\sum_{n=1}^{N}\log{p_{\theta}(y_{n}|x_{n})}$, where $\theta$ denotes the set of model parameters and $(y_{n},x_{n})$ is an (input sequence, output sequence) pair from the training set with size $N$ (@cho2014learning).

#### Multi-task seq2seq Learning

@luong2015multi extended the idea of encoder-decoder architecture even further by allowing multi-task learning (MLT) for seq2seq models. MLT aims to improve performance of one task using other related tasks. In their paper, they investigate three settings: a) *one-to-many* - where the encoder is shared between different tasks such as translation and syntactic parsing, b) *many-to-one* - where the encoders learn different tasks such as translation and image captioning and the decoder is shared, c) *many-to-many* - where the model consists of multiple encoders and decoders which is the case of autoencoders, unsupervised task used to learn a representation of monolingual data. @luong2015multi consider German-to-English and English-to-German translations as the primary task and try to determine whether other tasks can improve their performance and vice versa. After training deep LSTM models with 4 layers for different task combinations, they come to the conclusion that MLT can improve the performance of seq2seq models substantially. For instance, the translation quality improves after adding a small number of parsing minibatches (one-to-many setting) or after the model have been trained to generate image captions (many-to-one setting). In turn, translation task helps to parse large data corpus much better (one-to-many setting). In contrust to these achievements, autoencoder task does not show significant improvements in translation after two unsupervised learning tasks on English and German language data.

## Conclusion

tbd

